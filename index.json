[{"categories":["DevOps"],"content":"开篇","date":"2021-03-20","objectID":"/oops_series_two/","tags":["devops","platform"],"title":"Oops管理系统(二)","uri":"/oops_series_two/"},{"categories":["DevOps"],"content":"系列目录 《Oops管理系统(一)》 《Oops管理系统(二)》 ","date":"2021-03-20","objectID":"/oops_series_two/:1:0","tags":["devops","platform"],"title":"Oops管理系统(二)","uri":"/oops_series_two/"},{"categories":["DevOps"],"content":"1. 介绍 本篇，要先讲下整体的技术框架。 因为我们是基于go-admin这个脚手架来做前后端框架的，所以，先来说明下整个代码架构。 在这之前，我们先简单说明下go程序的代码执行顺序 ","date":"2021-03-20","objectID":"/oops_series_two/:2:0","tags":["devops","platform"],"title":"Oops管理系统(二)","uri":"/oops_series_two/"},{"categories":["DevOps"],"content":"1.1 Go程序执行顺序 执行go run或者编译后binary文件时，会先加载main package main package一般会import其他package，其他package也会import其依赖的package，这边会有一个递归的初始化操作。 package会执行global variables和init()的初始化 main package执行本身的global variables和init()的初始化 执行main() ","date":"2021-03-20","objectID":"/oops_series_two/:2:1","tags":["devops","platform"],"title":"Oops管理系统(二)","uri":"/oops_series_two/"},{"categories":["DevOps"],"content":"2. 目录结构 Oops基于project-layout和go-admin构建了如下图所示的目录结构 ","date":"2021-03-20","objectID":"/oops_series_two/:3:0","tags":["devops","platform"],"title":"Oops管理系统(二)","uri":"/oops_series_two/"},{"categories":["DevOps"],"content":"3. 源码解析 这边只说明应用相关的源码逻辑。 ","date":"2021-03-20","objectID":"/oops_series_two/:4:0","tags":["devops","platform"],"title":"Oops管理系统(二)","uri":"/oops_series_two/"},{"categories":["DevOps"],"content":"3.1 系统初始化 main.go package main import ( \"oops/cmd\" ) func main() { cmd.Execute() } main函数只有一个逻辑，加载cmd包以及执行cmd包中的Execute()函数。 cmd package package cmd import ( ... \"oops/internal/common/global\" \"github.com/spf13/cobra\" \"oops/cmd/api\" ... ) var rootCmd = \u0026cobra.Command{ ... } ... func init() { ... rootCmd.AddCommand(api.StartCmd) ... } //Execute : apply commands func Execute() { if err := rootCmd.Execute(); err != nil { os.Exit(-1) } } 这边用到了一个cobra的命令行框架，以及加载api的相关逻辑。 cmd/api package api import ( ... \"github.com/spf13/cobra\" \"oops/internal/app/admin/router\" ... ) var ( configYml string StartCmd = \u0026cobra.Command{ Use: \"server\", Short: \"Start API server\", Example: \"go-admin server -c config/settings.yml\", SilenceUsage: true, PreRun: func(cmd *cobra.Command, args []string) { setup() }, RunE: func(cmd *cobra.Command, args []string) error { return run() }, } ) var AppRouters = make([]func(), 0) func init() { ... //注册路由 fixme 其他应用的路由，在本目录新建文件放在init方法 AppRouters = append(AppRouters, router.InitRouter) } func setup() { //1. 读取配置 ... //2. 设置日志 ... //3. 初始化数据库链接 ... } func run() error { //运行http服务 ... return nil } 这里进行了注册路由的操作，并读取配置，对日志、数据库以及http server进行初始化。 internal/app/admin/router package router import ( ... \"github.com/gin-gonic/gin\" log \"github.com/go-admin-team/go-admin-core/logger\" \"oops/internal/app/admin/middleware\" \"oops/internal/app/admin/middleware/handler\" common \"oops/internal/common/middleware\" ... ) // InitRouter 路由初始化，不要怀疑，这里用到了 func InitRouter() { ... middleware.InitMiddleware(r) // the jwt middleware authMiddleware, err := middleware.AuthInit() if err != nil { log.Fatalf(\"JWT Init Error, %s\", err.Error()) } // 注册系统路由 InitSysRouter(r, authMiddleware) // 注册业务路由 // TODO: 这里可存放业务路由，里边并无实际路由只有演示代码 InitExamplesRouter(r, authMiddleware) } var ( routerNoCheckRole = make([]func(*gin.RouterGroup), 0) routerCheckRole = make([]func(v1 *gin.RouterGroup, authMiddleware *jwt.GinJWTMiddleware), 0) ) // 路由示例 func InitExamplesRouter(r *gin.Engine, authMiddleware *jwt.GinJWTMiddleware) *gin.Engine { // 无需认证的路由 examplesNoCheckRoleRouter(r) // 需要认证的路由 examplesCheckRoleRouter(r, authMiddleware) return r } // 无需认证的路由示例 func examplesNoCheckRoleRouter(r *gin.Engine) { // 可根据业务需求来设置接口版本 v1 := r.Group(\"/api/v1\") // 空接口防止v1定义无使用报错 v1.GET(\"/nilcheckrole\", nil) for _, f := range routerNoCheckRole { f(v1) } // {{无需认证路由自动补充在此处请勿删除}} //registerSysFileInfoRouter(v1) } // 需要认证的路由示例 func examplesCheckRoleRouter(r *gin.Engine, authMiddleware *jwtauth.GinJWTMiddleware) { // 可根据业务需求来设置接口版本 v1 := r.Group(\"/api/v1\") // 空接口防止v1定义无使用报错 v1.GET(\"/checkrole\", nil) for _, f := range routerCheckRole { f(v1, authMiddleware) } } 这里面实现了具体的路由注册接口逻辑，后续新增的业务路由注册只需要往routerCheckRole和routerNoCheckRole变量中插入数据即可。 一个例子： func init() { routerCheckRole = append(routerCheckRole, registerApplicationRouter) } // 需认证的路由代码 func registerApplicationRouter(v1 *gin.RouterGroup, authMiddleware *jwt.GinJWTMiddleware) { api := \u0026cmdb.Application{} r := v1.Group(\"/cmdb/application\").Use(authMiddleware.MiddlewareFunc()).Use(middleware2.AuthCheckRole()).Use(actions.PermissionAction()) { r.GET(\"\", api.GetApplicationList) r.GET(\"/:id\", api.GetApplication) r.POST(\"\", api.InsertApplication) r.PUT(\"\", api.UpdateApplication) r.DELETE(\"\", api.DeleteApplication) } } 该例子将应用相关的路由信息添加到了routerCheckRole里面。 ","date":"2021-03-20","objectID":"/oops_series_two/:4:1","tags":["devops","platform"],"title":"Oops管理系统(二)","uri":"/oops_series_two/"},{"categories":["DevOps"],"content":"3.2 定义一个业务 internal/app 目录结构说明 . ├── admin │ ├── apis │ │ ├── cmdb │ │ │ └── application.go │ ├── middleware │ ├── models │ │ ├── cmdb │ │ │ └── application.go │ ├── router │ │ ├── cmdb.go │ │ ├── router.go │ └── service │ ├── cmdb │ │ ├── application.go │ │ ├── dto │ │ │ ├── application.go admin：可以理解成一个project apis.cmdb：是project的api文件 middleware：是project的中间件 models.cmdb：是project的数据库层的模型 router：是project的路由 service.cmdb：是project的业务逻辑处理 service.cmdb.dto：是project的api对应的数据接收以及解析模型 现在，我们按照 models、service.dto、service、apis、router这个顺序来说明； models package cmdb import ( \"oops/internal/app/admin/models/system\" \"oops/internal/common/models\" ) type Application struct { models.ControlBy models.ModelTime AppId int `gorm:\"primaryKey;autoIncrement;comment:应用ID\" json:\"appId\"` AppName string `json:\"appName\" gorm:\"type:varchar(64);comment:应用名称\"` Language string `json:\"language\" gorm:\"type:varchar(128);comment:语言\"` Repo string `json:\"repo\" gorm:\"type:varchar(128);comment:代码仓库\"` DeptId int `json:\"deptId\" gorm:\"type:bigint(20);comment:部门\"` UserId int `json:\"userId\" gorm:\"type:bigint(20);comment:用户\"` Remark string `json:\"remark\" gorm:\"type:varchar(255);comment:备注\"` Status string `json:\"status\" gorm:\"type:varchar(4);comment:状态\"` Dept *system.SysDept `json:\"dept\"` User *system.SysUser `json:\"user\"` } func (Application) TableName() string { return \"application\" } func (e *Application) Generate() models.ActiveRecord { o := *e return \u0026o } func (e *Application) GetId() interface{} { return e.AppId } 首先，是一个结构体Application里边含有正常的数据库表字段，但是其中又包含了三个结构体： 1、models.Model 表id 默认主键是固定的ID和自增长的int类型 2、models.ControlBy 表创建人和修改人 数据库表默认必有字段 3、models.ModelTime 表创建时间和修改时间、删除时间的字段默认必有字段 dto package dto import ( \"oops/internal/common/apis\" \"github.com/gin-gonic/gin\" \"oops/internal/app/admin/models/cmdb\" \"oops/internal/common/dto\" common \"oops/internal/common/models\" ) // ApplicationSearch 搜索列表对应的数据接收模型 主要针对分页、列表； type ApplicationSearch struct { dto.Pagination `search:\"-\"` AppId int `form:\"AppId\" search:\"type:exact;column:app_id;table:application\" comment:\"应用ID\"` AppName string `form:\"appName\" search:\"type:contains;column:app_name;table:application\" comment:\"应用名称\"` Language string `form:\"language\" search:\"type:contains;column:language;table:application\" comment:\"语言\"` Repo string `form:\"repo\" search:\"type:contains;column:repo;table:application\" comment:\"仓库地址\"` DeptId string `form:\"deptId\" search:\"type:exact;column:dept_id;table:application\" comment:\"部门\"` UserId string `form:\"userId\" search:\"type:exact;column:user_id;table:application\" comment:\"用户\"` Status string `form:\"status\" search:\"type:exact;column:status;table:application\" comment:\"状态\"` } // GetNeedSearch 将search 转化为interface func (m *ApplicationSearch) GetNeedSearch() interface{} { return *m } // Bind 从上下文中解析数据 func (m *ApplicationSearch) Bind(ctx *gin.Context) error { log := apis.GetRequestLogger(ctx) err := ctx.ShouldBind(m) if err != nil { log.Debugf(\"ShouldBind error: %s\", err.Error()) } return err } // Generate 将数据转化成数据库使用的结构体 func (m *ApplicationSearch) Generate() dto.Index { o := *m return \u0026o } // ApplicationControl 创建、修改使用的数据接收模型 type ApplicationControl struct { AppId int `json:\"appId\" comment:应用ID\"` AppName string `json:\"appName\" comment:应用名称\"` Language string `json:\"language\" comment:语言\"` Repo string `json:\"repo\" comment:代码仓库\"` DeptId int `json:\"deptId\" comment:部门\"` UserId int `json:\"userId\" comment:用户\"` Remark string `json:\"remark\" comment:备注\"` Status string `json:\"status\" comment:状态\"` } // Bind 从上下文中解析数据 func (a *ApplicationControl) Bind(ctx *gin.Context) error { log := apis.GetRequestLogger(ctx) //err := ctx.ShouldBindUri(s) //if err != nil { // log.Debugf(\"ShouldBindUri error: %s\", err.Error()) // return err //} err := ctx.ShouldBind(a) if err != nil { log.Debugf(\"ShouldBind error: %s\", err.Error()) } return err } // Generate 将数据转化成数据库使用的结构体 func (a *ApplicationControl) Generate() dto.Control { cp := *a return \u0026cp } func (a *ApplicationControl) GenerateM() (common.ActiveRecord, error) { return \u0026cmdb.Applicati","date":"2021-03-20","objectID":"/oops_series_two/:4:2","tags":["devops","platform"],"title":"Oops管理系统(二)","uri":"/oops_series_two/"},{"categories":["DevOps"],"content":"4. 总结 本章说明了oops项目的后端代码结构，以及实现一个业务的逻辑操作。整个项目会用到的主流框架如下： go-admin cobra gorm gin casbin swagger viper(后续考虑加入) ","date":"2021-03-20","objectID":"/oops_series_two/:5:0","tags":["devops","platform"],"title":"Oops管理系统(二)","uri":"/oops_series_two/"},{"categories":["DevOps"],"content":"开篇","date":"2021-03-17","objectID":"/oops_series_one/","tags":["devops","platform"],"title":"Oops管理系统(一)","uri":"/oops_series_one/"},{"categories":["DevOps"],"content":"系列目录 《Oops管理系统(一)》 《Oops管理系统(二)》 ","date":"2021-03-17","objectID":"/oops_series_one/:1:0","tags":["devops","platform"],"title":"Oops管理系统(一)","uri":"/oops_series_one/"},{"categories":["DevOps"],"content":"1. 介绍 做了那么的久的运维，了解、运维和开发过各种各样的发布系统、运维系统、监控系统，经历了服务器从物理时代、到云时代再到容器时代的变革，服务器规模也经历从十几台到几千台甚至几万台规模的管理，管理手段也经历从手工到工具再到平台的演变。做过那么些的管理系统，因为各种各样的原因，可能是历史包袱，可能是人言式微，可能是早期技术没有那么的好，很多时候做出来的管理系统不是自己理想中的管理系统，因此，想花一些业余时间，写一个自己理解的DevOps管理系统。 ","date":"2021-03-17","objectID":"/oops_series_one/:2:0","tags":["devops","platform"],"title":"Oops管理系统(一)","uri":"/oops_series_one/"},{"categories":["DevOps"],"content":"2. 目标 个人理解，一个DevOps管理系统应该具备以下功能： 元数据管理，包括物理资源、云资源、容器资源、中间件资源、应用资源等 CI/CD，包括应用的打包、编译以及发布，服务器资源的创建、更新、删除等 监控告警，能实现不同指标的采集监控，监控数据可视化以及告警自定义等。 机器操作，可能是SSH登录，或者远程操作命令等。 ","date":"2021-03-17","objectID":"/oops_series_one/:3:0","tags":["devops","platform"],"title":"Oops管理系统(一)","uri":"/oops_series_one/"},{"categories":["DevOps"],"content":"3. 系统设计 完整的一套DevOps系统肯定是功能复杂、组件繁多的，不大可能自己造轮子，因此，底层会采用一些开源方案，然后在各个开源组件上构建自己的管理平台。 ","date":"2021-03-17","objectID":"/oops_series_one/:4:0","tags":["devops","platform"],"title":"Oops管理系统(一)","uri":"/oops_series_one/"},{"categories":["DevOps"],"content":"3.1 元数据管理 说到底，我们要做的是一套能完整接入软件工程生命周期的工具平台，一个软件最终的目的是要发布到线上环境供用户使用，服务器资源只是软件的载体。那么，我们把软件叫做应用，从软件层面上来讲，一个应用应该是唯一的，而从\"硬件\"层面来说，一台\"服务器\"也是唯一的。那么，我们就可以将这两个\"唯一\"建立一种联系，然后再通过这两个唯一延伸出各种关联信息，比如，一个应用可能需要关联它的代码仓库，对应的负责人，归属部门等等，而一台服务器可能需要知道它的机房、机柜，或者是云平台，区域等信息。下图是一个我自己构建的元数据关联图： 在设计元数据的表的时候，我们就可以根据这种关联关系来设计表。 ","date":"2021-03-17","objectID":"/oops_series_one/:4:1","tags":["devops","platform"],"title":"Oops管理系统(一)","uri":"/oops_series_one/"},{"categories":["DevOps"],"content":"3.2 CI/CD 一个应用的生命周期一般包含以下步骤： Design，接收需求，产品设计，沟通等，这边可能需要接入一些项目管理的工具，比如JIRA等、 Develop，程序员进行代码开发、架构设计等 Test，开发人员进行代码测试，测试人员进行功能测试等，这边可以引入一些自动化测试工具。 Deploy，各种人员可能会进行各种环境的部署，这边可以引入类似Jenkins的打包发布工具提高效率。 Monitor，进行监控数据的收集，通过平台输出可视化页面，配置告警、告警通知等功能。 ","date":"2021-03-17","objectID":"/oops_series_one/:4:2","tags":["devops","platform"],"title":"Oops管理系统(一)","uri":"/oops_series_one/"},{"categories":["DevOps"],"content":"3.3 监控告警 一个业务应用上线后，需要有工具来知道业务是否出现了问题，这时候就需要提供监控告警的一些工具。一个监控系统一般是如下图所示的架构 Agent，监控数据的收集器，可能是自研的跑在服务器上的采集客户端，也可能是嵌入到应用的收集器等待 Server，接收监控数据，进行存储的服务 Data，对数据进行整合、解析，告警触发、发送告警等操作的服务 Display，展示平台，供用户查看监控数据，配置告警规则的服务 ","date":"2021-03-17","objectID":"/oops_series_one/:4:3","tags":["devops","platform"],"title":"Oops管理系统(一)","uri":"/oops_series_one/"},{"categories":["DevOps"],"content":"3.4 机器操作 当整个DevOps平台完善的话，其实用户是可以不需要进入机器进行操作的，但是现实往往是不完美的，因此还需要类似堡垒机这样的服务来供用户登录机器进行一些日志查看，debug等操作。 ","date":"2021-03-17","objectID":"/oops_series_one/:4:4","tags":["devops","platform"],"title":"Oops管理系统(一)","uri":"/oops_series_one/"},{"categories":["DevOps"],"content":"4. 初步实现想法 管理页面的话，目前的话想基于go-admin来做，一个用vue+go实现的管理系统脚手架 整个元数据的表设计和管理页面设计，目前的话，应该是要自己写，会参考一些例如bk-cmdb、nightingale等开源方案的设计。 CI/CD的话，代码仓库会采用gitlab组件，发布的话可能采用Jenkins、Argo-CD等开源组件，这个之前做的不多，需要再做点调研，载体的话，现在是容器化时代，可能会直接对接k8s来操作。 监控的话，考虑到Kubernetes，直接会在Prometheus+grafana进行二次开发，日志监控选取ELK或者Loki 堡垒机现在还没有进行研究，后续再进行调研。 基础服务资源 上面的话，都是针对软件层面的，而应用依赖载体，因此，还需要有一些功能是用来做资源申请的，整体会考虑到兼容物理资源、云服务器资源、容器资源。除了物理资源以为，另外两个其实都可以通过自动化来实现按需创建，即要用的时候再申请。不过，我们还是保留人工审核创建的功能。因此需要做如下事情。 提供工单系统，用于资源的申请和下架，当然，一些事务处理，比如故障处理也可以放在这里做记录 对接物理资源，可能需要录入资源的页面，人工操作，优先需要把字段规范好，高端点，也可以做些自动发现功能。 对接各个云平台，可能采用terrform或者操作云平台API来构建基础设施，包括EC2、RDS、Redis、LB、消息队列、云存储等。 对接k8s集群，包括集群的创建，管理、以及各种k8s资源的操作 ","date":"2021-03-17","objectID":"/oops_series_one/:5:0","tags":["devops","platform"],"title":"Oops管理系统(一)","uri":"/oops_series_one/"},{"categories":["DevOps"],"content":"5. 总结 目前的话，暂时就以上这些想法，开发的话，后续会写成一个系列，来深入的说下各个功能的实现。 ","date":"2021-03-17","objectID":"/oops_series_one/:6:0","tags":["devops","platform"],"title":"Oops管理系统(一)","uri":"/oops_series_one/"},{"categories":["Nginx"],"content":"执行阶段","date":"2021-01-30","objectID":"/openresty_phases/","tags":["nginx"],"title":"Openresty执行阶段","uri":"/openresty_phases/"},{"categories":["Nginx"],"content":"Nginx执行阶段 NGX_HTTP_POST_READ_PHASE — 第一阶段， ngx_http_realip_module 在此阶段注册其处理程序，以允许在调用任何其他模块之前替换客户端地址。 NGX_HTTP_SERVER_REWRITE_PHASE — 该阶段处理server块(location块除外)定义的rewrite指令。 ngx_http_rewrite_module 在此阶段安装其处理程序。 NGX_HTTP_FIND_CONFIG_PHASE — 根据请求URI选择位置的特殊阶段。在此阶段之前，将相关虚拟服务器的默认位置分配给请求，并且任何请求位置配置的模块都将接收默认服务器位置的配置。该阶段为请求分配一个新位置。该阶段无法注册其他处理程序。 NGX_HTTP_REWRITE_PHASE — 同 NGX_HTTP_SERVER_REWRITE_PHASE, 但处理的是上一个阶段选择的location块内的定义的rewrite规则指令。 NGX_HTTP_POST_REWRITE_PHASE — 特殊阶段，如果请求的URI在rewirte期间更改，则将请求重定向到新的location块。这是通过再次请求 NGX_HTTP_FIND_CONFIG_PHASE 来完成的。此阶段无法注册其他处理程序。 NGX_HTTP_PREACCESS_PHASE — 与访问控制无关的用于不同类型的处理程序的公共阶段。标准nginx模块 ngx_http_limit_conn_module 和 ngx_http_limit_req_module 在此阶段注册其处理程序。 NGX_HTTP_ACCESS_PHASE — 验证客户端请求是否合法的阶段。例如ngx_http_access_module 和 ngx_http_auth_basic_module 等标准nginx模块在此阶段注册其处理程序。默认情况下，客户端必须通过该阶段所有处理程序的合法验证才能继续请求下一个阶段。 通过satisfy 指令，则可以允许客户端在通过该阶段任何一个处理程序的合法验证后直接进入下一个阶段。 NGX_HTTP_POST_ACCESS_PHASE — 特殊阶段，处理满足 satisfy any 指令的阶段。如果某些访问阶段处理程序拒绝了访问并且没有显式允许访问，则该请求完成。此阶段无法注册其他处理程序。 NGX_HTTP_PRECONTENT_PHASE — 在生成内容之前调用处理程序的阶段。一些标准nginx模块如 ngx_http_try_files_module 和 ngx_http_mirror_module 在此阶段注册其处理程序。 NGX_HTTP_CONTENT_PHASE — 正常生成响应的阶段. 多个Nginx标准模块在此阶段注册其处理程序, 包括 ngx_http_index_module 和 ngx_http_static_module。它们按顺序被调用直到某一个模块产生输出。也可以按location设置内容处理程序，如果 ngx_http_core_module的location配置已设置处理程序，则将其称为内容处理程序，并且在此阶段安装的处理程序将被忽略。 NGX_HTTP_LOG_PHASE — 执行请求日志记录的阶段。当前，只有 ngx_http_log_module 在此阶段注册其处理程序以进行访问日志记录。在释放请求之前，在请求处理的最后阶段调用日志阶段处理程序。 ","date":"2021-01-30","objectID":"/openresty_phases/:0:1","tags":["nginx"],"title":"Openresty执行阶段","uri":"/openresty_phases/"},{"categories":["Nginx"],"content":"Lua执行阶段 init_by_lua* — 在Nginx master 进程加载配置时候时，在全局LuaVM 级别上运行指定的lua代码。通常在该阶段注册全局变量或者预加载lua模块。 init_worker_by_lua* — 启用master进程后，在每次Nginx worker进程启动时运行指定的Lua代码。当禁用master进程时，此hook将仅在init_by_lua *之后运行。通常用于创建每个worker的重复计时器（通过Lua API的ngx.timer.at），也用于后端运行状况检查或其他定时例行工作。 ssl_certificate_by_lua* — 当Nginx即将启动下游https连接的SSL握手时，此指令运行用户Lua代码。 set_by_lua* — 该指令旨在执行简短，快速运行的代码块，因为在代码执行期间会阻止Nginx事件循环。因此，应该避免耗时的代码队列。 rewrite_by_lua* — 充当rewrite阶段处理程序，并针对每个请求执行指定的Lua代码字符串。 Lua代码可以进行API调用，并在独立的全局环境（即沙箱）中作为新生成的协程执行。请注意，此处理程序始终在标准ngx_http_rewrite_module之后运行。 access_by_lua* — 充当access阶段处理程序。请注意，此处理程序始终在标准ngx_http_access_module 之后运行。 content_by_lua* — 充当content阶段处理程序。 balancer_by_lua* — 该指令将Lua代码作为upstream{}配置块定义的任何upstream实体的upsteam balancer 运行 header_filter_by_lua* — 执行lua代码用于定义输出header过滤器 body_filter_by_lua* — 执行lua代码用于定义输出body过滤器 log_by_lua* — 在日志请求处理阶段执行lua代码，此操作不会替代当前的access日志，而是在它之前执行。 参考 nginx-http-phases openresty-reference ","date":"2021-01-30","objectID":"/openresty_phases/:0:2","tags":["nginx"],"title":"Openresty执行阶段","uri":"/openresty_phases/"},{"categories":["Cloud Native"],"content":"Kubernetes成熟度模型","date":"2021-01-28","objectID":"/k8s_model/","tags":["kubernetes","translate"],"title":"[译]Kubernetes成熟度模型","uri":"/k8s_model/"},{"categories":["Cloud Native"],"content":"原文链接：kubernetes maturity model 水平有限，本文不免存在遗漏或错误之处。如有疑问，请查阅原文。 ","date":"2021-01-28","objectID":"/k8s_model/:0:0","tags":["kubernetes","translate"],"title":"[译]Kubernetes成熟度模型","uri":"/k8s_model/"},{"categories":["Cloud Native"],"content":"前言 Kubernetes有很多好处。 同时，当组织采用云原生技术时，它可能变得复杂。 Kubernetes成熟度模型的存在可帮助您确定自己在迁移到原生云的过程中所处的位置，无论您是Kubernetes的新手还是有部署经验的人。 这是一个重要的工具，可帮助您自我确定您所处的阶段，了解环境中的差距并获得有关增强和改善Kubernetes技术栈的见解。 ","date":"2021-01-28","objectID":"/k8s_model/:1:0","tags":["kubernetes","translate"],"title":"[译]Kubernetes成熟度模型","uri":"/k8s_model/"},{"categories":["Cloud Native"],"content":"如何使用Kubernetes成熟度模型 Kubernetes和您的工作负载在不断变化。 使用此成熟度模型时，请知道，如果确实达到某个阶段，则可能仍需要重新访问以前的阶段。 另外，请注意，Kubernetes的成熟并非一朝一夕就能完成，而是需要时间。 Kubernetes成熟度模型应用作工具，以帮助您了解在迁移到云原生过程中需要集中精力或需要帮助的地方。 ","date":"2021-01-28","objectID":"/k8s_model/:1:1","tags":["kubernetes","translate"],"title":"[译]Kubernetes成熟度模型","uri":"/k8s_model/"},{"categories":["Cloud Native"],"content":"1. 准备阶段 从哪里开始？如何证明k8s的价值？谁可以信任？ 在该阶段，你将学习/精通以下内容： 云原生和k8s将如何帮助推动业务和技术目标。它将耗费什么？并就整个组织的目标达成共识。 明白云原生、容器、以及k8s的价值 能够向企业领导者描述该价值 得到团队，领导和整个组织的支持 ","date":"2021-01-28","objectID":"/k8s_model/:2:0","tags":["kubernetes","translate"],"title":"[译]Kubernetes成熟度模型","uri":"/k8s_model/"},{"categories":["Cloud Native"],"content":"必要条件 明白你的问题 为什么要使用kubernetes？想要通过kubernetes解决什么问题？ 同意使用OSS 转换到kubernetes需要你明白开源软件(OSS)在云原生生态中的角色和能量 接受投资未来 kubernetes的旅程将会耗费大量的时间和金钱。你需要面向未来投资。 ","date":"2021-01-28","objectID":"/k8s_model/:2:1","tags":["kubernetes","translate"],"title":"[译]Kubernetes成熟度模型","uri":"/k8s_model/"},{"categories":["Cloud Native"],"content":"介绍 在采用Kubernetes时，第一步是准备工作。在这里，理解并能够阐明云原生和Kubernetes为什么对组织很重要至关重要。一些核心概念包括理解云原生计算，容器和Kubernetes的价值和影响。在较高的层次上，我们在这里每个定义。 Cloud Native 云原生技术有利于各组织在公有云、私有云和混合云等新型动态环境中，构建和运行可弹性扩展的应用。云原生的代表技术包括容器、服务网格、微服务、不可变基础设施和声明式API。 这些技术能够构建容错性好、易于管理和便于观察的松耦合系统。结合可靠的自动化手段，云原生技术使工程师能够轻松地对系统作出频繁和可预测的重大变更。 云原生计算基金会（CNCF）致力于培育和维护一个厂商中立的开源生态系统，来推广云原生技术。我们通过将最前沿的模式民主化，让这些创新为大众所用。 Source: CNCF definition. 云原生计算的好处包括更快的发布速度，易于管理，通过容器化和云标准降低了成本，能够构建更可靠的系统，避免了供应商锁定以及改善了客户应用体验。 Container 一个打包代码及其所有依赖项的标准软件单元，使得应用程序可以从一个计算环境快速可靠地运行到另一个计算环境 Source: Docker 在k8s中，你运行的每个容器都是可重复的；通过包含依赖项实现标准化意味着无论您在哪里运行它，都可以得到相同的行为。容器将应用程序与基础主机基础结构分离。这使得在不同的云或OS环境中的部署更加容易 Source: CNCF concepts. Kubernetes Kubernetes是一个可移植的，可扩展的开源平台，用于管理容器化的工作负载和服务，可促进声明式配置和自动化。 它拥有一个庞大且快速增长的生态系统。 Kubernetes的服务，支持和工具广泛可用。Kubernetes为您提供了一个可弹性运行分布式系统的框架。 它负责应用程序的扩展和故障转移，提供部署模式等。 Source: CNCF What is Kubernetes. Infrastructure as code (IAC) 使用配置语言配置和管理基础结构的能力。 IAC为网络，负载平衡器，虚拟机，Kubernetes集群和监视等基础架构的管理带来了现代软件开发的可重复性，透明性和测试。 IAC的主要目标是减少错误和配置漂移，同时允许工程师将时间花在更高价值的任务上。 IAC定义了基础架构的最终状态，而不是定义要执行的一系列步骤-像Terraform这样的IAC工具可以针对您的基础架构多次运行，从而产生相同的预期结果。 使用云用户界面创建托管的Kubernetes集群是一个相对简单的过程，但是使用基础结构作为代码有助于标准化集群配置并管理集群节点的附件，例如网络策略，维护窗口以及身份和访问管理（IAM） 和工作量。 ","date":"2021-01-28","objectID":"/k8s_model/:2:2","tags":["kubernetes","translate"],"title":"[译]Kubernetes成熟度模型","uri":"/k8s_model/"},{"categories":["Cloud Native"],"content":"公司和文化的改变 云原生实践，容器和Kubernetes的采用可以有助于实现战略业务目标，包括通过适当的功能来帮助最大程度地节省成本，适应需求，降低和分散风险。 它还代表了整个组织的文化变化。在准备进行转型时，您需要团队，领导层和整个组织的支持，因为这将需要不同的技能，新的团队结构和改变的意愿。 ","date":"2021-01-28","objectID":"/k8s_model/:2:3","tags":["kubernetes","translate"],"title":"[译]Kubernetes成熟度模型","uri":"/k8s_model/"},{"categories":["Cloud Native"],"content":"挑战 在准备阶段，可能面临的挑战： 从何处开始或谁信任和倾听的不确定性。 不确定您的用例是否可行或可转换到云原生/ Kubernetes世界 需要证明业务价值/领导成本 ","date":"2021-01-28","objectID":"/k8s_model/:2:4","tags":["kubernetes","translate"],"title":"[译]Kubernetes成熟度模型","uri":"/k8s_model/"},{"categories":["Cloud Native"],"content":"输出 您的技术计划与业务/领导力支持相辅相成，以取得成功。 您接受并交流文化和跨职能工作流程的变化。 您接受这一旅程将需要大量的投资和对新技术的信任。 您了解开源软件（OSS）在云原生生态系统中的作用和功能。 您已经考虑并确认了希望获得的价值 你知道你的业务重点。 您了解有关安全性，效率和可靠性的要求。 ","date":"2021-01-28","objectID":"/k8s_model/:2:5","tags":["kubernetes","translate"],"title":"[译]Kubernetes成熟度模型","uri":"/k8s_model/"},{"categories":["Cloud Native"],"content":"学习资源 Deep Dive into Core Concepts Kubernetes Basic Training Why Infrastructure as Code and Kubernetes How to Create, View and Destroy a Pod in Kubernetes ","date":"2021-01-28","objectID":"/k8s_model/:2:6","tags":["kubernetes","translate"],"title":"[译]Kubernetes成熟度模型","uri":"/k8s_model/"},{"categories":["Cloud Native"],"content":"2. 改造阶段 如何设置Kubernetes基础架构并转移工作负载？ 在该阶段，你将学习/精通以下内容： Kubernetes的基础知识以及采用和转换现有思维方式，工作流程和实践到平台的能力。 你将精通kubernetes术语，并能够将现有技术映射到云原生环境 你将容器化你的应用并把工作负载转移到kubernetes上 你将清理你的技术债务并避免把它带到kubernetes上 ","date":"2021-01-28","objectID":"/k8s_model/:3:0","tags":["kubernetes","translate"],"title":"[译]Kubernetes成熟度模型","uri":"/k8s_model/"},{"categories":["Cloud Native"],"content":"必要条件 理解核心概念 你将为采用kubernetes做花费时间理解核心概念的准备。 全组织范围接受 您可以在整个组织中进行购买，以投资Kubernetes概念证明或将Kubernetes用于所有新应用程序。 优先级工作负载 您了解使用分阶段方法计划迁移到Kubernetes的工作负载 ","date":"2021-01-28","objectID":"/k8s_model/:3:1","tags":["kubernetes","translate"],"title":"[译]Kubernetes成熟度模型","uri":"/k8s_model/"},{"categories":["Cloud Native"],"content":"介绍 转变是您迁移到Kubernetes的阶段。在该阶段，你将通过部署你的第一个集群和负载来验证您的基础知识和理解。在改造阶段，您应该对基础知识有所准备，但同时可能缺乏完成该阶段所需的专业知识。 你将花费大量的时间在该阶段。进行一些关键活动时，它涵盖了您的初始实施，迁移和学习曲线。当您采用Kubernetes时，不要被“启动并运行”的文章所迷惑。在设置集群和准备生产之间存在功能差距。您可能会发现在此阶段进行Kubernetes概念验证或与Kubernetes专家合作以确保您正在设置第一个集群以满足工作负载的需求很有帮助。 ","date":"2021-01-28","objectID":"/k8s_model/:3:2","tags":["kubernetes","translate"],"title":"[译]Kubernetes成熟度模型","uri":"/k8s_model/"},{"categories":["Cloud Native"],"content":"采用新的语言和架构体系 当你要采用kubernetes时，你不只是做简单的准备，你将开始练习和理解新的语言、架构和负载需求。 术语学习 在准备阶段，你可能已经明白了一些术语，但在改造阶段期间，你将需要使用所有的kubernetes术语。一些必要的概念将包含在内，包括node、pod、namespace、ReplicaSet、控制器等等。尽管事先进行培训很重要，但是在实践中您会接触到每个功能时，您会更好地理解它们。 应用架构和理解需求 你需要将应用架构映射到新的云原生环境中。这样做将帮助您发现需求并发现应用程序的依赖关系，以便您可以成功拥抱容器。它还将使您能够重新查看以前的历史假设和决策。例如： Old World New World SSH to server Deploy by code and immutable infrastructure Dedicated workloads Self-healing and ephemeral workloads Add server to scale Add a pod or nod to scale Configuration management for deployment Containerization for deployment 工作负载理解 你将理解运行在集群中内不同类型的工作负载：Deployments、Jobs、CronJobs、ReplicaSet、DaemonSets和Pods ","date":"2021-01-28","objectID":"/k8s_model/:3:3","tags":["kubernetes","translate"],"title":"[译]Kubernetes成熟度模型","uri":"/k8s_model/"},{"categories":["Cloud Native"],"content":"技术改造 在第二阶段，你将开始在kubernetes上运行工作负载。为了成功应对技术改造，有10个主要的步骤需要执行。这是您将要执行的每个步骤的简要概述。准备在此过程的每个步骤上花费大量时间。 深入理解和项目计划 - 无论您是在本地，在数据中心还是已经迁移到云，您的第一步是深入研究现有技术栈。您需要调查技术栈的各个方面，从基础网络，基础结构，配置，密钥管理到如何部署应用程序及其依赖项。迁移到Kubernetes时，您需要确定对技术的要求。该步骤帮助你避免遗漏重要的依赖。在这次深入研究的基础上，您可以制定一个项目计划，这是您进行迁移的路线图。 应用容器化 - 您的应用程序可能已经被容器化了，在这种情况下，您可以继续执行第三步了。如果不是，则需要根据 twelve-factor app 方法细分应用程序。这非常重要，因为您将需要使应用程序能够生存下去（您的容器随时可能被杀死）。您需要能够干净地站立您的应用程序和容器。在此步骤中，我们建议您从构建工件中提取密钥和配置。 Kubernetes是短暂的，因此，您可以维护标准和安全性，并在容器运行时简单地注入密钥和配置。 建立云基础设施 - 你需要确认你的云服务提供商：AWS、GCP、Azure或者像EKS、GKS和AKS的kubernetes管理服务。如果你选择了kubernetes管理服务，你就不需要额外的工作用于构建你自己的kubernetes基础设施。此步骤的一部分，您需要设置基础云配置，VPC，安全组，身份验证和授权等。 构建Kubernets基础设施 - 在该步骤，需要考虑一些设计注意事项，以避免做出可能需要耗时的集群重建或网络和成本影响的选择。类似的注意事项有：您应该在哪些区域拥有多少个群集，并具有多少个可用区（AZ）？需要多少个单独的环境，集群和命名空间？服务之间应如何相互通信和发现？安全性是在VPC，集群还是Pod级别上？您的重点应该放在可重复性上。您将需要利用基础架构作为代码（IaC），以便可以以一种重复的方式构建集群。在此步骤中，请谨慎使用第一步中的深度研究/项目计划中的配置选项，以确保您不会错过应用程序要求。 编写YAML或者Helm charts - 在Fairwinds，我们将此步骤称为Kubernating。在这里定义Kubernetes资源以将其放入集群中。可以在此处编写Kubernetes资源YAML文件，但是现在大多数都使用Helm charts将应用程序部署到Kubernetes中。您将专门为您的容器镜像，配置映射模板，密钥或任何特殊应用程序要求编写YAML或Helm charts。 深入了解外部云依赖性 - 你的应用可能会有外部的依赖，例如key store，库、数据库或者其他资产。Kubernetes不是这些依赖生存的好地方。您将需要在Kubernetes之外管理您的有状态依赖关系。例如，您可以使用Amazon RDS之类的工具中的独立数据库，然后将其放入Kubernetes中。然后，您的应用程序可以在Kubernetes的pod中运行，并与这些依赖项对话。 定义Git工作流程 - Kubernetes的一个主要优点是能够以可重复的方式部署代码而无需人工干预。您通常会通过Git将代码提交到代码仓库，这将启动事件并与将这些更改移动到非生产集群的分支合并。然后，您将对代码进行测试和质量检查，然后合并到master中。这会将您的代码部署到登台或生产中。在此阶段，您只需定义Git工作流的外观即可，即当开发人员推送代码时，Kubernetes会发生什么？ 构建CI/CD管道 - 定义Git工作流程后，您将使用Jenkins或CircleCI等自动化工具来设置CI / CD平台。这会将您定义的工作流程转变为实际的构建管道。 非生产测试 - 完成单片式应用程序或微服务架构的步骤1-8之后，您将部署到非生产环境。在这里，您将使用该应用程序以确保其运行，具有足够的资源和限制，测试您的机密信息是否正确以及人们可以访问该应用程序。您将测试杀死pod会发生什么情况。从本质上讲，您将在开始生产之前踢一下轮胎。如果您运行的是整体应用程序，则可以更快地完成此阶段。如果要部署微服务应用程序体系结构，则将为每个服务完成步骤1-8，然后将其部署到非生产环境。一旦所有服务都存在，您就可以看到它们如何协同工作以确保您的应用程序一旦投入生产，应用是正常工作的。 生产推广 - 最后，一旦您在非生产环境中对应用程序进行了全面测试并对此感到满意，只要您构建的生产环境与登台相同，就可以部署流量并将其发送到您的应用程序。在这里，您只需更改负载平衡器或DNS。使用DNS，您可以根据需要进行故障回复。 ","date":"2021-01-28","objectID":"/k8s_model/:3:4","tags":["kubernetes","translate"],"title":"[译]Kubernetes成熟度模型","uri":"/k8s_model/"},{"categories":["Cloud Native"],"content":"债务清理 在实施Kubernetes时，您将有机会清理现有系统中产生的技术债务。 技术债务可以概括为持续吸引人们注意力以完成任务的任何工作流，流程，代码或硬件。 这可能包括已推迟的升级或更新，已解决的代码错误，旧版本依赖性或错误的配置。 自然地，当您迁移到Kubernetes时，您可以评估该技术债务的存在位置，从而避免在新环境中复制它。 ","date":"2021-01-28","objectID":"/k8s_model/:3:5","tags":["kubernetes","translate"],"title":"[译]Kubernetes成熟度模型","uri":"/k8s_model/"},{"categories":["Cloud Native"],"content":"生产力增加与损失 Kubernetes提供了机会来更改和改进团队协作以及交付应用程序或服务的方式。随着您的转变，您的团队也会发生变化。 Kubernetes是一个巨大的变化。您将采用一种新的工作方式，团队中的每个人都会以不同的方式学习该技术。开始时，随着您和您的团队对技术的适应，生产率将会受到影响。在此阶段要有耐心，因为长期的生产率收益将超过短期的损失。 ","date":"2021-01-28","objectID":"/k8s_model/:3:6","tags":["kubernetes","translate"],"title":"[译]Kubernetes成熟度模型","uri":"/k8s_model/"},{"categories":["Cloud Native"],"content":"工具 您需要决定如何做出有关工具的决定，包括： 您如何确定哪些问题需要工具解决方案 谁负责做出该决定？ 您如何审核工具？ 对于开源工具，您需要评估每个工具是否定期更新，以及是否有足够的社区支持来避免工具过时。在此阶段花时间与使用Kubernetes的开发人员合作回答这些问题。 ","date":"2021-01-28","objectID":"/k8s_model/:3:7","tags":["kubernetes","translate"],"title":"[译]Kubernetes成熟度模型","uri":"/k8s_model/"},{"categories":["Cloud Native"],"content":"在控制与最终灵活性之间取得平衡 Kubernetes的一个好处是开发人员无需操作团队即可提交代码的能力。 所需要的是好的政策，以确保安全性，可靠性和效率。 挑战在于，在设置第一个集群时，需要在为开发人员提供最终灵活性或控制基础架构之间取得平衡。 您是否会在一开始就向开发人员开放Kubernetes，以免影响生产力？ 还是会锁定系统？ 您将使用哪些配置来限制更改？ 您必须考虑如何平衡开发人员需求和公司政策。 ","date":"2021-01-28","objectID":"/k8s_model/:3:8","tags":["kubernetes","translate"],"title":"[译]Kubernetes成熟度模型","uri":"/k8s_model/"},{"categories":["Cloud Native"],"content":"挑战 在转换期间，您可能会面临以下挑战： 被复杂性淹没 庞大的Kubernetes生态系统导致决策瘫痪 缺乏内部专业知识或人才流失 Kubernetes最佳实践/执行的不确定性 这些挑战是任何IT环境的常见疑问。使用Kubernetes时也是如此，但是在这一领域的人才或专业知识甚至更少。如果出现这些挑战，您可以从托管的Kubernetes服务寻求帮助。 另外，您的团队中存在现有的行为。花时间了解哪些行为至关重要，哪些行为可以更改，哪些历史不再需要，何处存在可以简化的复杂性。 ","date":"2021-01-28","objectID":"/k8s_model/:3:9","tags":["kubernetes","translate"],"title":"[译]Kubernetes成熟度模型","uri":"/k8s_model/"},{"categories":["Cloud Native"],"content":"输出 您将在转换阶段投入大量时间并建立您的知识库 您将确定在Kubernetes中使用哪些应用程序 您将考虑整体解决方案与开放源代码和专有技术的拼凑而成 您将了解Kubernetes的基础知识，优点和缺点 您将成功进行Kubernetes的概念验证，因此您将能够决定是否继续 ","date":"2021-01-28","objectID":"/k8s_model/:3:10","tags":["kubernetes","translate"],"title":"[译]Kubernetes成熟度模型","uri":"/k8s_model/"},{"categories":["Cloud Native"],"content":"学习资源 Why Managed Services with EKS, GKE or AKS Strengths and Weaknesses of AKS, EKS and GKE Intro to Kubernetes + How to Deploy a Multi-tiered Web Application ","date":"2021-01-28","objectID":"/k8s_model/:3:11","tags":["kubernetes","translate"],"title":"[译]Kubernetes成熟度模型","uri":"/k8s_model/"},{"categories":["Cloud Native"],"content":"3. 部署阶段 实施流程，CI / CD，赋予开发人员权力并引入一些监控和可观察性。 在该阶段，你将学习/精通以下内容： Kubernetes实现了基线理解。您正在通过进行基本的开发，部署，管理和故障排除来练习技能。你将： 为您的Kubernetes工作负载奠定坚实的基础 准备在您的组织中部署Kubernetes 通过CI / CD实施构建和部署过程，并引入了一些有限的监控和可观察性。 使开发人员能够自助 ","date":"2021-01-28","objectID":"/k8s_model/:4:0","tags":["kubernetes","translate"],"title":"[译]Kubernetes成熟度模型","uri":"/k8s_model/"},{"categories":["Cloud Native"],"content":"必要条件 应用容器化/kubernetes设置 您将采用容器并成功运行Kubernetes POC或建立基础架构。 在预发环境运行工作负载 您将在预生产中拥有工作负载，或者可能具有某些生产级部署。 建立的过程 您将建立初始的Git工作流程，建立CI / CD管道并建立适当的测试流程。 ","date":"2021-01-28","objectID":"/k8s_model/:4:1","tags":["kubernetes","translate"],"title":"[译]Kubernetes成熟度模型","uri":"/k8s_model/"},{"categories":["Cloud Native"],"content":"介绍 当您达到此阶段时，您和您的团队将掌握基本知识。现在，一个应用程序或服务正在生产中运行，可以正确地了解外部依赖关系，通过负载平衡器将流量路由到Kubernetes，并且可以访问日志记录和指标。您还将拥有自动缩放功能。 Kubernetes成熟度模型的这一阶段让您承担所有工作，包括实施构建和部署过程，设置CI / CD，赋予开发人员权力以及引入一些有限的监视和可观察性。 您将在此阶段熟悉所有Kubernetes基础知识。这很重要，因此您可以建立坚实的基础。您几乎已经超过了学习曲线的驼峰。 ","date":"2021-01-28","objectID":"/k8s_model/:4:2","tags":["kubernetes","translate"],"title":"[译]Kubernetes成熟度模型","uri":"/k8s_model/"},{"categories":["Cloud Native"],"content":"CI/CD 您将开始建立结构化的构建和部署过程，以展示云和容器本机CI / CD系统的质量。 CI / CD将成为用于构建和部署的经过功能验证的工作流程。 您和您的团队可以半可靠地将代码交付给工作和生产环境，可以通过Git分支和标签管理部署，并可以判断构建和部署是否成功。 范围更广的团队了解部署的结果，包括应更改的内容以及在何处可以到达端点。最后，随着部署过程的成熟，CI / CD将有助于在部署过程的早期发现问题。 ","date":"2021-01-28","objectID":"/k8s_model/:4:3","tags":["kubernetes","translate"],"title":"[译]Kubernetes成熟度模型","uri":"/k8s_model/"},{"categories":["Cloud Native"],"content":"赋予开发人员和运维人员权力 您的开发人员应有权部署到Kubernetes。为了使之成为可能，您的团队将对从源代码管理（scm）到部署的工作流程有基本的了解，并有权访问scm中的合并/标记提交以触发部署。开发人员需要能够执行CI / CD流程的基本调试，并具有对Kubernetes资源定义或Helm图表的kubectl访问。 同时，您将希望操作员以及开发人员可以访问Kubernetes API，以确保他们了解工作负载在何处以及如何查看其状态。 这是确保他们能够调试在Kubernetes中遇到的简单错误或阻止程序的重要部分。 为确保Kubernetes取得成功，请在此阶段花费时间进行培训，并授权所有开发人员和操作团队进行这些活动。 ","date":"2021-01-28","objectID":"/k8s_model/:4:4","tags":["kubernetes","translate"],"title":"[译]Kubernetes成熟度模型","uri":"/k8s_model/"},{"categories":["Cloud Native"],"content":"用户基础 在此阶段，您的团队将能够操作Kubernetes的基础知识。这包括学习和执行Kubernetes操作基础知识，包括： 将操作员连接到Kubernetes API 了解如何列出和查看资源 执行基本动作（对动作如何理解有限的机械动作） ","date":"2021-01-28","objectID":"/k8s_model/:4:5","tags":["kubernetes","translate"],"title":"[译]Kubernetes成熟度模型","uri":"/k8s_model/"},{"categories":["Cloud Native"],"content":"有限的监控和可视化 您需要运营商和开发人员将监视和可观察性纳入他们的工作负载中。有流行的开源工具，例如Prometheus，还有来自Datadog等供应商的工具。在基础阶段，您将开始使用这些工具，对群集的监视和可观察性有限。您将开始了解工作量和基础结构的哪些方面需要深入了解或保持关注。 ","date":"2021-01-28","objectID":"/k8s_model/:4:6","tags":["kubernetes","translate"],"title":"[译]Kubernetes成熟度模型","uri":"/k8s_model/"},{"categories":["Cloud Native"],"content":"策略\u0026审核工具 在基础阶段结束时，您和您的团队将寻求工具来帮助您成熟的Kubernetes环境。这将包括探索开源工具open source tooling ，以帮助解决安全性，策略管理，工作负载配置错误，资源请求和限制等问题。您还可以探索来自软件供应商的工具，这些工具为Kubernetes提供策略驱动的解决方案。policy-driven solutions for Kubernetes. ","date":"2021-01-28","objectID":"/k8s_model/:4:7","tags":["kubernetes","translate"],"title":"[译]Kubernetes成熟度模型","uri":"/k8s_model/"},{"categories":["Cloud Native"],"content":"挑战 在基础阶段，您可能会面临以下挑战： 新的概念和技能可能会使团队进度缓慢或陷入困境 曾经工作的东西不太一样 尚未深入了解，一些问题似乎无法解决或令人沮丧 你有很多问题，没人可以解答 您可能会花费时间询问Kubernetes的问题是否可以解决，从生态系统寻求帮助或寻求Kubernetes专家的帮助。 ","date":"2021-01-28","objectID":"/k8s_model/:4:8","tags":["kubernetes","translate"],"title":"[译]Kubernetes成熟度模型","uri":"/k8s_model/"},{"categories":["Cloud Native"],"content":"输出 你已经为Kubernetes工作负载奠定了坚实的基础 您有信心并准备在整个组织中部署Kubernetes 企业看到了Kubernetes的价值 ","date":"2021-01-28","objectID":"/k8s_model/:4:9","tags":["kubernetes","translate"],"title":"[译]Kubernetes成熟度模型","uri":"/k8s_model/"},{"categories":["Cloud Native"],"content":"学习资源 Kube 101 Training Best Practices for Implementing CI/CD Pipelines in Kubernetes Kubernetes: What Needs Monitoring and Why Most Helpful Kubernetes Open Source Projects We Use ","date":"2021-01-28","objectID":"/k8s_model/:4:10","tags":["kubernetes","translate"],"title":"[译]Kubernetes成熟度模型","uri":"/k8s_model/"},{"categories":["Cloud Native"],"content":"4. 建立自信阶段 建立对自己的核心能力的信心，以成功地定期部署和发布功能。 在该阶段，你将学习/精通以下内容： 建立核心能力，以成功地定期部署和发布功能。您正在建立更深刻的理解，从而导致更多的自定义，实验和更广泛的组织使用。您将： 通过经验建立信心 围绕基础设施即代码和配置建立核心标准 选择可以紧密协作的Kubernetes附加组件 开始使用监控工具来帮助您了解服务挑战 ","date":"2021-01-28","objectID":"/k8s_model/:5:0","tags":["kubernetes","translate"],"title":"[译]Kubernetes成熟度模型","uri":"/k8s_model/"},{"categories":["Cloud Native"],"content":"必要条件 在生产环境运行工作负载 现在，你已经运行部分或者全部的工作负载在生产环境 实施CI/CD 您已经建立了CI / CD管道，它是经过功能验证的工作流程，用于构建和部署 初始化监控和可视化 您已经开始使用监视和可观察性工具来确定Kubernetes基础架构中的差距。 ","date":"2021-01-28","objectID":"/k8s_model/:5:1","tags":["kubernetes","translate"],"title":"[译]Kubernetes成熟度模型","uri":"/k8s_model/"},{"categories":["Cloud Native"],"content":"介绍 随着Kubernetes环境的成熟，您将奠定坚实的基础。现在，当您进入成熟度模型的第四阶段时，就该建立信心了。在第三阶段，您已经建立并运行了Kubernetes基础架构。在第四阶段，您将开始了解Kubernetes的细微差别。 重要的是要记住，建立信心将需要时间，因为您重复执行任务并经历类似的情况。 ","date":"2021-01-28","objectID":"/k8s_model/:5:2","tags":["kubernetes","translate"],"title":"[译]Kubernetes成熟度模型","uri":"/k8s_model/"},{"categories":["Cloud Native"],"content":"每天都有新的理解 想象你是刚刚学习骑自行车的时候，您可能可以直线骑行。建立信心时，您将开始转弯，越过颠簸，甚至在没有握住把手的情况下骑行。在第三阶段中，您学习了如何骑自行车，现在在第四阶段中，当您通过经验了解细微差别时，每天都在学习新的提示和技巧。 您会在某些方面感到很自在，但对其他方面则缺乏信心。例如，随着您对活动性和就绪性探针的熟悉，您将了解微小的配置更改如何改变工作负载的行为。您将更有能力进行更改以产生积极的影响。另一个示例是您可能已经制定了安全策略，但是不能确保所有群集都具有符合要求的配置。 在此阶段，您将了解这些细微差别，然后开始在团队中培训其他人。这将帮助您进一步建立信心。 当您建立信心时，您的好奇心将围绕您迄今为止所经历的事情以及如何进行改进而增加。如果您的Kubernetes集群出现故障，您将进行试验而不是惊慌。 ","date":"2021-01-28","objectID":"/k8s_model/:5:3","tags":["kubernetes","translate"],"title":"[译]Kubernetes成熟度模型","uri":"/k8s_model/"},{"categories":["Cloud Native"],"content":"胜任力 您在选择和实施第三方工具以增强Kubernetes体验方面的核心能力将会增强。您将研究有助于提高安全性的工具(Trivy, Polaris, Kubesec)，设置正确的资源利用率(Goldilocks)或帮助进行升级 (Nova)等等。 您还可以胜任正在执行的非标准活动，以及已更改和自定义的默认活动。 ","date":"2021-01-28","objectID":"/k8s_model/:5:4","tags":["kubernetes","translate"],"title":"[译]Kubernetes成熟度模型","uri":"/k8s_model/"},{"categories":["Cloud Native"],"content":"排查故障 您还将对故障排除更有信心。也许您的应用程序在每次部署时都会崩溃，而您却不知道为什么。您将开始研究配置并研究资源。以前您知道这个概念，但是您不了解它对应用程序的影响。现在，您发现没有为应用程序提供足够的资源来启动它。 您现在有一个可重复进行的故障排除周期，因此更改可以快速完成并反复进行，直到它们起作用为止。 ","date":"2021-01-28","objectID":"/k8s_model/:5:5","tags":["kubernetes","translate"],"title":"[译]Kubernetes成熟度模型","uri":"/k8s_model/"},{"categories":["Cloud Native"],"content":"挑战 在基础阶段，您可能会面临以下挑战： 担心单点故障和故障排除 团队实践不统一 对所做的决定感到遗憾 功能缺失 令人失望的表现 对您的Kubernetes集群充满信心与经验有关。但是，许多团队缺乏学习工作所需的时间。培训，专业和托管服务，审计和配置验证可在此提供帮助。 ","date":"2021-01-28","objectID":"/k8s_model/:5:6","tags":["kubernetes","translate"],"title":"[译]Kubernetes成熟度模型","uri":"/k8s_model/"},{"categories":["Cloud Native"],"content":"输出 通过经验以及Kubernetes专家培训，专业和托管服务，审计和配置验证的帮助，您已经对Kubernetes集群建立了信心。 您正在尝试使用Kubernetes，因为您已经实现了有关基础架构的代码（IaC）和配置标准 您开始监视和了解您的服务挑战 您可以放心地选择加载项，但是在部署和管理所有工具时会遇到挑战 ","date":"2021-01-28","objectID":"/k8s_model/:5:7","tags":["kubernetes","translate"],"title":"[译]Kubernetes成熟度模型","uri":"/k8s_model/"},{"categories":["Cloud Native"],"content":"学习资源 Kubernetes Audit and Improve 5 Kubernetes Security Tools You Should Use Managing Kubernetes Configuration for Security, Reliability and Efficiency ","date":"2021-01-28","objectID":"/k8s_model/:5:8","tags":["kubernetes","translate"],"title":"[译]Kubernetes成熟度模型","uri":"/k8s_model/"},{"categories":["Cloud Native"],"content":"5. 改进操作阶段 提升集群的安全性、可靠性和效率 在该阶段，你将学习/精通以下内容： 在整个企业中成功部署Kubernetes 通过花费时间配置集群来提高安全性，效率和可靠性 你的团队可以聚焦于业务，而不是维护kubernetes 您的挑战现在很复杂，需要您的内部团队以外的Kubernetes专业知识。 ","date":"2021-01-28","objectID":"/k8s_model/:6:0","tags":["kubernetes","translate"],"title":"[译]Kubernetes成熟度模型","uri":"/k8s_model/"},{"categories":["Cloud Native"],"content":"必要条件 kubernetes自信 你以前通过经验建立了在kubernets上的自信 标准实施 你已经建立了CI/CD，IaC和配置的标准 开始监控 您开始监视和了解您的服务挑战。 ","date":"2021-01-28","objectID":"/k8s_model/:6:1","tags":["kubernetes","translate"],"title":"[译]Kubernetes成熟度模型","uri":"/k8s_model/"},{"categories":["Cloud Native"],"content":"介绍 到达kubernetes成熟度模型的该阶段是一个标志性的里程碑。您将定期成功地将功能部署和交付到Kubernetes中。您的开发人员将会满意： 预计它们会影响Kubernetes的术语，例如Deployment，Daemon Set，Service或Namespace 修改Kubernetes资源的某些配置，例如ConfigMap或Helm图表 对CI / CD过程进行故障排除 对Kubernetes中的应用程序/服务进行故障排除，包括访问日志和事件以及检索指标/监控 您不仅拥有基础知识，而且现在可以专注于改善运营。 ","date":"2021-01-28","objectID":"/k8s_model/:6:2","tags":["kubernetes","translate"],"title":"[译]Kubernetes成熟度模型","uri":"/k8s_model/"},{"categories":["Cloud Native"],"content":"安全、效率和可靠 为了使组织成功采用Kubernetes，您将需要花费时间来提高安全性，效率和可靠性。重要的是要了解有关这些主题的配置。 安全 您需要确定谁负责Kubernetes集群安全以及如何对其进行管理。您是否可以快速识别配置错误，从而在容器和Kubernetes实施中留下安全漏洞？ 效率 Kubernetes是否有效运行？谁负责监视资源利用率，以确保您不会过度配置资源或配置资源不足。您的应用程序或服务的范围是什么？ 可靠 Kubernetes是否会带来任何停机挑战？系统可靠吗？您是否实现了所有的自我修复，自动缩放功能，并且没有引入配置问题？ 这些领域中的每一个都需要您和您的团队制定策略并建立方法，以轻松确保在整个集群中实施这些策略。策略驱动的配置验证可以帮助： 在CI / CD阶段通过开放策略代理（OPA）集成或作为准入控制器来实施自定义策略 通过在应用程序开发期间检测问题来防止错误，从而防止错误首先进入生产环境 通过自动扫描容器中的漏洞并审核群集中的漏洞来节省时间 通过确定如何提高Kubernetes计算资源的效率来降低成本 ","date":"2021-01-28","objectID":"/k8s_model/:6:3","tags":["kubernetes","translate"],"title":"[译]Kubernetes成熟度模型","uri":"/k8s_model/"},{"categories":["Cloud Native"],"content":"挑战 与每个成熟阶段一样，您会遇到新的痛点。您可能会遇到以下挑战： 复杂性现在不在您的舒适范围内 维护和运营工作量很大，并花费了团队时间和精力 团队担心人员不足，无聊，分心或技能不足以应对更深的Kubernetes挑战 达到这一成熟水平时，克服这些挑战可能会很复杂。它可能需要内部雇用专家，但是预算可能不存在。这是培训，专业和托管服务，审计和配置验证可以提供帮助的另一个领域。 ","date":"2021-01-28","objectID":"/k8s_model/:6:4","tags":["kubernetes","translate"],"title":"[译]Kubernetes成熟度模型","uri":"/k8s_model/"},{"categories":["Cloud Native"],"content":"输出 您在Kubernetes成熟度上取得了重要的里程碑 通过花费时间配置群集来提高安全性，效率和可靠性 您的团队将能够专注于您的业务，而不是维护Kubernetes 第五阶段不是终点线。如果不进行第6阶段和第7阶段，您将永远不会意识到Kubernetes的全部好处和价值。 ","date":"2021-01-28","objectID":"/k8s_model/:6:5","tags":["kubernetes","translate"],"title":"[译]Kubernetes成熟度模型","uri":"/k8s_model/"},{"categories":["Cloud Native"],"content":"学习资源 Kubernetes Audit and Improve Kubernetes Best Practices for Security Closing the Kubernetes Services Gap Kubernetes Best Practices for Reliability Kubernetes Best Practice for Efficient Resource Utilization ","date":"2021-01-28","objectID":"/k8s_model/:6:6","tags":["kubernetes","translate"],"title":"[译]Kubernetes成熟度模型","uri":"/k8s_model/"},{"categories":["Cloud Native"],"content":"6. 测量\u0026控制 通过复杂的监控来推动策略和控制，从而对工作负载有更深入的了解。 在该阶段，你将学习/精通以下内容： 复杂的监视和警报功能可让您对工作负载有更深入的功能了解。您将： 围绕允许的行为，安全性，配置和标准，提出更强的见解和更严格的控制 将基础结构作为代码和CI / CD驱动的流程加倍 探索网络策略，工作负载标识和服务网格以锁定工作负载能力 ","date":"2021-01-28","objectID":"/k8s_model/:7:0","tags":["kubernetes","translate"],"title":"[译]Kubernetes成熟度模型","uri":"/k8s_model/"},{"categories":["Cloud Native"],"content":"必要条件 部署和发布 您定期成功部署和发布功能 集群配置 您正在花费时间通过评估群集来提高安全性，效率和可靠性 团队聚焦于业务 您的Kubernetes基础架构是生产级的（技术部门有限），使您的团队可以专注于您的应用程序或服务 ","date":"2021-01-28","objectID":"/k8s_model/:7:1","tags":["kubernetes","translate"],"title":"[译]Kubernetes成熟度模型","uri":"/k8s_model/"},{"categories":["Cloud Native"],"content":"介绍 Kubernetes成熟度的下一阶段是引入更多的环境测量和控制。 您和您的团队在Kubernetes中运作良好，具有全面的了解，并在整个组织范围内得到采用。 您正在对Kubernetes进行更深入的功能理解，并就如何在集群和整个环境中完成工作提出了意见。 此外，团队已准备好解决先前阶段的技术债务。 先前的阶段已经引入了一些监视和可观察性。在此阶段，您将收集并处理更多数据，见解和工具，以开始了解要测量和跟踪的内容以及如何控制Kubernetes。 ","date":"2021-01-28","objectID":"/k8s_model/:7:2","tags":["kubernetes","translate"],"title":"[译]Kubernetes成熟度模型","uri":"/k8s_model/"},{"categories":["Cloud Native"],"content":"更复杂的监视和警报 现在，使用更复杂的监视和警报是帮助了解常见问题，Kubernetes错误以及如何解决它们的重点。 您将更熟悉如何构建监视仪表板以在需要帮助之前发现问题和常见的错误配置。 当问题发生时，您也将知道它们的大致位置，以便更轻松地进行故障排除。 ","date":"2021-01-28","objectID":"/k8s_model/:7:3","tags":["kubernetes","translate"],"title":"[译]Kubernetes成熟度模型","uri":"/k8s_model/"},{"categories":["Cloud Native"],"content":"测量和跟踪 在此阶段，您将开始改进衡量Kubernetes环境并跟踪成功的方式。测量将围绕五个关键领域： 安全 - 您将测量容器或群集中存在多少个漏洞以及哪些漏洞，以及修补工作负载，群集或附加组件的频率/时间。 审计 - 您将创建一个审计跟踪，以了解谁最近执行了操作以及集群中工作负载正在执行哪些操作。您将能够确定是否发生了未经授权的访问或操作。 漂移 - 您将能够确定哪些工作负载不符合您的标准，正在运行哪些版本的依赖项/集群附加组件以及工作负载是否与Kubernetes的未来版本兼容。 效率 - 您将进行测量以跟踪工作负载的典型或标准资源使用情况以及集群中节点的典型容量/使用情况。您还将知道群集扩展的频率。 速度 - 您将采取措施提高开发速度。这将包括了解部署的发布频率，多少用户访问您的集群以及在集群内执行的最常见操作。 ","date":"2021-01-28","objectID":"/k8s_model/:7:4","tags":["kubernetes","translate"],"title":"[译]Kubernetes成熟度模型","uri":"/k8s_model/"},{"categories":["Cloud Native"],"content":"控制 您将在工作负载和其他Kubernetes资源方面遇到痛苦，主要是在一致性方面。 工作负载可能不一致，也可能手动部署，然后进行修改。 跨容器和群集的配置中可能存在差异，这对于识别，更正和保持一致可能是一个挑战。 工作负载可能会杂乱无章，影响其他工作负载。 工作负载可能访问过多，从而导致安全问题。 可能存在可靠性或可伸缩性问题（无法充分扩展或扩展得太频繁）。 由于使用过多的资源或未清理工作负载，成本可能会攀升至很高。 随着您和您的团队的成熟，所有这些要点都是很自然的。在此阶段中，您要围绕安全性，配置和工作流来制定控制策略。以下是您需要考虑的一些示例。 ","date":"2021-01-28","objectID":"/k8s_model/:7:5","tags":["kubernetes","translate"],"title":"[译]Kubernetes成熟度模型","uri":"/k8s_model/"},{"categories":["Cloud Native"],"content":"安全 Kubernetes工作负载安全性至关重要。您需要控制群集权限，并且应该能够回答： 谁有权访问集群 用户可以在集群中执行哪些操作？ 工作负载在集群中可以采取什么行动？ 集群中具有什么级别的权限工作负载？ 集群中工作负载之间的网络策略是什么？ ","date":"2021-01-28","objectID":"/k8s_model/:7:6","tags":["kubernetes","translate"],"title":"[译]Kubernetes成熟度模型","uri":"/k8s_model/"},{"categories":["Cloud Native"],"content":"配置 坚固的Kubernetes环境将具有用于一致性的配置标准。您应该在以下位置设置控件： Kubernetes资源的住处和定义位置？ 什么变化会在何时发生？ 您的资源代码审查流程是什么？ 集群中可以部署什么类型的资源？ 哪些名称空间可供哪些用户使用？ 哪些名称空间工作负载部署到？ 如何设置可用于工作负载或名称空间的资源量？ 您在工作负载/部署中常见的标准/默认设置是什么？ ","date":"2021-01-28","objectID":"/k8s_model/:7:7","tags":["kubernetes","translate"],"title":"[译]Kubernetes成熟度模型","uri":"/k8s_model/"},{"categories":["Cloud Native"],"content":"工作流 同样，您将建立工作流，以了解如何部署工作负载和服务，升级路径和职责： 谁可以将工作负载和服务部署到您的集群？ 如何将工作负载和服务部署到您的集群？ 环境之间的提升路径是什么？ 谁负责您的环境的哪些方面？ 通过回答这些问题，您现在将拥有一组策略来开始在集群中实施配置更改。 您还具有Kubernetes经验，可以循环回到这些更高级的主题。 您将重新访问可能只是选择了默认选项的配置，检查发生了什么并进行更改。 ","date":"2021-01-28","objectID":"/k8s_model/:7:8","tags":["kubernetes","translate"],"title":"[译]Kubernetes成熟度模型","uri":"/k8s_model/"},{"categories":["Cloud Native"],"content":"挑战 跨工作负载，集群，团队的配置和流程不一致 用户和工作负载具有过多的访问权限，并且可能以不安全的方式运行/操作。 常见的可靠性问题很麻烦，并且由于缺乏有效的监视/警报而无法很好地理解。 ","date":"2021-01-28","objectID":"/k8s_model/:7:9","tags":["kubernetes","translate"],"title":"[译]Kubernetes成熟度模型","uri":"/k8s_model/"},{"categories":["Cloud Native"],"content":"输出 第六阶段将使您能够改善和完善Kubernetes环境，以确保其能够满足您的业务需求。 它还可以帮助您重新访问以前的阶段，以便在迁移新应用或设置新环境时避免引入不必要的问题。 您可能在成熟的这个阶段需要帮助，以确定可以在哪里进行改进。 在这里，对Kubernetes环境进行审核是一个很好的工具。 您也可以使用配置验证工具。 退出第六阶段时，您将建立协议和程序，以便团队与系统保持一致的交互并了解优先级。 您还将对基础架构（如代码和CI）有深入的了解 ","date":"2021-01-28","objectID":"/k8s_model/:7:10","tags":["kubernetes","translate"],"title":"[译]Kubernetes成熟度模型","uri":"/k8s_model/"},{"categories":["Cloud Native"],"content":"学习资源 Kubernetes: What Needs Monitoring and Why Kubernetes Audit and Improve Managing Kubernetes Configuration for Security, Reliability and Efficiency Why Infrastructure as Code and Kubernetes ","date":"2021-01-28","objectID":"/k8s_model/:7:11","tags":["kubernetes","translate"],"title":"[译]Kubernetes成熟度模型","uri":"/k8s_model/"},{"categories":["Cloud Native"],"content":"7. 优化\u0026自动化 消除人为错误和辛劳，提高可靠性并最大化效率。 在该阶段，你将学习/精通以下内容： 使用更复杂的工具来消除人为错误和辛劳，提高可靠性并最大程度地提高效率。 ","date":"2021-01-28","objectID":"/k8s_model/:8:0","tags":["kubernetes","translate"],"title":"[译]Kubernetes成熟度模型","uri":"/k8s_model/"},{"categories":["Cloud Native"],"content":"必要条件 改进和完善 您正在衡量和跟踪Kubernetes部署，以确保其交付符合业务需求。 建立协议 您已经制定了协议，策略和过程，因此团队可以与系统和优先级保持一致的交互。 IaC和CI/CD 您对作为代码和CI / CD的基础架构有深刻的理解，有助于一次性更改。 ","date":"2021-01-28","objectID":"/k8s_model/:8:1","tags":["kubernetes","translate"],"title":"[译]Kubernetes成熟度模型","uri":"/k8s_model/"},{"categories":["Cloud Native"],"content":"介绍 随着Kubernetes完全成熟，您现在将专注于优化和自动化环境。这包括优化Kubernetes的成本和效率，尽可能自动化以及定期运行配置验证以检查错误。 ","date":"2021-01-28","objectID":"/k8s_model/:8:2","tags":["kubernetes","translate"],"title":"[译]Kubernetes成熟度模型","uri":"/k8s_model/"},{"categories":["Cloud Native"],"content":"优化 现在，您正在跟踪和测量，您将在仪表板中拥有数据。这将帮助您优化Kubernetes使其更加高效或可靠。您将进行一些细微的改变，从而产生很大的变化。如，您可以通过以下方式优化群集： 根据您的工作负载需求使用正确的实例类型 扩展自定义指标与通用CPU使用率 转向多地区以更有效地为全球交通服务 跟踪和管理云支出成本 通过增加工作负载弹性来降低升级风险 您将永远不会停止优化集群。随着新数据的出现以及您的应用程序可与更多用户一起运行，您将需要不断查看仪表板并进行调整。这是成熟的最后阶段，很难做到。您需要一次解决一个问题以进行优化。 ","date":"2021-01-28","objectID":"/k8s_model/:8:3","tags":["kubernetes","translate"],"title":"[译]Kubernetes成熟度模型","uri":"/k8s_model/"},{"categories":["Cloud Native"],"content":"自动化 这是成熟的顶峰。到目前为止，您一直在手工做所有事情。在这里，您要自动化在前六个阶段中手动完成的所有操作。例如，您将： 查看您的基础架构即代码（IaC）以确保其牢固 使用监视失败来重新启动或管理有问题和失败的资源 自动审核并标记配置错误或安全问题 从生产中删除人员访问权限，转而使用服务帐户 通过软件和工具构建，升级和备份系统和基础架构 ","date":"2021-01-28","objectID":"/k8s_model/:8:4","tags":["kubernetes","translate"],"title":"[译]Kubernetes成熟度模型","uri":"/k8s_model/"},{"categories":["Cloud Native"],"content":"配置验证 最后，您将使用CI / CD流程中内置的配置验证工具，以确保不会将安全性，可靠性和效率问题部署到生产中。使用按政策编码，您将能够： 通过在CI / CD阶段或作为准入控制器的Open Policy Agent（OPA）集成，自动化部署护栏和最佳安全实践。 在应用程序开发过程中自动进行问题检测，以防止错误从一开始就进入生产。 通过扫描容器中的漏洞并审核群集中的弱点，可以持续了解Kubernetes的安全状况。 ","date":"2021-01-28","objectID":"/k8s_model/:8:5","tags":["kubernetes","translate"],"title":"[译]Kubernetes成熟度模型","uri":"/k8s_model/"},{"categories":["Cloud Native"],"content":"挑战 您可能会面临以下挑战： 从CI / CD到生产的政策管理 工作负载和其他Kubernetes资源使用资源效率低下，扩展不正确或性能不佳 成本攀升或不为人知 可靠性问题仍然需要大量的人为干预和辛劳 ","date":"2021-01-28","objectID":"/k8s_model/:8:6","tags":["kubernetes","translate"],"title":"[译]Kubernetes成熟度模型","uri":"/k8s_model/"},{"categories":["Cloud Native"],"content":"输出 您将通过策略驱动的容器和Kubernetes配置验证来简化从开发到操作的交接，从而加强法规遵从性。 在CI / CD流程到生产过程中都包含策略执行，可防止错误引起安全性或可靠性挑战。 您正在使用OPA创建自定义策略。 您可以深入研究Kubernetes的可靠性，效率和安全性实践。 ","date":"2021-01-28","objectID":"/k8s_model/:8:7","tags":["kubernetes","translate"],"title":"[译]Kubernetes成熟度模型","uri":"/k8s_model/"},{"categories":["Cloud Native"],"content":"学习资源 Managing Kubernetes Configuration for Security, Reliability and Efficiency Kubernetes Security, Reliability, Efficiency: It’s all about Configuration Fairwinds Insights: CI Pipeline to Protect Your Kubernetes Clusters Managing OPA Policies with Fairwinds Insights Addressing Kubernetes Security Vulnerabilities with Policy Enforcement ","date":"2021-01-28","objectID":"/k8s_model/:8:8","tags":["kubernetes","translate"],"title":"[译]Kubernetes成熟度模型","uri":"/k8s_model/"},{"categories":["DevOps"],"content":"翻译","date":"2021-01-13","objectID":"/talk_ops/","tags":["devops","translation"],"title":"[译]当我们谈论Ops，我们在谈论什么","uri":"/talk_ops/"},{"categories":["DevOps"],"content":"原文链接：What the Ops are you talking about? 水平有限，本文不免存在遗漏或错误之处。如有疑问，请查阅原文。 ","date":"2021-01-13","objectID":"/talk_ops/:0:0","tags":["devops","translation"],"title":"[译]当我们谈论Ops，我们在谈论什么","uri":"/talk_ops/"},{"categories":["DevOps"],"content":"背景 两年前，我因为效率低下的领导而获得了耻辱。我的背景是数据科学和机器学习，因此，我当然从我的工程同事那边学习到了DevOps。至少我们认为是这样的。 令人费解的是，即使我们遵循了日常站立会议所有敏捷开发的良好实践，讨论我们的难点，也没有将难题扔给别人的态度。我们紧密合作并且相互友爱。但是开发效率依然缓慢，这令整个团队很沮丧。 两年过后，我终于掌握了DevOps的含义，并且理解了它在数据团队中如此的相同而又如此不同。 ","date":"2021-01-13","objectID":"/talk_ops/:1:0","tags":["devops","translation"],"title":"[译]当我们谈论Ops，我们在谈论什么","uri":"/talk_ops/"},{"categories":["DevOps"],"content":"什么是Ops？ 在我们谈论以数据为中心的Ops时，先让我们从软件开始说起， 自从09年DevOps普及以来，软件行业就一直痴迷于各种Ops术语。十年前，从软件开发到部署的方法已经推陈出新。软件工程师开发应用，然后将其交付给运维工程师。该应用程序在部署期间经常中断，并在团队之间造成很大的摩擦。 DevOps实践的目的是简化部署过程。该想法是将自动化视为构建和部署软件应用程序的一等公民。 这种想法彻底改变了这个行业。许多组织开始建立跨职能团队来照顾整个SDLC。该团队将建立基础架构（基础工程师），开发应用程序（软件工程师），构建CI/CD管道（DevOps工程师），部署应用程序（每位工程师），然后连续监视和观察应用程序（站点可靠性工程师）。 在一个大团队里面，一个工程师可能只会有一项主要职能，但是在较小的团队中，一位工程师经常担任许多职务。理想的情况是使许多团队成员能够履行多项职能，从而消除瓶颈和关键人员的依存关系。所以实际上， DevOps并非是一项工作职能，而是更多的实践或文化。 在开始构建任何软件时都应采用它。 随着DevOps的兴起，各种各样的Ops诞生了。 SecOps以安全性为核心，GitOps致力于持续交付，NetOps确保网络可以支持数据流，而ITOps则专注于软件交付之外的操作任务。但是，这些操作的基石都源自DevOps所承诺的愿景： 在错误最小的情况下尽可能快的发布软件 ","date":"2021-01-13","objectID":"/talk_ops/:2:0","tags":["devops","translation"],"title":"[译]当我们谈论Ops，我们在谈论什么","uri":"/talk_ops/"},{"categories":["DevOps"],"content":"DataOps 🆚 MLOps 🆚 DevOps (and AIOps?) 注意：在本文中，分析团队是指使用SQL / PowerBI来生成业务洞察力的传统BI团队。 AI团队是指使用大数据技术构建高级分析和机器学习模型的团队。 有时他们是同一个团队，但我们将它们分开，以便更容易地解释概念。 五年前，“数据是新石油”一语成为炒作对象。世界各地的领导者开始倾注资源，建立大数据团队来挖掘这些宝贵的资产。这些团队交付的压力巨大—毕竟，我们如何才能兑现新石油的承诺？随着快速扩展，分析团队也经历了同样的痛苦。 然后，我们使这一切成为现实。 数据科学家成为21世纪最吃香的职业。我们正在建立和处于数据和分析的黄金时代。每个执行者都有一个仪表板，具有来自整个组织的数据和嵌入式预测模型的仪表板，每个客户都有基于其行为的个性化推荐。 但是，现在添加一个新功能需要花费数周甚至数月的时间。数据模型是混乱的并且没有人知道我们是使用信贷团队还是营销团队的活跃客户的定义。我们变得非常警惕将模型推向生成环境，因为我们不知道我们会破坏什么？ 因此，以数据为中心的社区团结在一起，保证不会因管理不善的数据流程而造成的效率低下，从那时起，各种以数据为中心的OPS诞生了 要了解所有这些不同的Ops，让我们来看看数据如何在组织中流动的场景： 数据是由与软件应用程序交互的客户生成的 软件将数据存储在其应用程序数据库中 分析团队从组织中的团队使用这些应用程序数据库构建ETL 然后，数据工程师将原始数据，合并的数据集（来自分析团队）和其他非结构化数据集摄取到某种形式的数据湖中 然后，数据科学家根据这些庞大的数据集建立模型 然后，这些模型采用用户生成的新数据进行预测。 然后，软件工程师将预测结果呈现给用户 并且周期继续 我们知道DevOps的诞生是由于开发团队和运维团队之间的摩擦。因此，想象一下运维，开发，分析和AI团队之间的4向界面所带来的令人头疼的问题。 为了说明不同的Ops如何解决上述过程，下面的图形绘制了每个作业功能在整个时间轴上执行的一些任务 理想情况下，应在项目开始时采用X-Ops文化，并在整个过程中实施实践. 总而言之，这就是每个Ops的意义 ","date":"2021-01-13","objectID":"/talk_ops/:3:0","tags":["devops","translation"],"title":"[译]当我们谈论Ops，我们在谈论什么","uri":"/talk_ops/"},{"categories":["DevOps"],"content":"DevOps更快地交付软件 一系列实践旨在消除开发团队和运维团队之间的障碍，以便更快地构建和部署软件。工程团队通常采用它，包括DevOps工程师，基础架构工程师，软件工程师，站点可靠性工程师和数据工程师。 ","date":"2021-01-13","objectID":"/talk_ops/:3:1","tags":["devops","translation"],"title":"[译]当我们谈论Ops，我们在谈论什么","uri":"/talk_ops/"},{"categories":["DevOps"],"content":"DataOps更快地交付数据 一系列实践旨在提高数据分析的质量和减少周期时间。DataOps主要的任务包括数据打标、数据测试、数据管道编排、数据版本控制和数据监控。分析和大数据团队是DataOps主要的支撑对象，但是任何生成和使用数据的人都应该采用良好的DataOps做法，其中包括数据分析师，BI分析师，数据科学家，数据工程师，有时还包括软件工程师。 ","date":"2021-01-13","objectID":"/talk_ops/:3:2","tags":["devops","translation"],"title":"[译]当我们谈论Ops，我们在谈论什么","uri":"/talk_ops/"},{"categories":["DevOps"],"content":"MLOps更快地提供机器学习模型 一套设计，构建和管理可重现，可测试和可持续的ML支持软件的实践。对于大数据/机器学习团队，MLOps包含大多数DataOps任务和其他特定于ML的任务，例如模型版本控制，测试，验证和监视。 ","date":"2021-01-13","objectID":"/talk_ops/:3:3","tags":["devops","translation"],"title":"[译]当我们谈论Ops，我们在谈论什么","uri":"/talk_ops/"},{"categories":["DevOps"],"content":"奖励：AIOps利用AI的功能增强了DevOps工具 有时人们会错误地将MLOps称为AIOps，但它们却大不相同。从Gartner： AIOps platforms utilize big data, modern machine learning and other advanced analytics technologies to directly and indirectly enhance IT operations (monitoring, automation and service desk) functions with proactive, personal and dynamic insight. 因此，AIOps通常是使用AI驱动技术来增强服务产品的DevOps工具。 AWS CloudWatch提供的警报和异常检测是AIOps的一个很好的例子. ","date":"2021-01-13","objectID":"/talk_ops/:3:4","tags":["devops","translation"],"title":"[译]当我们谈论Ops，我们在谈论什么","uri":"/talk_ops/"},{"categories":["DevOps"],"content":"Principals not Job Roles 一个误解是，为了实现这些行动所承诺的效率，他们需要从选择正确的技术开始。事实上，技术不是最重要的东西。 DataOps，MLOps和DevOps的实践必须与语言，框架，平台和基础架构无关。 每个人都有不同的workflow，该workflow应由负责人告知——而不是你想尝试的技术或最受欢迎的技术。首先要去技术的陷阱是，如果您想使用锤子，一切对您来说就像钉子一样。 所有的Ops都有相同的7个主要原则，但每个Ops都有自己的细微差别： ","date":"2021-01-13","objectID":"/talk_ops/:4:0","tags":["devops","translation"],"title":"[译]当我们谈论Ops，我们在谈论什么","uri":"/talk_ops/"},{"categories":["DevOps"],"content":"1. 合规 DevOps通常担心网络和应用程序的安全性。在MLOps领域，金融和医疗保健等行业通常需要模型可解释性。DataOps需要确保数据产品符合GDPR / HIPPA等法律。 🔧 Tools: PySyft 解耦私人数据以进行模型训练， AirCloak 用于数据匿名. Awesome AI Guidelines 策划有关AI的原则，标准和法规。 ","date":"2021-01-13","objectID":"/talk_ops/:4:1","tags":["devops","translation"],"title":"[译]当我们谈论Ops，我们在谈论什么","uri":"/talk_ops/"},{"categories":["DevOps"],"content":"2. 迭代开发 该原理源于敏捷方法论，该方法论着重于以可持续的方式持续产生业务价值。该产品经过迭代设计，构建，测试和部署，以最大程度地快速排除故障并学习原理。 ","date":"2021-01-13","objectID":"/talk_ops/:4:2","tags":["devops","translation"],"title":"[译]当我们谈论Ops，我们在谈论什么","uri":"/talk_ops/"},{"categories":["DevOps"],"content":"3. 可重现性 软件系统通常是确定性的：每次代码都应以完全相同的方式运行。因此，为了确保可重现性，DevOps只需要跟踪代码即可。 但是，由于任一数据漂移，机器学习模型通常都需要重新训练。为了重现结果，MLOps需要对模型进行版本控制，而DataOps需要对数据进行版本控制。当审核员询问要使用哪些数据来训练哪种模型来产生特定结果时，数据科学家需要能够回答这一问题。 🔧 Tools: 实验跟踪工具, 例如 KubeFlow, MLFlow 或者 SageMaker 都具有将元数据链接到实验运行的功能。 Pachyderm 和 DVC 用于数据版本控制。 ","date":"2021-01-13","objectID":"/talk_ops/:4:3","tags":["devops","translation"],"title":"[译]当我们谈论Ops，我们在谈论什么","uri":"/talk_ops/"},{"categories":["DevOps"],"content":"4. 测试 软件测试在于单元测试，集成测试和回归测试。DataOps需要严格的数据测试，其中包括架构更改，数据漂移，功能设计后的数据验证等。从机器学习的角度来看，模型准确性，安全性，偏见/公平性，可解释性都需要进行测试。 🔧 Tools: 诸如 Shap \u0026 Lime 用于可解释性, fiddler 用于解释性监控, great expectation 用于数据测试. ","date":"2021-01-13","objectID":"/talk_ops/:4:4","tags":["devops","translation"],"title":"[译]当我们谈论Ops，我们在谈论什么","uri":"/talk_ops/"},{"categories":["DevOps"],"content":"5. 持续部署 这里有三个组件用于机器学习模型的持续部署： 第一个组件是触发事件，即触发是数据科学家手动触发，日历计划事件和阈值触发吗？ 第二个组件是新模型的实际再培训。导致该模型的脚本，数据和超参数是什么？它们的版本以及如何相互链接。 最后一个组件是模型的实际部署，该部署必须由具有预警功能的部署管道进行编排。 🔧 Tools: 大多数workflow管理工具都具有此功能，例如AWS SageMaker，AzureML，DataRobot等。开源工具例如Seldon, Kubeflow KFServing. ","date":"2021-01-13","objectID":"/talk_ops/:4:5","tags":["devops","translation"],"title":"[译]当我们谈论Ops，我们在谈论什么","uri":"/talk_ops/"},{"categories":["DevOps"],"content":"6. 自动化 自动化是DevOps的核心价值，实际上有很多专门针对自动化各个方面的工具。以下是机器学习项目的一些资源： Awesome Machine Learning Awesome Production Machine Learning ","date":"2021-01-13","objectID":"/talk_ops/:4:6","tags":["devops","translation"],"title":"[译]当我们谈论Ops，我们在谈论什么","uri":"/talk_ops/"},{"categories":["DevOps"],"content":"7. 监控 需要监视软件应用程序，机器学习模型和数据管道也需要监视。对于DataOps，监视新数据的分布是否有任何数据和/或概念漂移很重要。在MLOps方面，除了模型降级之外，如果您的模型具有公共API，监视对抗攻击也至关重要。 🔧 Tools: 大多数workflow管理框架都有某种形式的监控。. 其他流行的工具包括 Prometheus 用于指标监控, Orbit by Dessa 用于数据\u0026模型监控. ","date":"2021-01-13","objectID":"/talk_ops/:4:7","tags":["devops","translation"],"title":"[译]当我们谈论Ops，我们在谈论什么","uri":"/talk_ops/"},{"categories":["DevOps"],"content":"结论 采用正确的X-Ops文化，以加快数据驱动和机器学习驱动的软件产品的交付。请记住，有关技术的原则： 建立跨学科技能: 培养T型人才和团队，弥补差距并统一问责制 尽早实现自动化: 融合在技术堆栈上并实现自动化，减轻工程费用的流程 着眼于最终方案: 预先投资解决方案设计以减少从PoC到生产的摩擦 ","date":"2021-01-13","objectID":"/talk_ops/:5:0","tags":["devops","translation"],"title":"[译]当我们谈论Ops，我们在谈论什么","uri":"/talk_ops/"},{"categories":["Other"],"content":"flag","date":"2021-01-02","objectID":"/2021_flag/","tags":["life"],"title":"2021年flag","uri":"/2021_flag/"},{"categories":["Other"],"content":"2021年已经到来，在这里给自己列一个flag清单。 ","date":"2021-01-02","objectID":"/2021_flag/:0:0","tags":["life"],"title":"2021年flag","uri":"/2021_flag/"},{"categories":["Other"],"content":"习惯成自然 每2周完成一个ARTS(Algorithm|Review|Technique|Share) 完成12篇博客(每月一篇) 完成4篇英文技术文章翻译(每季度一篇) ","date":"2021-01-02","objectID":"/2021_flag/:0:1","tags":["life"],"title":"2021年flag","uri":"/2021_flag/"},{"categories":["Other"],"content":"读书清单 《性能之巅 洞悉系统、企业与云计算》 《Web性能权威指南》 ","date":"2021-01-02","objectID":"/2021_flag/:0:2","tags":["life"],"title":"2021年flag","uri":"/2021_flag/"},{"categories":["Other"],"content":"极客时间清单 程序员的数学基础课 MySQL实战45讲 ","date":"2021-01-02","objectID":"/2021_flag/:0:3","tags":["life"],"title":"2021年flag","uri":"/2021_flag/"},{"categories":["Cloud Native"],"content":"OAM","date":"2020-11-15","objectID":"/k8s_series_oam/","tags":["kubernetes"],"title":"Kubernetes系列：OAM","uri":"/k8s_series_oam/"},{"categories":["Cloud Native"],"content":"系列目录 《Kubernetes系列：开篇》 《Kubernetes系列：概述》 《Kubernetes系列：架构》 《Kubernetes系列：容器》 《Kubernetes系列：网络》 《Kubernetes系列：存储》 《Kubernetes系列：Service》 《Kubernetes系列：Ingress》 《Kubernetes系列：OAM》 ","date":"2020-11-15","objectID":"/k8s_series_oam/:1:0","tags":["kubernetes"],"title":"Kubernetes系列：OAM","uri":"/k8s_series_oam/"},{"categories":["Cloud Native"],"content":"1. 介绍 开放应用程序模型(OAM)是与运行时无关的规范，用于定义云原生应用程序。 OAM专注于应用程序，而不是容器或协调器。 OAM带来了模块化，可扩展和可移植的设计，可用于对云原生应用程序建模，并以统一的方式将应用程序交付给Kubernetes，云或IoT设备等任何运行时。 ","date":"2020-11-15","objectID":"/k8s_series_oam/:2:0","tags":["kubernetes"],"title":"Kubernetes系列：OAM","uri":"/k8s_series_oam/"},{"categories":["Cloud Native"],"content":"2. KubeVela ","date":"2020-11-15","objectID":"/k8s_series_oam/:3:0","tags":["kubernetes"],"title":"Kubernetes系列：OAM","uri":"/k8s_series_oam/"},{"categories":["Cloud Native"],"content":"2.1 是什么 云原生技术的趋势正在朝着使用Kubernetes作为通用抽象层跨云和本地基础架构追求一致的应用程序交付的趋势。 对于平台构建者而言，KubeVela是一个框架，使他们能够轻松创建用户友好但高度可扩展的平台。详细地说，KubeVela通过执行以下操作减轻了构建此类平台的麻烦： 以应用为中心。 KubeVela强制采用一种应用程序概念作为其主要API，并且所有KubeVela的功能仅可满足应用程序的需求。这是通过采用开放应用程序模型作为KubeVela的核心API来实现的。 本地扩展。KubeVela中的应用程序由各种模块化组件（称为：服务）组成。 Kubernetes生态系统的功能可以随时通过Kubernetes CRD注册机制作为新的工作负载类型或特征添加到KubeVela中。 简单但可扩展的抽象机制。KubeVela引入了一个模板引擎（支持CUELang等），用于从下划线的Kubernetes资源中提取面向用户的模式。KubeVela提供了一组内置的抽象作为起点，并且平台构建者可以随时自由地对其进行修改。抽象更改在运行时生效，无需重新编译或重新部署KubeVela。 借助KubeVela，平台构建者现在终于获得了工具支持，以高信心和低周转时间设计并向其最终用户交付任何新功能。 对于开发人员而言，使用KubeVela构建的此类平台将使他们能够以最小的努力设计并将其应用程序发布到Kubernetes。他们只需要一个简单的应用程序定义，而不是管理少量的基础结构细节，而是遵循以开发人员为中心的工作流，该工作流可以轻松地与任何CI / CD管道集成。 ","date":"2020-11-15","objectID":"/k8s_series_oam/:3:1","tags":["kubernetes"],"title":"Kubernetes系列：OAM","uri":"/k8s_series_oam/"},{"categories":["Cloud Native"],"content":"2.2 对比 PaaS 它们提供了完整的应用程序管理功能，旨在改善开发人员的体验和效率。KubeVela可以提供类似的体验，但是与大多数现有的PaaS产品相比，其内置功能轻巧得多，并且易于维护。KubeVela核心组件不过是一组Kubernetes控制器/插件。 KubeVela被设计为核心引擎，其主要目标是使平台团队能够通过简单地注册CRD和定义模板来创建“类似PaaS”的体验。与此经验相比，大多数现有的PaaS系统要么不可扩展，要么具有自己的附加系统。因此，对他们来说，在受支持的应用程序类型和受支持的功能上强加约束是很常见的，而这在基于KubeVela的体验中是不会发生的。 Serverless Platforms 无服务器平台（例如AWS Lambda）可提供非凡的用户体验和敏捷性，以部署无服务器应用程序。但是，这些平台在可扩展性方面施加了更多限制。它们可以说是“硬编码” PaaS。 通过将自己注册为新的工作负载类型和特征，可以轻松地将基于Kubernetes的Knative，OpenFaaS等无服务器平台与KubeVela集成。即使对于AWS Lambda，也有成功的故事，可以通过Crossplane开发的工具将其与KubeVela集成。 与平台无关的开发人员工具 典型的例子是Hashicorp的Waypoint。 Waypoint是面向开发人员的工具，它引入了一致的工作流程（即构建，部署，发布），以在不同平台之上发布应用程序。 KubeVela可以像任何其他受支持的平台一样集成到Waypoint中。在这种情况下，开发人员将使用Waypoint工作流而不是KubeVela Appfile / CLI来管理应用程序，并且该集成中仍然可以使用KubeVela的所有功能，包括抽象。 Helm Helm是Kubernetes的软件包管理器，它为Kubernetes作为一个单元提供打包，安装和升级一组YAML文件。 KubeVela充分利用Helm作为功能和依赖项的软件包格式。 尽管KubeVela本身不是包管理器，但它是平台构建者以简单且可重复的方式创建上层平台的核心引擎。 Kubernetes KubeVela是用于构建上层平台的Kubernetes插件。它利用Kubernetes的本机可扩展性和功能解决了一个棘手的问题-使运输应用程序在Kubernetes上令人愉悦。 ","date":"2020-11-15","objectID":"/k8s_series_oam/:3:2","tags":["kubernetes"],"title":"Kubernetes系列：OAM","uri":"/k8s_series_oam/"},{"categories":["Cloud Native"],"content":"2.3 安装配置 2.3.1 KubeVale安装 添加 helm chart repo helm repo add kubevela https://kubevelacharts.oss-cn-hangzhou.aliyuncs.com/coreCopy to clipboardErrorCopied 更新 chart repo helm repo updateCopy to clipboardErrorCopied 安装 KubeVela helm install --create-namespace -n vela-system kubevela kubevela/vela-core 2.3.2 部署应用 $ kubectl apply -f https://raw.githubusercontent.com/oam-dev/kubevela/master/docs/examples/vela-app.yaml application.core.oam.dev/first-vela-app createdCopy to clipboardErrorCopied 检查 status 处于 running 状态并且services的healthy为true $ kubectl get application first-vela-app -o yaml apiVersion: core.oam.dev/v1alpha2 kind: Application metadata: generation: 1 name: first-vela-app ... namespace: default spec: components: - name: express-server type: webservice settings: image: crccheck/hello-world port: 8000 traits: - name: ingress properties: domain: testsvc.example.com http: /: 8000 status: ... services: - healthy: true name: express-server traits: - healthy: true message: 'Visiting URL: testsvc.example.com, IP: your ip address' type: ingress status: runningCopy to clipboardErrorCopied k8s资源将被创建 $ kubectl get deployment NAME READY UP-TO-DATE AVAILABLE AGE express-server-v1 1/1 1 1 8m $ kubectl get svc NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE express-server ClusterIP 172.21.11.152 \u003cnone\u003e 8000/TCP 7m43s kubernetes ClusterIP 172.21.0.1 \u003cnone\u003e 443/TCP 116d $ kubectl get ingress NAME CLASS HOSTS ADDRESS PORTS AGE express-server \u003cnone\u003e testsvc.example.com \u003cyour ip address\u003e 80 7m47sCopy to clipboardErrorCopied 访问服务 $ curl -H \"Host:testsvc.example.com\" http://\u003cyour ip address\u003e/ \u003cxmp\u003e Hello World ## . ## ## ## == ## ## ## ## ## === /\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\\___/ === ~~~ {~~ ~~~~ ~~~ ~~~~ ~~ ~ / ===- ~~~ \\______ o _,/ \\ \\ _,' `'--.._\\..--'' \u003c/xmp\u003e ","date":"2020-11-15","objectID":"/k8s_series_oam/:3:3","tags":["kubernetes"],"title":"Kubernetes系列：OAM","uri":"/k8s_series_oam/"},{"categories":["Cloud Native"],"content":"2.3 详解 2.3.1 Workflow Platform 职责：为部署环境和可重用模块（例如组件类型，操作行为）定义模板，并将其注册到集群中。 示例：基础架构运营商，平台构建者。 EndUser 职责：选择部署环境，使用可用模块对应用程序进行建模和组装，然后将应用程序部署到目标环境。 示例：应用程序开发人员，应用程序运营商。 2.3.2 Application Application是KubeVela的核心API。它的主要目的是用于应用程序封装和抽象，即它允许开发人员使用单个工件来使用简化的原语来捕获完整的应用程序定义。 应用程序封装对于简化管理任务很重要，并且可以充当锚点以避免操作期间的配置漂移。此外，作为抽象对象，Application无需依赖底层细节就为Kubernetes的入门功能提供了更为简单的途径。例如，开发人员将能够在无需每次都定义详细的Kubernetes部署+服务组合的情况下对“ Web服务”进行建模，或者可以在不引用基础KEDA ScaleObject的情况下声明自动扩展需求。 例子： apiVersion:core.oam.dev/v1alpha2kind:Applicationmetadata:name:websitespec:components:- name:backendtype:workersettings:image:busyboxcmd:- sleep- '1000'- name:frontendtype:webservicesettings:image:nginxtraits:- name:autoscalerproperties:min:1max:10- name:sidecarproperties:name:\"sidecar-test\"image:\"fluentd\" 2.3.3 Workload Types 对于Application中的每个组件，它的.type字段表示此组件的运行时特征（即工作负载类型），. settings要求对其进行实例化的配置。些典型的组件类型是Long Running Web Service, One-time Off Task or Redis Database 所有受支持的组件类型都应预先安装在平台中，或者由组件供应商（例如第三方软件所有者）提供。 2.3.4 Traits 可选地，每个组件都有一个.traits部分，该部分通过操作行为（例如负载平衡策略，网络入口路由，自动扩展策略或升级策略等）来扩展其组件实例。其.name字段引用特定的特征定义，.properties设置给定特征的详细配置值。 受支持的特征大部分是平台提供的操作功能，但平台还允许用户带来自己的特征。 我们还将KubeVela中的工作负载类型和特征称为“application/capability modules”。 2.3.5 Definitions 应用程序中的工作负载设置模式和特征属性都由在一组定义对象中分别预定义的应用程序模块强制实施。平台团队或组件提供者负责按照开放应用程序模型（OAM）中的工作负载定义和特征定义规范在目标集群中注册和管理定义。 当前，KubeVela在定义中支持CUE作为模块类型，而在后续版本中将提供更多类型，例如Helm和Terraform。在Helm的情况下，图表可以在Application中直接引用为组件类型，其值.yaml将成为组件的规格 2.3.6 Environment 在将应用程序发布到生产环境之前，在测试/登台工作空间中测试代码很重要。在KubeVela中，我们将这些工作空间简称为“部署环境”或“环境”。每个环境都有其自己的配置（例如，域，Kubernetes集群和名称空间，配置数据，访问控制策略等），以允许用户创建不同的部署环境，例如“测试”和“生产”。 当前，KubeVela环境仅映射到Kubernetes命名空间，而集群级环境仍在开发中。 2.3.7 组件概览 2.3.8 架构 KubeVela中的封装引擎负责应用程序的抽象和封装（即Application和Definition的控制器） 部署引擎（当前为WIP）负责逐步部署应用程序（即AppDeployment的控制器）。 ","date":"2020-11-15","objectID":"/k8s_series_oam/:3:4","tags":["kubernetes"],"title":"Kubernetes系列：OAM","uri":"/k8s_series_oam/"},{"categories":["Cloud Native"],"content":"3. 结论 该篇我们讲述了OAM模型，并讲述了OAM模型的实现KubeVela. 参考 https://kubevela.io/#/en/introduction https://oam.dev/ ","date":"2020-11-15","objectID":"/k8s_series_oam/:4:0","tags":["kubernetes"],"title":"Kubernetes系列：OAM","uri":"/k8s_series_oam/"},{"categories":["Cloud Native"],"content":"Ingress","date":"2020-11-11","objectID":"/k8s_series_ingress/","tags":["kubernetes"],"title":"Kubernetes系列：Ingress","uri":"/k8s_series_ingress/"},{"categories":["Cloud Native"],"content":"系列目录 《Kubernetes系列：开篇》 《Kubernetes系列：概述》 《Kubernetes系列：架构》 《Kubernetes系列：容器》 《Kubernetes系列：网络》 《Kubernetes系列：存储》 《Kubernetes系列：Service》 《Kubernetes系列：Ingress》 《Kubernetes系列：OAM》 ","date":"2020-11-11","objectID":"/k8s_series_ingress/:1:0","tags":["kubernetes"],"title":"Kubernetes系列：Ingress","uri":"/k8s_series_ingress/"},{"categories":["Cloud Native"],"content":"1. 介绍 ","date":"2020-11-11","objectID":"/k8s_series_ingress/:2:0","tags":["kubernetes"],"title":"Kubernetes系列：Ingress","uri":"/k8s_series_ingress/"},{"categories":["Cloud Native"],"content":"1.1 Ingress Ingress 公开了从集群外部到集群内service的 HTTP 和 HTTPS 路由。 流量路由由 Ingress 资源上定义的规则控制。 可以将 Ingress 配置为服务提供外部可访问的 URL、负载均衡流量、终止 SSL/TLS，以及提供基于名称的虚拟主机等能力。 Ingress Controller 通常负责通过负载均衡器来实现 Ingress，尽管它也可以配置边缘路由器或其他前端来帮助处理流量。 ","date":"2020-11-11","objectID":"/k8s_series_ingress/:2:1","tags":["kubernetes"],"title":"Kubernetes系列：Ingress","uri":"/k8s_series_ingress/"},{"categories":["Cloud Native"],"content":"1.2 Ingress Controller 为了让 Ingress 资源工作，集群必须有一个正在运行的 Ingress Controller。 与作为 kube-controller-manager 可执行文件的一部分运行的其他类型的控制器不同，Ingress 控制器不是随集群自动启动的。 ","date":"2020-11-11","objectID":"/k8s_series_ingress/:2:2","tags":["kubernetes"],"title":"Kubernetes系列：Ingress","uri":"/k8s_series_ingress/"},{"categories":["Cloud Native"],"content":"2. Ingress Contoller 选择 下表是一些常用的Contoller对比： control plane data plane backend service discovery protocols ssl termination websocket routing scope resiliency lb algorithms auth Tracing canary/shadow istio integration state Paid support Linkaaaaaaaaaaaaaa dashboard sticky sessions lua ingress-nginx nginx dynamic http,https,tcp (separate lb),udp,grpc,fastcgi,IPC socket yes yes host,path(with regex) cross-namespace rate limit, retries rr,ewma,ip_hash basic, digest, external auth yes canary kubernetes https://kubernetes.github.io/ingress-nginx/ Metrics can be seen in Grafana Yes Yes kong nginx dynamic http,https, grpc yes yes host, header, path, method cross-namespace active and passive health check, circuit break, rate limit, retries rr, hash, header, cookie Basic Auth, HMAC, JWT, Key, LDAP, OAuth 2.0, PASETO, plus paid Kong Enterprise options like OpenID Connect yes canary kubernetes https://github.com/Kong/kubernetes-ingress-controller Admin Dashboard + Grafana+PrometheusstatsdDatadogSignalFx Yes Yes traefik traefik dynamic http,https,grpc,tcp + tls (alpha) yes yes host,path cross-namespace circuit break, retries rr, wrr basic, digest and forward auth in alpha yes canary yes kubernetes https://docs.traefik.io/configuration/backends/kubernetes/ Included Yes no istio ingress envoy dynamic tcp,http,https,grpc yes yes host,user cross-namespace circuit break, retries rr,leastconn,random,passthrough JWT yes yes kubernetes https://istio.io/docs/tasks/traffic-management/ingress/ Metrics can be seen in Grafana and Prometheus, tracing can be seen through jaeger or zipkin UI Yes Yes ","date":"2020-11-11","objectID":"/k8s_series_ingress/:3:0","tags":["kubernetes"],"title":"Kubernetes系列：Ingress","uri":"/k8s_series_ingress/"},{"categories":["Cloud Native"],"content":"2.1 ingress-nginx github.com/kubernetes/ingress-nginx Implemented in: Go/Lua (while nginx is written in C) License: Apache 2.0 官方的ingress controller，它是由社区开发的。基于nginx Web服务器，并补充了一组用于实现额外功能的Lua插件。 2.1.1 配置 一个最小的 Ingress 资源示例： apiVersion:networking.k8s.io/v1kind:Ingressmetadata:name:minimal-ingressannotations:nginx.ingress.kubernetes.io/rewrite-target:/spec:rules:- http:paths:- path:/testpathpathType:Prefixbackend:service:name:testport:number:80 与所有其他 Kubernetes 资源一样，Ingress 需要使用 apiVersion、kind 和 metadata 字段。 Ingress 对象的命名必须是合法的 DNS 子域名名称。 有关使用配置文件的一般信息，请参见部署应用、 配置容器、 管理资源。 Ingress 经常使用注解（annotations）来配置一些选项，具体取决于 Ingress 控制器，例如 重写目标注解。 不同的 Ingress 控制器 支持不同的注解。查看文档以供你选择 Ingress 控制器，以了解支持哪些注解。 Ingress 规约 提供了配置负载均衡器或者代理服务器所需的所有信息。 最重要的是，其中包含与所有传入请求匹配的规则列表。 Ingress 资源仅支持用于转发 HTTP 流量的规则。 2.1.2 Ingress 规则 每个 HTTP 规则都包含以下信息： 可选的 host。在此示例中，未指定 host，因此该规则适用于通过指定 IP 地址的所有入站 HTTP 通信。 如果提供了 host（例如 foo.bar.com），则 rules 适用于该 host。 路径列表 paths（例如，/testpath）,每个路径都有一个由 serviceName 和 servicePort 定义的关联后端。 在负载均衡器将流量定向到引用的服务之前，主机和路径都必须匹配传入请求的内容。 backend（后端）是 Service 文档中所述的服务和端口名称的组合。 与规则的 host 和 path 匹配的对 Ingress 的 HTTP（和 HTTPS ）请求将发送到列出的 backend。 通常在 Ingress 控制器中会配置 defaultBackend（默认后端），以服务于任何不符合规约中 path 的请求。 2.1.3 DefaultBackend 没有 rules 的 Ingress 将所有流量发送到同一个默认后端。 defaultBackend 通常是 Ingress 控制器 的配置选项，而非在 Ingress 资源中指定。 如果 hosts 或 paths 都没有与 Ingress 对象中的 HTTP 请求匹配，则流量将路由到默认后端。 2.1.4 资源后端 Resource 后端是一个 ObjectRef，指向同一名字空间中的另一个 Kubernetes，将其作为 Ingress 对象。Resource 与 Service 配置是互斥的，在 二者均被设置时会无法通过合法性检查。 Resource 后端的一种常见用法是将所有入站数据导向带有静态资产的对象存储后端。 service/networking/ingress-resource-backend.yaml apiVersion:networking.k8s.io/v1kind:Ingressmetadata:name:ingress-resource-backendspec:defaultBackend:resource:apiGroup:k8s.example.comkind:StorageBucketname:static-assetsrules:- http:paths:- path:/iconspathType:ImplementationSpecificbackend:resource:apiGroup:k8s.example.comkind:StorageBucketname:icon-assets 创建了如上的 Ingress 之后，你可以使用下面的命令查看它： kubectl describe ingress ingress-resource-backend Name: ingress-resource-backend Namespace: default Address: Default backend: APIGroup: k8s.example.com, Kind: StorageBucket, Name: static-assets Rules: Host Path Backends ---- ---- -------- * /icons APIGroup: k8s.example.com, Kind: StorageBucket, Name: icon-assets Annotations: \u003cnone\u003e Events: \u003cnone\u003e 2.1.5 路径类型 Ingress 中的每个路径都需要有对应的路径类型（Path Type）。未明确设置 pathType 的路径无法通过合法性检查。当前支持的路径类型有三种： ImplementationSpecific：对于这种路径类型，匹配方法取决于 IngressClass。 具体实现可以将其作为单独的 pathType 处理或者与 Prefix 或 Exact 类型作相同处理。 Exact：精确匹配 URL 路径，且区分大小写。 Prefix：基于以 / 分隔的 URL 路径前缀匹配。匹配区分大小写，并且对路径中的元素逐个完成。 路径元素指的是由 / 分隔符分隔的路径中的标签列表。 如果每个 p 都是请求路径 p 的元素前缀，则请求与路径 p 匹配。 ","date":"2020-11-11","objectID":"/k8s_series_ingress/:3:1","tags":["kubernetes"],"title":"Kubernetes系列：Ingress","uri":"/k8s_series_ingress/"},{"categories":["Cloud Native"],"content":"2.2 Kong Ingress github.com/Kong/kubernetes-ingress-controller Implemented in: Go License: Apache 2.0 Kong Ingress建立在NGINX之上，并增加了扩展其功能的Lua模块。 它的主要优点是易于安装和配置的大量其他模块/插件（包括来自第三方开发人员的模块/插件）。 它为多种附加功能开辟了道路。附带地，内置功能已经提供了许多可能性。使用CRD执行配置。 Kong的一个重要功能是它只能在一个环境中运行（而不是支持跨命名空间），这是一个颇具争议的主题：有人认为它是一个缺点（您必须为每个环境生成实例），而其他人则认为这是一个特殊功能（较高的隔离级别，因此一个控制器的故障影响仅限于其环境）。 ","date":"2020-11-11","objectID":"/k8s_series_ingress/:3:2","tags":["kubernetes"],"title":"Kubernetes系列：Ingress","uri":"/k8s_series_ingress/"},{"categories":["Cloud Native"],"content":"2.3 Istio Ingress istio.io/docs/tasks/traffic-management/ingress Implemented in: Go License: Apache 2.0 Istio是IBM，Google和Lyft（Envoy的原始作者）的一个联合项目，它是一个全面的服务网格解决方案。它不仅可以管理所有传入的外部流量（作为Ingress控制器），还可以控制集群内部的所有流量。在幕后，Istio将Envoy用作每种服务的辅助代理。从本质上讲，它是一个可以执行几乎所有操作的大型处理器。其中心思想是最大程度的控制，可扩展性，安全性和透明性。 借助Istio Ingress，您可以微调流量路由，服务之间的访问授权，平衡，监控，金丝雀发布等。 Here is a great intro to learn about Istio: “Back to microservices with Istio”. ","date":"2020-11-11","objectID":"/k8s_series_ingress/:3:3","tags":["kubernetes"],"title":"Kubernetes系列：Ingress","uri":"/k8s_series_ingress/"},{"categories":["Cloud Native"],"content":"2.4 Traefik github.com/containous/traefik Implemented in: Go License: MIT 最初，此代理是为微服务及其动态环境的请求路由而创建的，因此，它具有许多有用的功能：连续更新配置（不重新启动），支持多种负载平衡算法，Web UI，指标导出，支持各种协议，REST API，canary版本等。他对开箱即用的“加密”证书的支持是另一个不错的功能。主要缺点是，为了组织控制器的高可用性，您必须安装并连接其自己的KV存储器。 ","date":"2020-11-11","objectID":"/k8s_series_ingress/:3:4","tags":["kubernetes"],"title":"Kubernetes系列：Ingress","uri":"/k8s_series_ingress/"},{"categories":["Cloud Native"],"content":"3. 结论 本文讲述了ingress是啥，以及一些基础的配置。 参考 https://medium.com/flant-com/comparing-ingress-controllers-for-kubernetes-9b397483b46b https://kubernetes.io/docs/concepts/services-networking/ingress/ https://docs.konghq.com/kubernetes-ingress-controller/1.1.x/introduction/ https://doc.traefik.io/traefik/middlewares/overview/ ","date":"2020-11-11","objectID":"/k8s_series_ingress/:4:0","tags":["kubernetes"],"title":"Kubernetes系列：Ingress","uri":"/k8s_series_ingress/"},{"categories":["Cloud Native"],"content":"服务","date":"2020-11-10","objectID":"/k8s_series_service/","tags":["kubernetes"],"title":"Kubernetes系列：Service","uri":"/k8s_series_service/"},{"categories":["Cloud Native"],"content":"系列目录 《Kubernetes系列：开篇》 《Kubernetes系列：概述》 《Kubernetes系列：架构》 《Kubernetes系列：容器》 《Kubernetes系列：网络》 《Kubernetes系列：存储》 《Kubernetes系列：Service》 《Kubernetes系列：Ingress》 《Kubernetes系列：OAM》 ","date":"2020-11-10","objectID":"/k8s_series_service/:1:0","tags":["kubernetes"],"title":"Kubernetes系列：Service","uri":"/k8s_series_service/"},{"categories":["Cloud Native"],"content":"1. 介绍 在Kubernetes中，Service是将运行在一组Pods上的应用程序公开为网络服务的抽象方法。 ","date":"2020-11-10","objectID":"/k8s_series_service/:2:0","tags":["kubernetes"],"title":"Kubernetes系列：Service","uri":"/k8s_series_service/"},{"categories":["Cloud Native"],"content":"1. 1 为什么需要Service？ pod是一个非永久性的资源。如果我们使用Deployment来运行应用程序，则pod是可以被动态创建和销毁的。 这导致了一个问题： 如果一组 Pod（称为“后端”）为集群内的其他 Pod（称为“前端”）提供功能， 那么前端如何找出并跟踪要连接的 IP 地址，以便前端可以使用提供工作负载的后端部分？ ","date":"2020-11-10","objectID":"/k8s_series_service/:2:1","tags":["kubernetes"],"title":"Kubernetes系列：Service","uri":"/k8s_series_service/"},{"categories":["Cloud Native"],"content":"1.2 Service资源 Kubernetes Service 定义了这样一种抽象：逻辑上的一组 Pod，一种可以访问它们的策略 —— 通常称为微服务。 Service 所针对的 Pods 集合通常是通过选择算符来确定的。 ","date":"2020-11-10","objectID":"/k8s_series_service/:2:2","tags":["kubernetes"],"title":"Kubernetes系列：Service","uri":"/k8s_series_service/"},{"categories":["Cloud Native"],"content":"2. 配置 Service 在 Kubernetes 中是一个 REST 对象，和 Pod 类似。 像所有的 REST 对象一样，Service 定义可以基于 POST 方式，请求 API server 创建新的实例。 Service 对象的名称必须是合法的 DNS 标签名称。 ","date":"2020-11-10","objectID":"/k8s_series_service/:3:0","tags":["kubernetes"],"title":"Kubernetes系列：Service","uri":"/k8s_series_service/"},{"categories":["Cloud Native"],"content":"2.1 一般配置 一个例子，有一组 Pod，它们对外暴露了 9376 端口，同时还被打上 app=MyApp 标签： apiVersion:v1kind:Servicemetadata:name:my-servicespec:selector:app:MyAppports:- protocol:TCPport:80targetPort:9376 上述配置创建一个名称为 “my-service” 的 Service 对象，它会将请求代理到使用 TCP 端口 9376，并且具有标签 \"app=MyApp\" 的 Pod 上。 ","date":"2020-11-10","objectID":"/k8s_series_service/:3:1","tags":["kubernetes"],"title":"Kubernetes系列：Service","uri":"/k8s_series_service/"},{"categories":["Cloud Native"],"content":"2.2 多端口 对于某些服务，你需要公开多个端口。 Kubernetes 允许你在 Service 对象上配置多个端口定义。 为服务使用多个端口时，必须提供所有端口名称，以使它们无歧义。 一个例子， apiVersion:v1kind:Servicemetadata:name:my-servicespec:selector:app:MyAppports:- name:httpprotocol:TCPport:80targetPort:9376- name:httpsprotocol:TCPport:443targetPort:9377 说明： 与一般的Kubernetes名称一样，端口名称只能包含小写字母数字字符 和 -。 端口名称还必须以字母数字字符开头和结尾。 例如，名称 123-abc 和 web 有效，但是 123_abc 和 -web 无效。 ","date":"2020-11-10","objectID":"/k8s_series_service/:3:2","tags":["kubernetes"],"title":"Kubernetes系列：Service","uri":"/k8s_series_service/"},{"categories":["Cloud Native"],"content":"2.3 服务类型 对一些应用的某些部分（如前端），可能希望将其暴露给 Kubernetes 集群外部 的 IP 地址。 Kubernetes ServiceTypes 允许指定你所需要的 Service 类型，默认是 ClusterIP。 Type 的取值以及行为如下： ClusterIP：通过集群的内部 IP 暴露服务，选择该值时服务只能够在集群内部访问。 这也是默认的 ServiceType。 NodePort：通过每个节点上的 IP 和静态端口（NodePort）暴露服务。 NodePort 服务会路由到自动创建的 ClusterIP 服务。 通过请求 \u003c节点 IP\u003e:\u003c节点端口\u003e，你可以从集群的外部访问一个 NodePort 服务。 LoadBalancer：使用云提供商的负载均衡器向外部暴露服务。 外部负载均衡器可以将流量路由到自动创建的 NodePort 服务和 ClusterIP 服务上。 ExternalName：通过返回 CNAME 和对应值，可以将服务映射到 externalName 字段的内容（例如，foo.bar.example.com）。 无需创建任何类型代理。 NodePort例子： apiVersion:v1kind:Servicemetadata:name:my-servicespec:type:NodePortselector:app:MyAppports:# 默认情况下，为了方便起见，`targetPort` 被设置为与 `port` 字段相同的值。- port:80targetPort:80# 可选字段# 默认情况下，为了方便起见，Kubernetes 控制平面会从某个范围内分配一个端口号（默认：30000-32767）nodePort:30007 LoadBalnacer例子： apiVersion:v1kind:Servicemetadata:name:my-servicespec:selector:app:MyAppports:- protocol:TCPport:80targetPort:9376clusterIP:10.0.171.239type:LoadBalancerstatus:loadBalancer:ingress:- ip:192.0.2.127 来自外部负载均衡器的流量将直接重定向到后端 Pod 上，不过实际它们是如何工作的，这要依赖于云提供商。 ","date":"2020-11-10","objectID":"/k8s_series_service/:3:3","tags":["kubernetes"],"title":"Kubernetes系列：Service","uri":"/k8s_series_service/"},{"categories":["Cloud Native"],"content":"3. 工作原理 在 Kubernetes 集群中，每个 Node 运行一个 kube-proxy 进程。 kube-proxy 负责为 Service 实现了一种 VIP（虚拟 IP）的形式，而不是 ExternalName 的形式。 ","date":"2020-11-10","objectID":"/k8s_series_service/:4:0","tags":["kubernetes"],"title":"Kubernetes系列：Service","uri":"/k8s_series_service/"},{"categories":["Cloud Native"],"content":"3.1 为什么不使用DNS轮询 使用服务代理有以下几个原因： DNS 实现的历史由来已久，它不遵守记录 TTL，并且在名称查找结果到期后对其进行缓存。 有些应用程序仅执行一次 DNS 查找，并无限期地缓存结果。 即使应用和库进行了适当的重新解析，DNS 记录上的 TTL 值低或为零也可能会给 DNS 带来高负载，从而使管理变得困难。 ","date":"2020-11-10","objectID":"/k8s_series_service/:4:1","tags":["kubernetes"],"title":"Kubernetes系列：Service","uri":"/k8s_series_service/"},{"categories":["Cloud Native"],"content":"3.2 userspace 代理模式 这种模式，kube-proxy 会监视 Kubernetes 控制平面对 Service 对象和 Endpoints 对象的添加和移除操作。 对每个 Service，它会在本地 Node 上打开一个端口（随机选择）。 任何连接到“代理端口”的请求，都会被代理到 Service 的后端 Pods 中的某个上面（如 Endpoints 所报告的一样）。 使用哪个后端 Pod，是 kube-proxy 基于 SessionAffinity 来确定的。 最后，它配置 iptables 规则，捕获到达该 Service 的 clusterIP（是虚拟 IP） 和 Port 的请求，并重定向到代理端口，代理端口再代理请求到后端Pod。 默认情况下，用户空间模式下的 kube-proxy 通过轮转算法选择后端。 ","date":"2020-11-10","objectID":"/k8s_series_service/:4:2","tags":["kubernetes"],"title":"Kubernetes系列：Service","uri":"/k8s_series_service/"},{"categories":["Cloud Native"],"content":"3.3 iptables模式 iptables相关原理可以看下这篇文章 这种模式，kube-proxy 会监视 Kubernetes 控制节点对 Service 对象和 Endpoints 对象的添加和移除。 对每个 Service，它会配置 iptables 规则，从而捕获到达该 Service 的 clusterIP 和端口的请求，进而将请求重定向到 Service 的一组后端中的某个 Pod 上面。 对于每个 Endpoints 对象，它也会配置 iptables 规则，这个规则会选择一个后端组合。 默认的策略是，kube-proxy 在 iptables 模式下随机选择一个后端。 使用 iptables 处理流量具有较低的系统开销，因为流量由 Linux netfilter 处理， 而无需在用户空间和内核空间之间切换。 这种方法也可能更可靠。 如果 kube-proxy 在 iptables 模式下运行，并且所选的第一个 Pod 没有响应， 则连接失败。 这与用户空间模式不同：在这种情况下，kube-proxy 将检测到与第一个 Pod 的连接已失败， 并会自动使用其他后端 Pod 重试。 你可以使用 Pod 就绪探测器 验证后端 Pod 可以正常工作，以便 iptables 模式下的 kube-proxy 仅看到测试正常的后端。 这样做意味着你避免将流量通过 kube-proxy 发送到已知已失败的 Pod。 例子 # 以aws为例,cluster ip为10.100.254.226，endpoint为172.31.178.122:80,172.31.178.161:80,172.31.179.80:80,nodeport为30028 # pod or node --\u003e cluster ip --\u003e endpoint *nat -A PREROUTING -m comment --comment \"kubernetes service portals\" -j KUBE-SERVICES #pod所有流量先进入KUBE-SERVICES检查 -A OUTPUT -m comment --comment \"kubernetes service portals\" -j KUBE-SERVICES #node流量通过OUTPUT进入KUBE-SERVICES -A KUBE-SERVICES -d 10.100.254.226/32 -p tcp -m comment --comment \"ops-test/nginx-service: cluster IP\" -m tcp --dport 80 -j KUBE-SVC-473SUSYUDXM6XRRH #匹配目的ip为10.100.254.226 #随机选择后端 -A KUBE-SVC-473SUSYUDXM6XRRH -m statistic --mode random --probability 0.33333333349 -j KUBE-SEP-WWIE6AZWAXCTMCNN -A KUBE-SVC-473SUSYUDXM6XRRH -m statistic --mode random --probability 0.50000000000 -j KUBE-SEP-ZBMAWDEBUXPXQHDH -A KUBE-SVC-473SUSYUDXM6XRRH -j KUBE-SEP-VPHXZB6KBMWRSLML #将目标地址转换为172.31.178.243:9300 -A KUBE-SEP-VPHXZB6KBMWRSLML -s 172.31.179.80/32 -j KUBE-MARK-MASQ -A KUBE-SEP-VPHXZB6KBMWRSLML -p tcp -m tcp -j DNAT --to-destination 172.31.179.80:80 通过路由规则找对应的pod(pod到pod间通信) # endpoint --\u003e cluster ip --\u003e pod or node 回来的包经过conntrack模块直接做SNAT操作 # externel --\u003e nodeport --\u003e endpoint *nat -A PREROUTING -m comment --comment \"kubernetes service portals\" -j KUBE-SERVICES #外部所有流量先进入KUBE-SERVICES检查 -A KUBE-SERVICES -m comment --comment \"kubernetes service nodeports; NOTE: this must be the last rule in this chain\" -m addrtype --dst-type LOCAL -j KUBE-NODEPORTS #KUBE-SERVICES最后一条进入KUBE-NODEPORTS -A KUBE-NODEPORTS -p tcp -m comment --comment \"ops-test/nginx-service:\" -m tcp --dport 30028 -j KUBE-MARK-MASQ #打标 -A KUBE-NODEPORTS -p tcp -m comment --comment \"ops-test/nginx-service:\" -m tcp --dport 30028 -j KUBE-SVC-473SUSYUDXM6XRRH #后续的DNAT跟cluster ip类似 # endpoint --\u003e nodeport 回来的包经过conntrack模块直接做SNAT操作，转换成nodeport # nodeport --\u003e externel -A KUBE-POSTROUTING -m comment --comment \"kubernetes service traffic requiring SNAT\" -m mark --mark 0x4000/0x4000 -j MASQUERADE --random-fully #跟外部交互需要做SNAT ","date":"2020-11-10","objectID":"/k8s_series_service/:4:3","tags":["kubernetes"],"title":"Kubernetes系列：Service","uri":"/k8s_series_service/"},{"categories":["Cloud Native"],"content":"3.4 ipv 代理模式 在 ipvs 模式下，kube-proxy 监视 Kubernetes 服务和端点，调用 netlink 接口相应地创建 IPVS 规则， 并定期将 IPVS 规则与 Kubernetes 服务和端点同步。 该控制循环可确保IPVS 状态与所需状态匹配。访问服务时，IPVS 将流量定向到后端Pod之一。 IPVS代理模式基于类似于 iptables 模式的 netfilter 挂钩函数， 但是使用哈希表作为基础数据结构，并且在内核空间中工作。 这意味着，与 iptables 模式下的 kube-proxy 相比，IPVS 模式下的 kube-proxy 重定向通信的延迟要短，并且在同步代理规则时具有更好的性能。 与其他代理模式相比，IPVS 模式还支持更高的网络流量吞吐量。 IPVS 提供了更多选项来平衡后端 Pod 的流量。 这些是： rr：轮替（Round-Robin） lc：最少链接（Least Connection），即打开链接数量最少者优先 dh：目标地址哈希（Destination Hashing） sh：源地址哈希（Source Hashing） sed：最短预期延迟（Shortest Expected Delay） nq：从不排队（Never Queue） 说明： 要在 IPVS 模式下运行 kube-proxy，必须在启动 kube-proxy 之前使 IPVS 在节点上可用。 当 kube-proxy 以 IPVS 代理模式启动时，它将验证 IPVS 内核模块是否可用。 如果未检测到 IPVS 内核模块，则 kube-proxy 将退回到以 iptables 代理模式运行。 # pod or node --\u003e cluster ip --\u003e endpoint -A PREROUTING -m comment --comment \"kubernetes service portals\" -j KUBE-SERVICES -A OUTPUT -m comment --comment \"kubernetes service portals\" -j KUBE-SERVICES -A KUBE-SERVICES -m set --match-set KUBE-CLUSTER-IP dst,dst -j ACCEPT ipvs处理dnat，进入POSTROUTING链 # endpoint --\u003e cluster ip --\u003e pod or node 回来的包经过conntrack模块直接做SNAT操作 # externel --\u003e nodeport --\u003e endpoint *nat -A PREROUTING -m comment --comment \"kubernetes service portals\" -j KUBE-SERVICES -A KUBE-SERVICES -m addrtype --dst-type LOCAL -j KUBE-NODE-PORT -A KUBE-NODE-PORT -p tcp -m comment --comment \"Kubernetes nodeport TCP port with externalTrafficPolicy=local\" -m set --match-set KUBE-NODE-PORT-LOCAL-TCP dst -j RETURN -A KUBE-NODE-PORT -p tcp -m comment --comment \"Kubernetes nodeport TCP port for masquerade purpose\" -m set --match-set KUBE-NODE-PORT-TCP dst -j KUBE-MARK-MASQ ipvs处理dnat，进入POSTROUTING链 # endpoint --\u003e nodeport 回来的包经过conntrack模块直接做SNAT操作，转换成nodeport -A POSTROUTING -m comment --comment \"kubernetes postrouting rules\" -j KUBE-POSTROUTING -A POSTROUTING -s 169.254.123.0/24 ! -o docker0 -j MASQUERADE -A KUBE-POSTROUTING -m comment --comment \"kubernetes service traffic requiring SNAT\" -m mark --mark 0x4000/0x4000 -j MASQUERADE -A KUBE-POSTROUTING -m comment --comment \"Kubernetes endpoints dst ip:port, source ip for solving hairpin purpose\" -m set --match-set KUBE-LOOP-BACK dst,dst,src -j MASQUERADE ipvs 会使用 iptables 进行包过滤、SNAT、masquared(伪装)。具体来说，ipvs 将使用ipset来存储需要DROP或masquared的流量的源或目标地址，以确保 iptables 规则的数量是恒定的，这样我们就不需要关心我们有多少服务了 下表就是 ipvs 使用的 ipset 集合： set name members usage KUBE-CLUSTER-IP All service IP + port Mark-Masq for cases that masquerade-all=true or clusterCIDR specified KUBE-LOOP-BACK All service IP + port + IP masquerade for solving hairpin purpose KUBE-EXTERNAL-IP service external IP + port masquerade for packages to external IPs KUBE-LOAD-BALANCER load balancer ingress IP + port masquerade for packages to load balancer type service KUBE-LOAD-BALANCER-LOCAL LB ingress IP + port with externalTrafficPolicy=local accept packages to load balancer with externalTrafficPolicy=local KUBE-LOAD-BALANCER-FW load balancer ingress IP + port with loadBalancerSourceRanges package filter for load balancer with loadBalancerSourceRanges specified KUBE-LOAD-BALANCER-SOURCE-CIDR load balancer ingress IP + port + source CIDR package filter for load balancer with loadBalancerSourceRanges specified KUBE-NODE-PORT-TCP nodeport type service TCP port masquerade for packets to nodePort(TCP) KUBE-NODE-PORT-LOCAL-TCP nodeport type service TCP port with externalTrafficPolicy=local accept packages to nodeport service with externalTrafficPolicy=local KUBE-NODE-PORT-UDP nodeport type service UDP port masquerade for packets to nodePort(UDP) KUBE-NODE-PORT-LOCAL-UDP nodeport type service UDP port with externalTrafficPolicy=local accept packages to nodeport service with externalTrafficPolicy=local ","date":"2020-11-10","objectID":"/k8s_series_service/:4:4","tags":["kubernetes"],"title":"Kubernetes系列：Service","uri":"/k8s_series_service/"},{"categories":["Cloud Native"],"content":"3.5 第三方插件，基于BPF/XDP实现K8S Service功能 [译] 基于 BPF/XDP 实现 K8s Service 负载均衡 (LPC, 2020) 3.5.1 Socket 层负载均衡（东西向流量） Socket 层 BPF 负载均衡负责处理集群内的东西向流量。 实现方式是：将 BPF 程序 attach 到 socket 的系统调用 hooks，使客户端直接和后端 pod 建连和通信，如下图所示，这里能 hook 的系统调用包括 connect()、sendmsg()、 recvmsg()、getpeername()、bind() 等， 这里的一个问题是，K8s 使用的还是 cgroup v1，但这个功能需要使用 v2， 而由于兼容性问题，v2 完全替换 v1 还需要很长时间。所以我们目前所能做的就是 支持 v1 和 v2 的混合模式。这也是为什么 Cilium 会 mount 自己的 cgroup v2 instance 的原因（将宿主机 /var/run/cilium/cgroupv2 mount 到 cilium-agent 容器内，译注）。 Cilium mounts cgroup v2, attaches BPF to root cgroup. Hybrid use works well for root v2. 具体到实现上， connect + sendmsg 做正向变换（translation） recvmsg + getpeername 做反向变换， 这个变换或转换是基于 socket structure 的，此时还没有创建 packet，因此 **不存在 packet 级别的 NAT！**目前已经支持 TCP/UDP v4/v6, v4-in-v6。 应用对此是无感知的，它以为自己连接到的还是 Service IP，但其实是 PodIP。 想知道 socket-level translation 具体是如何实现的， 可参考 Cracking kubernetes node proxy (aka kube-proxy)， 其中有一个 20 多行 bpf 代码实现的例子，可认为是 Cilium 相关代码的极度简化。译注。 查找后端pod Service lookup 不一定能选到所有的 backend pods（scoped lookup），我们将 backend pods 拆成不同的集合。 这样设计的好处：可以根据流量类型，例如是来自集群内还是集群外（ internal/external），来选择不同的 backends。例如，如果是到达 node 的 external traffic，我们可以限制它只能选择本机上的 backend pods，这样相比于转发到其他 node 上的 backend 就少了一跳。 另外，还支持通配符（wildcard）匹配，这样就能将 Service 暴露到 localhost 或者 loopback 地址，能在宿主机 netns 访问 Service。但这种方式不会将 Service 暴露到宿 主机外面。 显然，这种 socket 级别的转换是非常高效和实用的，它可以直接将客户端 pod 连 接到某个 backend pod，与 kube-proxy 这样的实现相比，转发路径少了好几跳。 此外，bind BPF 程序在 NodePort 冲突时会直接拒绝应用的请求，因此相比产生流 量（packet）然后在后面的协议栈中被拒绝，bind 这里要更加高效，因为此时 流量（packet）都还没有产生。 对这一功能至关重要的两个函数： bpf_get_socket_cookie() 主要用于 UDP sockets，我们希望每个 UDP flow 都能选中相同的 backend pods。 bpf_get_netns_cookie() 用在两个地方： 用于区分 host netns 和 pod netns，例如检测到在 host netns 执行 bind 时，直接拒绝（reject）； 用于 serviceSessionAffinity，实现在某段时间内永远选择相同的 backend pods。 由于 cgroup v2 不感知 netns，因此在这个 context 中我们没用 Pod 源 IP 信 息，通过这个 helper 能让它感知到源 IP，并以此作为它的 source identifier。 3.5.2 TC \u0026 XDP 层负载均衡（南北向流量） 第二种是进出集群的流量，称为南北向流量，在宿主机 tc 或 XDP hook 里处理。 BPF 做的事情，将入向流量转发到后端 Pod， 如果 Pod 在本节点，做 DNAT； 如果在其他节点，还需要做 SNAT 或者 DSR。 这些都是 packet 级别的操作。 ","date":"2020-11-10","objectID":"/k8s_series_service/:4:5","tags":["kubernetes"],"title":"Kubernetes系列：Service","uri":"/k8s_series_service/"},{"categories":["Cloud Native"],"content":"4. 服务发现 Kubernetes 支持两种基本的服务发现模式 —— 环境变量和 DNS。 ","date":"2020-11-10","objectID":"/k8s_series_service/:5:0","tags":["kubernetes"],"title":"Kubernetes系列：Service","uri":"/k8s_series_service/"},{"categories":["Cloud Native"],"content":"环境变量 当 Pod 运行在 Node 上，kubelet 会为每个活跃的 Service 添加一组环境变量。 它同时支持 Docker links兼容 变量 （查看 makeLinkVariables）、 简单的 {SVCNAME}_SERVICE_HOST 和 {SVCNAME}_SERVICE_PORT 变量。 这里 Service 的名称需大写，横线被转换成下划线。 举个例子，一个名称为 redis-master 的 Service 暴露了 TCP 端口 6379， 同时给它分配了 Cluster IP 地址 10.0.0.11，这个 Service 生成了如下环境变量： REDIS_MASTER_SERVICE_HOST=10.0.0.11 REDIS_MASTER_SERVICE_PORT=6379 REDIS_MASTER_PORT=tcp://10.0.0.11:6379 REDIS_MASTER_PORT_6379_TCP=tcp://10.0.0.11:6379 REDIS_MASTER_PORT_6379_TCP_PROTO=tcp REDIS_MASTER_PORT_6379_TCP_PORT=6379 REDIS_MASTER_PORT_6379_TCP_ADDR=10.0.0.11 说明： 当你具有需要访问服务的 Pod 时，并且你正在使用环境变量方法将端口和集群 IP 发布到客户端 Pod 时，必须在客户端 Pod 出现 之前 创建服务。 否则，这些客户端 Pod 将不会设定其环境变量。 如果仅使用 DNS 查找服务的集群 IP，则无需担心此设定问题。 ","date":"2020-11-10","objectID":"/k8s_series_service/:5:1","tags":["kubernetes"],"title":"Kubernetes系列：Service","uri":"/k8s_series_service/"},{"categories":["Cloud Native"],"content":"DNS 你可以（几乎总是应该）使用附加组件 为 Kubernetes 集群设置 DNS 服务。 支持集群的 DNS 服务器（例如 CoreDNS）监视 Kubernetes API 中的新服务，并为每个服务创建一组 DNS 记录。 如果在整个集群中都启用了 DNS，则所有 Pod 都应该能够通过其 DNS 名称自动解析服务。 例如，如果你在 Kubernetes 命名空间 my-ns 中有一个名为 my-service 的服务， 则控制平面和 DNS 服务共同为 my-service.my-ns 创建 DNS 记录。 my-ns 命名空间中的 Pod 应该能够通过简单地按名检索 my-service 来找到它 （my-service.my-ns 也可以工作）。 其他命名空间中的 Pod 必须将名称限定为 my-service.my-ns。 这些名称将解析为为服务分配的集群 IP。 Kubernetes 还支持命名端口的 DNS SRV（服务）记录。 如果 my-service.my-ns 服务具有名为 http　的端口，且协议设置为 TCP， 则可以对 _http._tcp.my-service.my-ns 执行 DNS SRV 查询查询以发现该端口号, \"http\" 以及 IP 地址。 Kubernetes DNS 服务器是唯一的一种能够访问 ExternalName 类型的 Service 的方式。 更多关于 ExternalName 信息可以查看 DNS Pod 和 Service。 ","date":"2020-11-10","objectID":"/k8s_series_service/:5:2","tags":["kubernetes"],"title":"Kubernetes系列：Service","uri":"/k8s_series_service/"},{"categories":["Cloud Native"],"content":"5. 无头服务（Headless Services） 有时不需要或不想要负载均衡，以及单独的 Service IP。 遇到这种情况，可以通过指定 Cluster IP（spec.clusterIP）的值为 \"None\" 来创建 Headless Service。 你可以使用无头 Service 与其他服务发现机制进行接口，而不必与 Kubernetes 的实现捆绑在一起。 对这无头 Service 并不会分配 Cluster IP，kube-proxy 不会处理它们， 而且平台也不会为它们进行负载均衡和路由。 DNS 如何实现自动配置，依赖于 Service 是否定义了选择算符。 ","date":"2020-11-10","objectID":"/k8s_series_service/:6:0","tags":["kubernetes"],"title":"Kubernetes系列：Service","uri":"/k8s_series_service/"},{"categories":["Cloud Native"],"content":"带选择算符的服务 对定义了选择算符的无头服务，Endpoint 控制器在 API 中创建了 Endpoints 记录， 并且修改 DNS 配置返回 A 记录（地址），通过这个地址直接到达 Service 的后端 Pod 上。 ","date":"2020-11-10","objectID":"/k8s_series_service/:6:1","tags":["kubernetes"],"title":"Kubernetes系列：Service","uri":"/k8s_series_service/"},{"categories":["Cloud Native"],"content":"无选择算符的服务 对没有定义选择算符的无头服务，Endpoint 控制器不会创建 Endpoints 记录。 然而 DNS 系统会查找和配置，无论是： 对于 ExternalName 类型的服务，查找其 CNAME 记录 对所有其他类型的服务，查找与 Service 名称相同的任何 Endpoints 的记录 ","date":"2020-11-10","objectID":"/k8s_series_service/:6:2","tags":["kubernetes"],"title":"Kubernetes系列：Service","uri":"/k8s_series_service/"},{"categories":["Cloud Native"],"content":"6. 设计哲学 ","date":"2020-11-10","objectID":"/k8s_series_service/:7:0","tags":["kubernetes"],"title":"Kubernetes系列：Service","uri":"/k8s_series_service/"},{"categories":["Cloud Native"],"content":"避免冲突 Kubernetes 最主要的哲学之一，是用户不应该暴露那些能够导致他们操作失败、但又不是他们的过错的场景。 对于 Service 资源的设计，这意味着如果用户的选择有可能与他人冲突，那就不要让用户自行选择端口号。 这是一个隔离性的失败。 为了使用户能够为他们的 Service 选择一个端口号，我们必须确保不能有2个 Service 发生冲突。 Kubernetes 通过为每个 Service 分配它们自己的 IP 地址来实现。 为了保证每个 Service 被分配到一个唯一的 IP，需要一个内部的分配器能够原子地更新 etcd 中的一个全局分配映射表， 这个更新操作要先于创建每一个 Service。 为了使 Service 能够获取到 IP，这个映射表对象必须在注册中心存在， 否则创建 Service 将会失败，指示一个 IP 不能被分配。 在控制平面中，一个后台 Controller 的职责是创建映射表 （需要支持从使用了内存锁的 Kubernetes 的旧版本迁移过来）。 同时 Kubernetes 会通过控制器检查不合理的分配（如管理员干预导致的） 以及清理已被分配但不再被任何 Service 使用的 IP 地址。 ","date":"2020-11-10","objectID":"/k8s_series_service/:7:1","tags":["kubernetes"],"title":"Kubernetes系列：Service","uri":"/k8s_series_service/"},{"categories":["Cloud Native"],"content":"Service IP 地址 不像 Pod 的 IP 地址，它实际路由到一个固定的目的地，Service 的 IP 实际上 不能通过单个主机来进行应答。 相反，我们使用 iptables（Linux 中的数据包处理逻辑）来定义一个 虚拟 IP 地址（VIP），它可以根据需要透明地进行重定向。 当客户端连接到 VIP 时，它们的流量会自动地传输到一个合适的 Endpoint。 环境变量和 DNS，实际上会根据 Service 的 VIP 和端口来进行填充。 kube-proxy支持三种代理模式: 用户空间，iptables和IPVS；它们各自的操作略有不同。 Userspace 作为一个例子，考虑前面提到的图片处理应用程序。 当创建后端 Service 时，Kubernetes master 会给它指派一个虚拟 IP 地址，比如 10.0.0.1。 假设 Service 的端口是 1234，该 Service 会被集群中所有的 kube-proxy 实例观察到。 当代理看到一个新的 Service， 它会打开一个新的端口，建立一个从该 VIP 重定向到 新端口的 iptables，并开始接收请求连接。 当一个客户端连接到一个 VIP，iptables 规则开始起作用，它会重定向该数据包到 “服务代理” 的端口。 “服务代理” 选择一个后端，并将客户端的流量代理到后端上。 这意味着 Service 的所有者能够选择任何他们想使用的端口，而不存在冲突的风险。 客户端可以简单地连接到一个 IP 和端口，而不需要知道实际访问了哪些 Pod。 iptables 再次考虑前面提到的图片处理应用程序。 当创建后端 Service 时，Kubernetes 控制面板会给它指派一个虚拟 IP 地址，比如 10.0.0.1。 假设 Service 的端口是 1234，该 Service 会被集群中所有的 kube-proxy 实例观察到。 当代理看到一个新的 Service， 它会配置一系列的 iptables 规则，从 VIP 重定向到每个 Service 规则。 该特定于服务的规则连接到特定于 Endpoint 的规则，而后者会重定向（目标地址转译）到后端。 当客户端连接到一个 VIP，iptables 规则开始起作用。一个后端会被选择（或者根据会话亲和性，或者随机）， 数据包被重定向到这个后端。 不像用户空间代理，数据包从来不拷贝到用户空间，kube-proxy 不是必须为该 VIP 工作而运行， 并且客户端 IP 是不可更改的。 当流量打到 Node 的端口上，或通过负载均衡器，会执行相同的基本流程， 但是在那些案例中客户端 IP 是可以更改的。 IPVS 在大规模集群（例如 10000 个服务）中，iptables 操作会显着降低速度。 IPVS 专为负载平衡而设计，并基于内核内哈希表。 因此，你可以通过基于 IPVS 的 kube-proxy 在大量服务中实现性能一致性。 同时，基于 IPVS 的 kube-proxy 具有更复杂的负载均衡算法（最小连接、局部性、 加权、持久性）。 ","date":"2020-11-10","objectID":"/k8s_series_service/:7:2","tags":["kubernetes"],"title":"Kubernetes系列：Service","uri":"/k8s_series_service/"},{"categories":["Cloud Native"],"content":"7. 结论 该篇，我们了解了service是什么，以及怎么在k8s定义一个service，并简单了解了service的实现原理。 参考 https://kubernetes.io/zh/docs/concepts/services-networking/service/ ","date":"2020-11-10","objectID":"/k8s_series_service/:8:0","tags":["kubernetes"],"title":"Kubernetes系列：Service","uri":"/k8s_series_service/"},{"categories":["Linux"],"content":"深入理解LVS","date":"2020-11-09","objectID":"/deep_lvs/","tags":["linux","kubernets","security"],"title":"深入理解LVS","uri":"/deep_lvs/"},{"categories":["Linux"],"content":"1. 介绍 接上一篇深入理解iptables，kubernetes service技术还用到ipvs技术，讲到ipvs，那就得说说LVS了，这篇我们来了解下LVS具体的实现机制。 ","date":"2020-11-09","objectID":"/deep_lvs/:1:0","tags":["linux","kubernets","security"],"title":"深入理解LVS","uri":"/deep_lvs/"},{"categories":["Linux"],"content":"2. IPVS IPVS（IP虚拟服务器）实现传输层负载均衡，通常称为第4层LAN交换。 负载均衡器的概念可以看这篇文章，或者翻译版本。 大多数情况下，负载均衡器和代理这两个术语会被混用在一起，所谓的代理，简单来说，就是接收客户端的数据包再转发到对应的后端服务器上。 ipvs就在这样的软件，它依赖netfilter的功能来实现数据包的转发，我们还是先拉源码定义来看下。 ip_vs_core.c static const struct nf_hook_ops ip_vs_ops4[] = { /* After packet filtering, change source only for VS/NAT */ { .hook = ip_vs_reply4, .pf = NFPROTO_IPV4, .hooknum = NF_INET_LOCAL_IN, .priority = NF_IP_PRI_NAT_SRC - 2, }, /* After packet filtering, forward packet through VS/DR, VS/TUN, * or VS/NAT(change destination), so that filtering rules can be * applied to IPVS. */ { .hook = ip_vs_remote_request4, .pf = NFPROTO_IPV4, .hooknum = NF_INET_LOCAL_IN, .priority = NF_IP_PRI_NAT_SRC - 1, }, /* Before ip_vs_in, change source only for VS/NAT */ { .hook = ip_vs_local_reply4, .pf = NFPROTO_IPV4, .hooknum = NF_INET_LOCAL_OUT, .priority = NF_IP_PRI_NAT_DST + 1, }, /* After mangle, schedule and forward local requests */ { .hook = ip_vs_local_request4, .pf = NFPROTO_IPV4, .hooknum = NF_INET_LOCAL_OUT, .priority = NF_IP_PRI_NAT_DST + 2, }, /* After packet filtering (but before ip_vs_out_icmp), catch icmp * destined for 0.0.0.0/0, which is for incoming IPVS connections */ { .hook = ip_vs_forward_icmp, .pf = NFPROTO_IPV4, .hooknum = NF_INET_FORWARD, .priority = 99, }, /* After packet filtering, change source only for VS/NAT */ { .hook = ip_vs_reply4, .pf = NFPROTO_IPV4, .hooknum = NF_INET_FORWARD, .priority = 100, }, }; 根据定义，ipvs挂载在LOCAL_IN、FORWORD和LOCAL_OUT的hook点上了，根据优先级，我们在iptables流程图上新增了ipvs的位置 ipvs内部自己针对不同的模式定义了不同hook，在后续的lvs模式中我们再细说。从上图可以看到，ipvs是在不同的hook点上，增加了自己的逻辑，将数据包转发出去。 ","date":"2020-11-09","objectID":"/deep_lvs/:2:0","tags":["linux","kubernets","security"],"title":"深入理解LVS","uri":"/deep_lvs/"},{"categories":["Linux"],"content":"3. LVS Linux虚拟服务器（LVS）是服务器的群集，对于外部客户端而言似乎是一个服务器。 ","date":"2020-11-09","objectID":"/deep_lvs/:3:0","tags":["linux","kubernets","security"],"title":"深入理解LVS","uri":"/deep_lvs/"},{"categories":["Linux"],"content":"3.1 术语 ipvs，在director上执行代理逻辑的代码 lvs，director+realserver，这些服务构成了一个虚拟服务器对client提供服务。 director，运行ipvs代码的节点。client连接到director。director将数据包转发给realserver。director不过是具有使LVS工作的特殊规则的IP路由器 realserver，提供真实服务的主机，处理来自client的请求。 client，连接到director的VIP上的主机或用户级别的进程。 forwarding method(NAT|DR|Tun)，director是一台路由器，与普通路由器相比，其转发数据包的规则有所不同。forwarding method确定director如何将数据包从client发送到realserver。 scheduling，director用来选择realserver以服务于来自client的新连接请求的算法。 ","date":"2020-11-09","objectID":"/deep_lvs/:3:1","tags":["linux","kubernets","security"],"title":"深入理解LVS","uri":"/deep_lvs/"},{"categories":["Linux"],"content":"3.2 LVS基础架构 在LVS上会有各个IP，我们先看下各个IP代表的组件，便于理解后续的内容。 client IP = CIP virtual IP = VIP - the IP on the director that the client connects to director IP = DIP - the IP on the director in the DIP/RIP (DRIP) network (this is the realserver gateway for LVS-NAT) realserver IP = RIP (and RIP1, RIP2...) the IP on the realserver director GW = DGW - the director's gw (only needed for LVS-NAT) (this can be the realserver gateway for LVS-DR and LVS-Tun) ","date":"2020-11-09","objectID":"/deep_lvs/:3:2","tags":["linux","kubernets","security"],"title":"深入理解LVS","uri":"/deep_lvs/"},{"categories":["Linux"],"content":"3.3 转发模式 为便于理解，我们用iptables上的chain来代替各个hook点。 3.3.1 NAT模式原理 外部请求 当client请求到达director的VIP时，先经过了PREROUTING，然后经过路由判断，为本机ip，进入INPUT，ipvs会判断是首次进入的包，还是已经建立连接的数据包。 当为首次进入的包，执行ip_vs_remote_request的hook，进行realserver的选择，更改目标ip为RIP，当为已经建立连接的数据包，执行ip_vs_reply的hook，直接转发到对应的realserver，更改目标ip为RIP， realserver收到请求后，处理请求，返回的数据要先经过到网关(即director) director收到请求后，判断目标ip为CIP，不是本机ip，进入FORWARD，执行ip_vs_reply的hook，将源ip改为VIP，发送给client 本地请求 当local_client请求往外发送时，经过了OUTPUT，当为发次连接建立的包时，此时执行ip_vs_local_request的hook，进行realserver选择，将目标ip改为RIP，当为已经建立连接的数据包，则执行ip_vs_local_reply，直接转发给对应的realserver。 realserver收到请求后，处理请求，返回数据要先经过到网关(即director) director收到请求后，判断目标ip为CIP，并且是本机IP，进入INPUT，执行ip_vs_reply的hook，将源ip改为VIP，发送给local_client。 特性 RIP 最好是内网ip realserver的网关必须指向DIP DIP和RIP必须在同一个网段内 请求和回应的报文必须经过director，因此director容易成为瓶颈 NAT支持端口转发 3.3.2 DR模式原理 注： realserver必须配置VIP在local网卡上，不然请求到realserver后，realserver会认为不是到本机的请求而丢弃或者转发 开启arp抑制 client发送请求到director的VIP上，经过PREROUTING后判断为本机IP，进入INPUT，当为首次请求，执行ip_vs_remote_request的逻辑，若是已建立连接的请求，执行ip_vs_reply的逻辑，将VIP-MAC改为RIP-MAC，VIP不变。 经过arp解析后，realserver收到请求，解析后发现为本机IP，做请求处理。并响应客户端请求。 realserver通过自己连接的对应的交换机路由器，将数据包发送给客户端。 DR模式下director的状态机问题 由上面可知，DR模式下，director只接收请求，不回复请求，那么在director上面，TCP连接的状态机是怎样的呢？ 我们知道，一般来说，服务端会有这么几个TCP状态，LISTEN、SYN-RECEIVED、ESTABLISH、CLOSE_WAIT、LAST_ACK状态，但是，director实际上并不处理请求，那当客户端的请求过来后，director要怎么更新自己的状态呢？ 当收到TCP包中SYN的标志位为1时，director将该连接的的状态机更新为SYN-RECEIVED 当收到TCP包中ACK的标志位为1时，就将TCP状态机更新为ESTABLISH 当收到TCP包中FIN和ACK标志位为1时，就将TCP状态机更新为LAST_ACK状态 特性 保证前端路由将目标地址为VIP的报文统统发给director，而不是realserver director和realserver的VIP为同一个VIP realserver可以使用私有地址，也可以是公网地址。如果使用公网地址，此时可以通过互联网对RIP进行直接访问 realserver跟director必须在同一个物理网络中 所有的请求报文经过director，但响应报文必须不能经过realserver 不支持地址转换，也不支持端口映射 realserver可以是大多数常见的操作系统 realserver的网关决不允许指向DIP（因为我们不允许响应经过director) realserver上的lo接口绑定VIP地址 DR模式是市面上用的最广的。 3.3.3 TUN模式 Tun模式基于DR模式，不过增加了一步IP隧道封装，实现了跨网段传输的功能。 client发送请求到director的VIP上，经过PREROUTING后判断为本机IP，进入INPUT，当为首次请求，执行ip_vs_remote_request的逻辑，若是已建立连接的请求，执行ip_vs_reply的逻辑，在原IP首部上添加新的源IP为DIP，目的IP为RIP的IP首部，然后通过POSTROUTING发送出去 经过路由，到达真实的realserver 经过arp解析后，realserver收到请求，经过两层解析后发现为本机IP，做请求处理。并响应客户端请求。 realserver通过自己连接的对应的交换机路由器，将数据包发送给客户端。 特性 RIP、VIP、DIP全是公网地址 realserver的网关不会也不可能指向DIP 所有的请求报文经过director，但响应报文必须不能经过director 不支持端口映射 realserver的系统必须支持IP隧道 ","date":"2020-11-09","objectID":"/deep_lvs/:3:3","tags":["linux","kubernets","security"],"title":"深入理解LVS","uri":"/deep_lvs/"},{"categories":["Linux"],"content":"3.4 ARP广播问题 在DR模式下时，会存在一个问题，我所有的realserver和director都配置了VIP，从网络模型中，我们知道，最终传输的是mac地址，那么这个时候，到底谁的mac地址是准确的呢？ 我们要保证请求的VIP必须是director，这样我们的负载均衡才是生效的，因此要在realserver上进行ARP抑制配置，禁止它处理外部的arp请求，也不允许自己向外部广播ARP数据 echo \"1\" \u003e/proc/sys/net/ipv4/conf/lo/arp_ignore ## 忽略收到的ARP请求 echo \"2\" \u003e/proc/sys/net/ipv4/conf/lo/arp_announce ## lo接口不对外广播ARP数据 echo \"1\" \u003e/proc/sys/net/ipv4/conf/all/arp_ignore echo \"2\" \u003e/proc/sys/net/ipv4/conf/all/arp_announce arp_ignore: 定义接收到ARP请求时的相应级别 0 -- 只要本地配置有相应地址，就给予响应 1 -- 仅回应目标IP地址为接收网卡本地地址的ARP请求 2 -- 只响应目标IP地址为接收网卡本地地址的ARP请求，并且arp请求的源IP必须和接收网卡同网段 4~7 -- 保留未使用 8 -- 不回应所有ARP请求 arp_announce: 定义将自己的地址向外通告时的通告级别 0 -- 将本地任何接口上的任何地址向外通告 1 -- 试图仅向目标网络通告与其网络匹配的地址 2 -- 仅向与本地接口上地址匹配的网络进行通告 ","date":"2020-11-09","objectID":"/deep_lvs/:3:4","tags":["linux","kubernets","security"],"title":"深入理解LVS","uri":"/deep_lvs/"},{"categories":["Linux"],"content":"3.5 Keepalived相关 假设主备 director包含rs， 可以这样处理： 经过 director1 的包，如果 mac address 不是 director2 的，用 iptables 给包打 mark=i 经过 director2 的包，如果 mac address 不是 director1 的，用 iptables 给包打 mark=j 同时配置 LVS，不用三元组(ip,port,protocol)来表示 virtual_server，而用 fwmark-service，keepalived 配置 lvs 使用 fwmark-service。 这样，如果是 director 转发过来的包，就不会进入 LVS 进行负载（防止两个 director 互相扔皮球，进入死循环），而是被 RS 服务处理。而客户端进来的包，就会进入 LVS 进行负载。 iptables -t mangle -I PREROUTING -d $VIP -p tcp -m tcp --dport $VPORT -m mac ! --mac-source $MAC_Director2 -j MARK --set-mark 0x3 iptables -t mangle -I PREROUTING -d $VIP -p tcp -m tcp --dport $VPORT -m mac ! --mac-source $MAC_Director1 -j MARK --set-mark 0x4 #keealived configuration virtual_server fwmark 3 { # node2 配置 fwmark 4 delay_loop 10 lb_algo rr lb_kind DR protocol TCP real_server RIP1 8080 { weight 1 MISC_CHECK { # some check configuration } } real_server RIP2 8080 { weight 1 MISC_CHECK { # some check configuration } } ","date":"2020-11-09","objectID":"/deep_lvs/:3:5","tags":["linux","kubernets","security"],"title":"深入理解LVS","uri":"/deep_lvs/"},{"categories":["Linux"],"content":"4. 总结 本篇讲述了ipvs的工作机制，以及LVS相关模式的实现原理。在kubernetes下的话用的是ipvs的nat模式。搞懂了这个，对理解kubernetes的service帮助很大。 参考 lvs how to linux kernel https://ivanzz1001.github.io/records/post/lb/2018/06/01/lb_lvs_part2 ","date":"2020-11-09","objectID":"/deep_lvs/:4:0","tags":["linux","kubernets","security"],"title":"深入理解LVS","uri":"/deep_lvs/"},{"categories":["Linux"],"content":"深入理解iptables","date":"2020-11-07","objectID":"/deep_iptables/","tags":["linux","kubernets","security"],"title":"深入理解iptables","uri":"/deep_iptables/"},{"categories":["Linux"],"content":"1. 介绍 最近在刚好在看Kubernetes的service相关内容，里面用到了iptables和ipvs技术，好久没看iptables了，快忘记了，刚好复习重新记忆一下。 讲iptables和ipvs，有个东西就一定得清楚，那就是netfilter ","date":"2020-11-07","objectID":"/deep_iptables/:1:0","tags":["linux","kubernets","security"],"title":"深入理解iptables","uri":"/deep_iptables/"},{"categories":["Linux"],"content":"2. netfilter netfilter是一个数据包处理框架。 netfilter具备以下几个功能： 数据包过滤 网络地址(端口)转换 数据包日志记录 用户空间数据包排队 其他数据包处理功能 ","date":"2020-11-07","objectID":"/deep_iptables/:2:0","tags":["linux","kubernets","security"],"title":"深入理解iptables","uri":"/deep_iptables/"},{"categories":["Linux"],"content":"2.1 netfilter架构 netfilter 提供了 5 个 hook 点。包经过协议栈时会触发内核模块注册在这里的处理函数 。触发哪个 hook 取决于包的方向（是发送还是接收）、包的目的地址、以及包在上一个 hook 点是被丢弃还是拒绝等等。 下面几个 hook 是内核协议栈中已经定义好的： NF_IP_PRE_ROUTING: 接收到的包进入协议栈后立即触发此 hook，在进行任何路由判断 （将包发往哪里）之前 NF_IP_LOCAL_IN: 接收到的包经过路由判断，如果目的是本机，将触发此 hook NF_IP_FORWARD: 接收到的包经过路由判断，如果目的是其他机器，将触发此 hook NF_IP_LOCAL_OUT: 本机产生的准备发送的包，在进入协议栈后立即触发此 hook NF_IP_POST_ROUTING: 本机产生的准备发送的包或者转发的包，在经过路由判断之后， 将触发此 hook 注册处理函数时必须提供优先级，以便 hook 触发时能按照 优先级高低调用处理函数。这使得多个模块（或者同一内核模块的多个实例）可以在同一 hook 点注册，并且有确定的处理顺序。内核模块会依次被调用，每次返回一个结果给 netfilter 框架，提示该对这个包做以下几个操作之一： NF_ACCEPT: 继续正常遍历 NF_DROP: 丢弃数据包，不再进行遍历 NF_STOLEN: 该模块接收了该包，不再进行遍历 NF_QUEUE: 将数据包排队（通常用于用户空间处理） NF_REPEAT: 再次调用此hook ","date":"2020-11-07","objectID":"/deep_iptables/:2:1","tags":["linux","kubernets","security"],"title":"深入理解iptables","uri":"/deep_iptables/"},{"categories":["Linux"],"content":"3. iptables iptables是建立在netfilter框架上的数据包选择系统。可以用于配置数据包过滤规则集。 ","date":"2020-11-07","objectID":"/deep_iptables/:3:0","tags":["linux","kubernets","security"],"title":"深入理解iptables","uri":"/deep_iptables/"},{"categories":["Linux"],"content":"3.1 表和链（Tables and Chains） iptables 使用 table 来组织规则，根据用来做什么类型的判断（the type of decisions they are used to make）标准，将规则分为不同 table。 filter： 最常用的 table 之一，用于判断是否允许一个包通过。 nat：用于实现网络地址转换规则。当包进入协议栈的时候，这些规则决定是否以及如何修改包的源/目的地址，以改变包被 路由时的行为。nat table 通常用于将包路由到无法直接访问的网络。 mangle：用于修改包的 IP 头。例如，可以修改包的 TTL，增加或减少包可以经过的跳数。这个 table 还可以对包打只在内核内有效的“标记”（internal kernel “mark”），后 续的 table 或工具处理的时候可以用到这些标记。标记不会修改包本身，只是在包的内核 表示上做标记。 raw：iptables 防火墙是有状态的：对每个包进行判断的时候是依赖已经判断过的包。建立在 netfilter 之上的连接跟踪（connection tracking）特性使得 iptables 将包 看作已有的连接或会话的一部分，而不是一个由独立、不相关的包组成的流。连接跟踪逻 辑在包到达网络接口之后很快就应用了。raw table 唯一目的就是提供一个让包绕过连接跟踪的框架。 security table 的作用是给包打上 SELinux 标记，以此影响 SELinux 或其他可以解读 SELinux 安全上下文的系统处理包的行为。这些标记可以基于单个包，也可以基于连接。 在每个 table 内部，规则被进一步组织成 chain，内置的 chain 是由内置的 hook 触发 的。chain 基本上能决定（basically determin）规则何时被匹配。 下面可以看出，内置的 chain 名字和 netfilter hook 名字是一一对应的： PREROUTING: 由 NF_IP_PRE_ROUTING hook 触发 INPUT: 由 NF_IP_LOCAL_IN hook 触发 FORWARD: 由 NF_IP_FORWARD hook 触发 OUTPUT: 由 NF_IP_LOCAL_OUT hook 触发 POSTROUTING: 由 NF_IP_POST_ROUTING hook 触发 chain 使管理员可以控制在包的传输路径上哪个点（where in a packet’s delivery path）应用策略。因为每个 table 有多个 chain，因此一个 table 可以在处理过程中的多 个地方施加影响。特定类型的规则只在协议栈的特定点有意义，因此并不是每个 table 都 会在内核的每个 hook 注册 chain。 内核一共只有 5 个 netfilter hook，因此不同 table 的 chain 最终都是注册到这几个点 。例如，有三个 table 有 PRETOUTING chain。当这些 chain 注册到对应的 NF_IP_PRE_ROUTING hook 点时，它们需要指定优先级，应该依次调用哪个 table 的 PRETOUTING chain，优先级从高到低。我们一会就会看到 chain 的优先级问题。 ","date":"2020-11-07","objectID":"/deep_iptables/:3:1","tags":["linux","kubernets","security"],"title":"深入理解iptables","uri":"/deep_iptables/"},{"categories":["Linux"],"content":"3.2 优先级 前面已经分别讨论了 table 和 chain，接下来看每个 table 里各有哪些 chain。另外，我 们还将讨论注册到同一 hook 的不同 chain 的优先级问题。 首先我们拉linux源码的定义来看一下 iptable_filter.c ... /** filter表挂载在LOCAL_IN、FORWORD、LOCAL_OUT hook点上 **/ #define FILTER_VALID_HOOKS ((1 \u003c\u003c NF_INET_LOCAL_IN) | \\ (1 \u003c\u003c NF_INET_FORWARD) | \\ (1 \u003c\u003c NF_INET_LOCAL_OUT)) ... static const struct xt_table packet_filter = { .name = \"filter\", .valid_hooks = FILTER_VALID_HOOKS, .me = THIS_MODULE, .af = NFPROTO_IPV4, .priority = NF_IP_PRI_FILTER, .table_init = iptable_filter_table_init, }; ... iptable_mangle.c ... /** mangle表挂载在所有的hook点上 **/ #define MANGLE_VALID_HOOKS ((1 \u003c\u003c NF_INET_PRE_ROUTING) | \\ (1 \u003c\u003c NF_INET_LOCAL_IN) | \\ (1 \u003c\u003c NF_INET_FORWARD) | \\ (1 \u003c\u003c NF_INET_LOCAL_OUT) | \\ (1 \u003c\u003c NF_INET_POST_ROUTING)) ... static const struct xt_table packet_mangler = { .name = \"mangle\", .valid_hooks = MANGLE_VALID_HOOKS, .me = THIS_MODULE, .af = NFPROTO_IPV4, .priority = NF_IP_PRI_MANGLE, .table_init = iptable_mangle_table_init, }; ... iptable_nat.c ... /** nat表挂载在PRE_ROUTING、POST_ROUTING、LOCAL_OUT和LOCAL_IN hook点上**/ static const struct xt_table nf_nat_ipv4_table = { .name = \"nat\", .valid_hooks = (1 \u003c\u003c NF_INET_PRE_ROUTING) | (1 \u003c\u003c NF_INET_POST_ROUTING) | (1 \u003c\u003c NF_INET_LOCAL_OUT) | (1 \u003c\u003c NF_INET_LOCAL_IN), .me = THIS_MODULE, .af = NFPROTO_IPV4, .table_init = iptable_nat_table_init, }; ... /** nat表又细分为dnat和snat，具备不同的优先级， dnat挂载在PRE_ROUTING和OUTPUT hook点上 snat挂载在INPUT和POSTROUTING hook点上 **/ static const struct nf_hook_ops nf_nat_ipv4_ops[] = { { .hook = iptable_nat_do_chain, .pf = NFPROTO_IPV4, .hooknum = NF_INET_PRE_ROUTING, .priority = NF_IP_PRI_NAT_DST, }, { .hook = iptable_nat_do_chain, .pf = NFPROTO_IPV4, .hooknum = NF_INET_POST_ROUTING, .priority = NF_IP_PRI_NAT_SRC, }, { .hook = iptable_nat_do_chain, .pf = NFPROTO_IPV4, .hooknum = NF_INET_LOCAL_OUT, .priority = NF_IP_PRI_NAT_DST, }, { .hook = iptable_nat_do_chain, .pf = NFPROTO_IPV4, .hooknum = NF_INET_LOCAL_IN, .priority = NF_IP_PRI_NAT_SRC, }, }; iptable_raw.c ... /** raw表挂载在PRE_ROUTING和LOCAL_OUT hook点上 **/ #define RAW_VALID_HOOKS ((1 \u003c\u003c NF_INET_PRE_ROUTING) | (1 \u003c\u003c NF_INET_LOCAL_OUT)) ... static const struct xt_table packet_raw = { .name = \"raw\", .valid_hooks = RAW_VALID_HOOKS, .me = THIS_MODULE, .af = NFPROTO_IPV4, .priority = NF_IP_PRI_RAW, .table_init = iptable_raw_table_init, }; static const struct xt_table packet_raw_before_defrag = { .name = \"raw\", .valid_hooks = RAW_VALID_HOOKS, .me = THIS_MODULE, .af = NFPROTO_IPV4, .priority = NF_IP_PRI_RAW_BEFORE_DEFRAG, .table_init = iptable_raw_table_init, }; iptable_security.c ... /** security表挂载在LOCAL_IN、FORWARD和LOCAL_OUT hook点上 **/ #define SECURITY_VALID_HOOKS (1 \u003c\u003c NF_INET_LOCAL_IN) | \\ (1 \u003c\u003c NF_INET_FORWARD) | \\ (1 \u003c\u003c NF_INET_LOCAL_OUT) ... static const struct xt_table security_table = { .name = \"security\", .valid_hooks = SECURITY_VALID_HOOKS, .me = THIS_MODULE, .af = NFPROTO_IPV4, .priority = NF_IP_PRI_SECURITY, .table_init = iptable_security_table_init, }; netfilter_ipv4.h ... /* IP Hooks */ /* After promisc drops, checksum checks. */ #define NF_IP_PRE_ROUTING 0 /* If the packet is destined for this box. */ #define NF_IP_LOCAL_IN 1 /* If the packet is destined for another interface. */ #define NF_IP_FORWARD 2 /* Packets coming from a local process. */ #define NF_IP_LOCAL_OUT 3 /* Packets about to hit the wire. */ #define NF_IP_POST_ROUTING 4 #define NF_IP_NUMHOOKS 5 #endif /* ! __KERNEL__ */ /* 定义了各表的优先级 */ enum nf_ip_hook_priorities { NF_IP_PRI_FIRST = INT_MIN, NF_IP_PRI_RAW_BEFORE_DEFRAG = -450, NF_IP_PRI_CONNTRACK_DEFRAG = -400, NF_IP_PRI_RAW = -300, NF_IP_PRI_SELINUX_FIRST = -225, NF_IP_PRI_CONNTRACK = -200, NF_IP_PRI_MANGLE = -150, NF_IP_PRI_NAT_DST = -100, NF_IP_PRI_FILTER = 0, NF_IP_PRI_SECURITY = 50, NF_IP_PRI_NAT_SRC = 100, NF_IP_PRI_SELINUX_LAST = 225, NF_IP_PRI_CONNTRACK_HELPER = 300, NF_IP_PRI_CONNTRACK_CONFIRM = INT_MAX, NF_IP_PRI_LAST = INT_MAX, }; ... 优先级的定义为值越小，优先级越高，我们用下面的表格来展示了 table 和 chain 的关系。横向是 table， 纵向是 chain，Y 表示 这个 table 里面有这个 chain。例如，第二行表示 raw table 有 PRETOUTING 和 OUTPUT 两 个 cha","date":"2020-11-07","objectID":"/deep_iptables/:3:2","tags":["linux","kubernets","security"],"title":"深入理解iptables","uri":"/deep_iptables/"},{"categories":["Linux"],"content":"4. 规则 规则放置在特定 table 的特定 chain 里面。当 chain 被调用的时候，包会依次匹配 chain 里面的规则。每条规则都有一个匹配部分和一个动作部分。 ","date":"2020-11-07","objectID":"/deep_iptables/:4:0","tags":["linux","kubernets","security"],"title":"深入理解iptables","uri":"/deep_iptables/"},{"categories":["Linux"],"content":"4.1 匹配 规则的匹配部分指定了一些条件，包必须满足这些条件才会和相应的将要执行的动作（“ target”）进行关联。 匹配系统非常灵活，还可以通过 iptables extension 大大扩展其功能。规则可以匹配协 议类型、目的或源地址、目的或源端口、目的或源网段、接收或发送的接口（网卡）、协议 头、连接状态等等条件。这些综合起来，能够组合成非常复杂的规则来区分不同的网络流 量。 ","date":"2020-11-07","objectID":"/deep_iptables/:4:1","tags":["linux","kubernets","security"],"title":"深入理解iptables","uri":"/deep_iptables/"},{"categories":["Linux"],"content":"4.2 目标 包符合某种规则的条件而触发的动作（action）叫做目标（target）。目标分为两种类型： 终止目标（terminating targets）：这种 target 会终止 chain 的匹配，将控制权 转移回 netfilter hook。根据返回值的不同，hook 或者将包丢弃，或者允许包进行下一 阶段的处理 非终止目标（non-terminating targets）：非终止目标执行动作，然后继续 chain 的执行。虽然每个 chain 最终都会回到一个终止目标，但是在这之前，可以执行任意多 个非终止目标 每个规则可以跳转到哪个 target 依上下文而定，例如，table 和 chain 可能会设置 target 可用或不可用。规则里激活的 extensions 和匹配条件也影响 target 的可用性。 ","date":"2020-11-07","objectID":"/deep_iptables/:4:2","tags":["linux","kubernets","security"],"title":"深入理解iptables","uri":"/deep_iptables/"},{"categories":["Linux"],"content":"5. 自定义 chain 这里要介绍一种特殊的非终止目标：跳转目标（jump target）。jump target 是跳转到其 他 chain 继续处理的动作。我们已经讨论了很多内置的 chain，它们和调用它们的 netfilter hook 紧密联系在一起。然而，iptables 也支持管理员创建他们自己的用于管理 目的的 chain。 向用户自定义 chain 添加规则和向内置的 chain 添加规则的方式是相同的。不同的地方 在于，用户定义的 chain 只能通过从另一个规则跳转（jump）到它，因为它们没有注册到 netfilter hook。 用户定义的 chain 可以看作是对调用它的 chain 的扩展。例如，用户定义的 chain 在结 束的时候，可以返回 netfilter hook，也可以继续跳转到其他自定义 chain。 这种设计使框架具有强大的分支功能，使得管理员可以组织更大更复杂的网络规则。 ","date":"2020-11-07","objectID":"/deep_iptables/:5:0","tags":["linux","kubernets","security"],"title":"深入理解iptables","uri":"/deep_iptables/"},{"categories":["Linux"],"content":"6. 连接跟踪 在讨论 raw table 和 匹配连接状态的时候，我们介绍了构建在 netfilter 之上的连 接跟踪系统。连接跟踪系统使得 iptables 基于连接上下文而不是单个包来做出规则判 断，给 iptables 提供了有状态操作的功能。 连接跟踪在包进入协议栈之后很快（very soon）就开始工作了。在给包分配连接之前所做 的工作非常少，只有检查 raw table 和一些基本的完整性检查。 跟踪系统将包和已有的连接进行比较，如果包所属的连接已经存在就更新连接状态，否则就创建一个新连接。如果 raw table 的某个chain对包标记为目标是 NOTRACK，那这 个包会跳过连接跟踪系统。 ","date":"2020-11-07","objectID":"/deep_iptables/:6:0","tags":["linux","kubernets","security"],"title":"深入理解iptables","uri":"/deep_iptables/"},{"categories":["Linux"],"content":"连接的状态 连接跟踪系统中的连接状态有： NEW：如果到达的包关连不到任何已有的连接，但包是合法的，就为这个包创建一个新连接。对 面向连接的（connection-aware）的协议例如 TCP 以及非面向连接的（connectionless ）的协议例如 UDP 都适用 ESTABLISHED：当一个连接收到应答方向的合法包时，状态从 NEW 变成 ESTABLISHED。对 TCP 这个合法包其实就是 SYN/ACK 包；对 UDP 和 ICMP 是源和目 的 IP 与原包相反的包 RELATED：包不属于已有的连接，但是和已有的连接有一定关系。这可能是辅助连接（ helper connection），例如 FTP 数据传输连接，或者是其他协议试图建立连接时的 ICMP 应答包 INVALID：包不属于已有连接，并且因为某些原因不能用来创建一个新连接，例如无法 识别、无法路由等等 UNTRACKED：如果在 raw table 中标记为目标是 UNTRACKED，这个包将不会进入连 接跟踪系统 SNAT：包的源地址被 NAT 修改之后会进入的虚拟状态。连接跟踪系统据此在收到 反向包时对地址做反向转换 DNAT：包的目的地址被 NAT 修改之后会进入的虚拟状态。连接跟踪系统据此在收到 反向包时对地址做反向转换 这些状态可以定位到连接生命周期内部，管理员可以编写出更加细粒度、适用范围更大、更 安全的规则。 ","date":"2020-11-07","objectID":"/deep_iptables/:6:1","tags":["linux","kubernets","security"],"title":"深入理解iptables","uri":"/deep_iptables/"},{"categories":["Linux"],"content":"7. 总结 netfilter 包过滤框架和 iptables 防火墙是 Linux 服务器上大部分防火墙解决方案的基础。netfilter 的内核 hook 和协议栈足够紧密，提供了包经过系统时的强大控制功能。 iptables 防火墙基于这些功能提供了一个灵活的、可扩展的、将策略需求转化到内核的方 法。而k8s的service使用到的一种方式就是通过iptables加自定义chain来实现的。最后，再提供一张wiki上面的内核协议栈各 hook 点位置和 iptables 规则优先级的经典配图 参考 https://www.digitalocean.com/community/tutorials/a-deep-dive-into-iptables-and-netfilter-architecture https://www.netfilter.org/documentation/HOWTO/netfilter-hacking-HOWTO-3.html linux kernel source code ","date":"2020-11-07","objectID":"/deep_iptables/:7:0","tags":["linux","kubernets","security"],"title":"深入理解iptables","uri":"/deep_iptables/"},{"categories":["Cloud Native"],"content":"存储","date":"2020-11-05","objectID":"/k8s_series_storage/","tags":["kubernetes"],"title":"Kubernetes系列：存储","uri":"/k8s_series_storage/"},{"categories":["Cloud Native"],"content":"系列目录 《Kubernetes系列：开篇》 《Kubernetes系列：概述》 《Kubernetes系列：架构》 《Kubernetes系列：容器》 《Kubernetes系列：网络》 《Kubernetes系列：存储》 《Kubernetes系列：Service》 《Kubernetes系列：Ingress》 《Kubernetes系列：OAM》 ","date":"2020-11-05","objectID":"/k8s_series_storage/:1:0","tags":["kubernetes"],"title":"Kubernetes系列：存储","uri":"/k8s_series_storage/"},{"categories":["Cloud Native"],"content":"1. 介绍 容器中的文件在磁盘上是临时存放的，这给容器中运行的较重要的应用程序带来一些问题： 当容器崩溃时文件丢失。kubelet 会重新启动容器， 但容器会以干净的状态重启。 在 Pod 中同时运行多个容器时，这些容器之间通常需要共享文件。 Kubernetes 中的 Volume 抽象很好的解决了这些问题。 ","date":"2020-11-05","objectID":"/k8s_series_storage/:2:0","tags":["kubernetes"],"title":"Kubernetes系列：存储","uri":"/k8s_series_storage/"},{"categories":["Cloud Native"],"content":"2. 存储 为了管理存储，Kubernetes提供了Secret用于管理敏感信息，ConfigMap存储配置，Volume、PV、PVC、StorageClass等用来管理存储卷。 ","date":"2020-11-05","objectID":"/k8s_series_storage/:3:0","tags":["kubernetes"],"title":"Kubernetes系列：存储","uri":"/k8s_series_storage/"},{"categories":["Cloud Native"],"content":"2.1 Volume Kubernetes 支持很多类型的卷。 Pod 可以同时使用任意数目的卷类型。 临时卷类型的生命周期与 Pod 相同，但持久卷可以比 Pod 的存活期长。 因此，卷的存在时间会超出 Pod 中运行的所有容器，并且在容器重新启动时数据也会得到保留。 当 Pod 不再存在时，卷也将不再存在。 卷的核心是包含一些数据的一个目录，Pod 中的容器可以访问该目录。 所采用的特定的卷类型将决定该目录如何形成的、使用何种介质保存数据以及目录中存放 的内容。 使用卷时, 在 .spec.volumes 字段中设置为 Pod 提供的卷，并在 .spec.containers[*].volumeMounts 字段中声明卷在容器中的挂载位置。 容器中的进程看到的是由它们的 Docker 镜像和卷组成的文件系统视图。 Docker 镜像 位于文件系统层次结构的根部。各个卷则挂载在镜像内的指定路径上。 卷不能挂载到其他卷之上，也不能与其他卷有硬链接。 Pod 配置中的每个容器必须独立指定各个卷的挂载位置。 2.1.1 卷类型 Kubernetes 支持下列类型的卷：详细 awsElasticBlockStore awsElasticBlockStore 卷将 Amazon Web服务（AWS）EBS 卷 挂载到你的 Pod 中。与 emptyDir 在 Pod 被删除时也被删除不同，EBS 卷的内容在删除 Pod 时 会被保留，卷只是被卸载掉了。 这意味着 EBS 卷可以预先填充数据，并且该数据可以在 Pod 之间共享。 说明： 你在使用 EBS 卷之前必须使用 aws ec2 create-volume 命令或者 AWS API 创建该卷。 使用 awsElasticBlockStore 卷时有一些限制： Pod 运行所在的节点必须是 AWS EC2 实例。 这些实例需要与 EBS 卷在相同的地域（Region）和可用区（Availability-Zone）。 EBS 卷只支持被挂载到单个 EC2 实例上。 azureDisk azureDisk 卷类型用来在 Pod 上挂载 Microsoft Azure 数据盘（Data Disk） 。 若需了解更多详情，请参考 azureDisk 卷插件。 azureFile azureFile 卷类型用来在 Pod 上挂载 Microsoft Azure 文件卷（File Volume）（SMB 2.1 和 3.0）。 更多详情请参考 azureFile 卷插件。 cephfs cephfs 卷允许你将现存的 CephFS 卷挂载到 Pod 中。 不像 emptyDir 那样会在 Pod 被删除的同时也会被删除，cephfs 卷的内容在 Pod 被删除 时会被保留，只是卷被卸载了。这意味着 cephfs 卷可以被预先填充数据，且这些数据可以在 Pod 之间共享。同一 cephfs 卷可同时被多个写者挂载。 说明： 在使用 Ceph 卷之前，你的 Ceph 服务器必须已经运行并将要使用的 share 导出（exported）。 更多信息请参考 CephFS 示例。 cinder 说明： Kubernetes 必须配置了 OpenStack Cloud Provider。 cinder 卷类型用于将 OpenStack Cinder 卷挂载到 Pod 中。 configMap configMap 卷 提供了向 Pod 注入配置数据的方法。 ConfigMap 对象中存储的数据可以被 configMap 类型的卷引用，然后被 Pod 中运行的 容器化应用使用。 引用 configMap 对象时，你可以在 volume 中通过它的名称来引用。 你可以自定义 ConfigMap 中特定条目所要使用的路径。 下面的配置显示了如何将名为 log-config 的 ConfigMap 挂载到名为 configmap-pod 的 Pod 中： apiVersion:v1kind:Podmetadata:name:configmap-podspec:containers:- name:testimage:busyboxvolumeMounts:- name:config-volmountPath:/etc/configvolumes:- name:config-volconfigMap:name:log-configitems:- key:log_levelpath:log_level log-config ConfigMap 以卷的形式挂载，并且存储在 log_level 条目中的所有内容 都被挂载到 Pod 的 /etc/config/log_level 路径下。 请注意，这个路径来源于卷的 mountPath 和 log_level 键对应的 path。 说明： 在使用 ConfigMap 之前你首先要创建它。 容器以 subPath 卷挂载方式使用 ConfigMap 时，将无法接收 ConfigMap 的更新。 文本数据挂载成文件时采用 UTF-8 字符编码。如果使用其他字符编码形式，可使用 binaryData 字段。 downwardAPI downwardAPI 卷用于使 downward API 数据对应用程序可用。 这种卷类型挂载一个目录并在纯文本文件中写入所请求的数据。 说明： 容器以 subPath 卷挂载方式使用 downwardAPI 时，将不能接收到它的更新。 更多详细信息请参考 downwardAPI 卷示例。 emptyDir 当 Pod 分派到某个 Node 上时，emptyDir 卷会被创建，并且在 Pod 在该节点上运行期间，卷一直存在。 就像其名称表示的那样，卷最初是空的。 尽管 Pod 中的容器挂载 emptyDir 卷的路径可能相同也可能不同，这些容器都可以读写 emptyDir 卷中相同的文件。 当 Pod 因为某些原因被从节点上删除时，emptyDir 卷中的数据也会被永久删除。 说明： 容器崩溃并不会导致 Pod 被从节点上移除，因此容器崩溃期间 emptyDir 卷中的数据是安全的。 emptyDir 的一些用途： 缓存空间，例如基于磁盘的归并排序。 为耗时较长的计算任务提供检查点，以便任务能方便地从崩溃前状态恢复执行。 在 Web 服务器容器服务数据时，保存内容管理器容器获取的文件。 取决于你的环境，emptyDir 卷存储在该节点所使用的介质上；这里的介质可以是磁盘或 SSD 或网络存储。但是，你可以将 emptyDir.medium 字段设置为 \"Memory\"，以告诉 Kubernetes 为你挂载 tmpfs（基于 RAM 的文件系统）。 虽然 tmpfs 速度非常快，但是要注意它与磁盘不同。 tmpfs 在节点重启时会被清除，并且你所写入的所有文件都会计入容器的内存消耗，受容器内存限制约束。 说明： 当启用 SizeMemoryBackedVolumes 特性门控时， 你可以为基于内存提供的卷指定大小。 如果未指定大小，则基于内存的卷的大小为 Linux 主机上内存的 50％。 emptyDir 配置示例 apiVersion:v1kind:Podmetadata:name:test-pdspec:containers:- image:k8s.gcr.io/test-webservername:test-containervolumeMounts:- mountPath:/cachename:cache-volumevolumes:- name:cache-volumeemptyDir:{} fc (光纤通道) fc 卷类型允许将现有的光纤通道块存储卷挂载到 Pod 中。 可以使用卷配置中的参数 targetWWNs 来指定单个或多个目标 WWN（World Wide Names）。 如果指定了多个 WWN，targetWWNs 期望这些 WWN 来自多路径连接。 注意： 你必须配置 FC SAN Zoning，以便预先向目标 WWN 分配和屏蔽这些 LUN（卷）， 这样 Kubernetes 主机才可以访问它们。 更多详情请参考 FC 示例。 gcePersistentDisk gcePersistentDisk 卷能将谷歌计算引擎 (GCE) 持久盘（PD） 挂载到你的 Pod 中。 不像 emptyDir 那样会在 Pod 被删除的同时也会被删除，持久盘卷的内容在删除 Pod 时会被保留，卷只是被卸载了。 这意味着持久盘卷可以被预先填充数据，并且这些数据可以在 Pod 之间共享。 注意： 在使用 PD 前，你必须使用 gcloud 或者 GCE API 或 UI 创建它。 使用 gcePersistentDisk 时有一些限制： 运行 Pod 的节点必须是 GCE VM 这些 VM 必须和持久盘位于相同的 GCE 项目和区域（zone） GCE PD 的一个特点是它们可以同时被多个消费者以只读方式挂载。 这意味着你可以用数据集预先填充 PD，然后根据需要并行地在尽可能多的 Pod 中提供该数据集。 不幸的是，PD 只能由单个使用者以读写模式挂载 —— 即不允许同时写入。 在由 ReplicationController 所管理的 Pod 上使用 GCE PD 将会失败，除非 PD 是只读模式或者副本的数量是 0 或 1。 glusterfs glusterfs 卷能将 Glusterfs (一个开源的网络文件系统) 挂载到你的 Pod 中。不像 empty","date":"2020-11-05","objectID":"/k8s_series_storage/:3:1","tags":["kubernetes"],"title":"Kubernetes系列：存储","uri":"/k8s_series_storage/"},{"categories":["Cloud Native"],"content":"2.2 ConfigMap ConfigMap 是一种 API 对象，用来将非机密性的数据保存到键值对中。使用时， Pods 可以将其用作环境变量、命令行参数或者存储卷中的配置文件。 ConfigMap 将您的环境配置信息和 容器镜像 解耦，便于应用配置的修改。 注意： ConfigMap 并不提供保密或者加密功能。 如果你想存储的数据是机密的，请使用 Secret， 或者使用其他第三方工具来保证你的数据的私密性，而不是用 ConfigMap。 2.2.1 ConfigMap 对象 ConfigMap 是一个 API 对象， 让你可以存储其他对象所需要使用的配置。 和其他 Kubernetes 对象都有一个 spec 不同的是，ConfigMap 使用 data 和 binaryData 字段。这些字段能够接收键-值对作为其取值。data 和 binaryData 字段都是可选的。data 字段设计用来保存 UTF-8 字节序列，而 binaryData 则 被设计用来保存二进制数据。 ConfigMap 的名字必须是一个合法的 DNS 子域名。 data 或 binaryData 字段下面的每个键的名称都必须由字母数字字符或者 -、_ 或 . 组成。在 data 下保存的键名不可以与在 binaryData 下 出现的键名有重叠。 从 v1.19 开始，你可以添加一个 immutable 字段到 ConfigMap 定义中，创建 不可变更的 ConfigMap。 2.2.2 创建ConfigMap apiVersion:v1kind:ConfigMapmetadata:name:myconfigmapdata:# 类属性键；每一个键都映射到一个简单的值player_initial_lives:\"3\"ui_properties_file_name:\"user-interface.properties\"# 类文件键game.properties:|enemy.types=aliens,monsters player.maximum-lives=5 user-interface.properties:|color.good=purple color.bad=yellow allow.textmode=true 2.2.3 使用ConfigMap apiVersion:v1kind:Podmetadata:name:mypodspec:containers:- name:mypodimage:redisvolumeMounts:- name:foomountPath:\"/etc/foo\"readOnly:truevolumes:- name:fooconfigMap:name:myconfigmap ","date":"2020-11-05","objectID":"/k8s_series_storage/:3:2","tags":["kubernetes"],"title":"Kubernetes系列：存储","uri":"/k8s_series_storage/"},{"categories":["Cloud Native"],"content":"2.3 Secret Secret 对象类型用来保存敏感信息，例如密码、OAuth 令牌和 SSH 密钥。 将这些信息放在 secret 中比放在 Pod 的定义或者 容器镜像 中来说更加安全和灵活。 ecret 是一种包含少量敏感信息例如密码、令牌或密钥的对象。 这样的信息可能会被放在 Pod 规约中或者镜像中。 用户可以创建 Secret，同时系统也创建了一些 Secret。 注意： Kubernetes Secret 默认情况下存储为 base64-编码的、非加密的字符串。 默认情况下，能够访问 API 的任何人，或者能够访问 Kubernetes 下层数据存储（etcd） 的任何人都可以以明文形式读取这些数据。 为了能够安全地使用 Secret，我们建议你（至少）： 为 Secret 启用静态加密； 启用 RBAC 规则来限制对 Secret 的读写操作。 要注意，任何被允许创建 Pod 的人都默认地具有读取 Secret 的权限。 2.3.1 Secret概览 要使用 Secret，Pod 需要引用 Secret。 Pod 可以用三种方式之一来使用 Secret： 作为挂载到一个或多个容器上的 卷 中的文件。 作为容器的环境变量 由 kubelet 在为 Pod 拉取镜像时使用 Secret 对象的名称必须是合法的 DNS 子域名。 在为创建 Secret 编写配置文件时，你可以设置 data 与/或 stringData 字段。 data 和 stringData 字段都是可选的。data 字段中所有键值都必须是 base64 编码的字符串。如果不希望执行这种 base64 字符串的转换操作，你可以选择设置 stringData 字段，其中可以使用任何字符串作为其取值。 2.3.2 Secret类型 在创建 Secret 对象时，你可以使用 Secret 资源的 type 字段，或者与其等价的 kubectl 命令行参数（如果有的话）为其设置类型。 Secret 的类型用来帮助编写程序处理 Secret 数据。 Kubernetes 提供若干种内置的类型，用于一些常见的使用场景。 针对这些类型，Kubernetes 所执行的合法性检查操作以及对其所实施的限制各不相同。 内置类型 用法 Opaque 用户定义的任意数据 kubernetes.io/service-account-token 服务账号令牌 kubernetes.io/dockercfg ~/.dockercfg 文件的序列化形式 kubernetes.io/dockerconfigjson ~/.docker/config.json 文件的序列化形式 kubernetes.io/basic-auth 用于基本身份认证的凭据 kubernetes.io/ssh-auth 用于 SSH 身份认证的凭据 kubernetes.io/tls 用于 TLS 客户端或者服务器端的数据 bootstrap.kubernetes.io/token 启动引导令牌数据 通过为 Secret 对象的 type 字段设置一个非空的字符串值，你也可以定义并使用自己 Secret 类型。如果 type 值为空字符串，则被视为 Opaque 类型。 Kubernetes 并不对类型的名称作任何限制。不过，如果你要使用内置类型之一， 则你必须满足为该类型所定义的所有要求。 2.3.3 创建Secret $ echo -n '1f2d1e2e67df' | base64 MWYyZDFlMmU2N2Rm apiVersion:v1kind:Secretmetadata:name:mysecrettype:Opaquedata:username:YWRtaW4=password:MWYyZDFlMmU2N2Rm 2.3.4 使用Secret apiVersion:v1kind:Podmetadata:name:mypodspec:containers:- name:mypodimage:redisvolumeMounts:- name:foomountPath:\"/etc/foo\"readOnly:truevolumes:- name:foosecret:secretName:mysecret ","date":"2020-11-05","objectID":"/k8s_series_storage/:3:3","tags":["kubernetes"],"title":"Kubernetes系列：存储","uri":"/k8s_series_storage/"},{"categories":["Cloud Native"],"content":"2.4 PV 和 PVC 存储的管理是一个与计算实例的管理完全不同的问题。PersistentVolume 子系统为用户 和管理员提供了一组 API，将存储如何供应的细节从其如何被使用中抽象出来。 为了实现这点，我们引入了两个新的 API 资源：PersistentVolume 和 PersistentVolumeClaim。 2.4.1 PV 持久卷（PersistentVolume，PV）是集群中的一块存储，可以由管理员事先供应，或者 使用存储类（Storage Class）来动态供应。 持久卷是集群资源，就像节点也是集群资源一样。PV 持久卷和普通的 Volume 一样，也是使用 卷插件来实现的，只是它们拥有独立于任何使用 PV 的 Pod 的生命周期。 此 API 对象中记述了存储的实现细节，无论其背后是 NFS、iSCSI 还是特定于云平台的存储系统。 2.4.2 PVC 持久卷申领（PersistentVolumeClaim，PVC）表达的是用户对存储的请求。概念上与 Pod 类似。 Pod 会耗用节点资源，而 PVC 申领会耗用 PV 资源。Pod 可以请求特定数量的资源（CPU 和内存）；同样 PVC 申领也可以请求特定的大小和访问模式 （例如，可以要求 PV 卷能够以 ReadWriteOnce、ReadOnlyMany 或 ReadWriteMany 模式之一来挂载，参见访问模式）。 2.4.4 PV和PVC的生命周期 供应 PV 卷的供应有两种方式：静态供应或动态供应。 静态供应，集群管理员创建若干 PV 卷。这些卷对象带有真实存储的细节信息，并且对集群 用户可用（可见）。PV 卷对象存在于 Kubernetes API 中，可供用户消费（使用）。 动态供应，这一供应操作是基于 StorageClass 来实现的：PVC 申领必须请求某个 存储类，同时集群管理员必须 已经创建并配置了该类，这样动态供应卷的动作才会发生。 如果 PVC 申领指定存储类为 \"\"，则相当于为自身禁止使用动态供应的卷。 绑定 用户创建一个带有特定存储容量和特定访问模式需求的PVC对象；在静态供应中，该PVC要手动绑定PV，在动态供应场景下，这个 PVC 对象可能已经创建完毕。 主控节点中的控制回路监测新的 PVC 对象，寻找与之匹配的 PV 卷（如果可能的话）， 并将二者绑定到一起。一旦绑定关系建立，则 PersistentVolumeClaim 绑定就是排他性的，无论该 PVC 申领是 如何与 PV 卷建立的绑定关系。 PVC 申领与 PV 卷之间的绑定是一种一对一的映射，实现上使用 ClaimRef 来记述 PV 卷 与 PVC 申领间的双向绑定关系。 使用 Pod 将 PVC 申领当做存储卷来使用。集群会检视 PVC 申领，找到所绑定的卷，并 为 Pod 挂载该卷。 一旦用户有了申领对象并且该申领已经被绑定，则所绑定的 PV 卷在用户仍然需要它期间 一直属于该用户。用户通过在 Pod 的 volumes 块中包含 persistentVolumeClaim 节区来调度 Pod，访问所申领的 PV 卷。 回收 当用户不再使用其存储卷时，他们可以从 API 中将 PVC 对象删除，从而允许 该资源被回收再利用。PersistentVolume 对象的回收策略告诉集群，当其被 从申领中释放时如何处理该数据卷。 目前，数据卷可以被 Retained（保留）或 Deleted（删除）。 Retain，使得用户可以手动回收资源。当 PersistentVolumeClaim 对象 被删除时，PersistentVolume 卷仍然存在，对应的数据卷被视为\"已释放（released）\"。 由于卷上仍然存在这前一申领人的数据，该卷还不能用于其他申领。 管理员可以通过下面的步骤来手动回收该卷： 删除 PersistentVolume 对象。与之相关的、位于外部基础设施中的存储资产 （例如 AWS EBS、GCE PD、Azure Disk 或 Cinder 卷）在 PV 删除之后仍然存在。 根据情况，手动清除所关联的存储资产上的数据。 手动删除所关联的存储资产；如果你希望重用该存储资产，可以基于存储资产的 定义创建新的 PersistentVolume 卷对象。 Delete，对于支持 Delete 回收策略的卷插件，删除动作会将 PersistentVolume 对象从 Kubernetes 中移除，同时也会从外部基础设施（如 AWS EBS、GCE PD、Azure Disk 或 Cinder 卷）中移除所关联的存储资产。 动态供应的卷会继承其 StorageClass 中设置的回收策略，该策略默认 为 Delete。 管理员需要根据用户的期望来配置 StorageClass；否则 PV 卷被创建之后必须要被 编辑或者修补。 ","date":"2020-11-05","objectID":"/k8s_series_storage/:3:4","tags":["kubernetes"],"title":"Kubernetes系列：存储","uri":"/k8s_series_storage/"},{"categories":["Cloud Native"],"content":"2.5 StorageClass StorageClass 为管理员提供了描述存储 “类” 的方法。 不同的类型可能会映射到不同的服务质量等级或备份策略，或是由集群管理员制定的任意策略。 Kubernetes 本身并不清楚各种类代表的什么。这个类的概念在其他存储系统中有时被称为 “配置文件”。 每个 StorageClass 都包含 provisioner、parameters 和 reclaimPolicy 字段， 这些字段会在 StorageClass 需要动态分配 PersistentVolume 时会使用到。 StorageClass 对象的命名很重要，用户使用这个命名来请求生成一个特定的类。 当创建 StorageClass 对象时，管理员设置 StorageClass 对象的命名和其他参数，一旦创建了对象就不能再对其更新。 管理员可以为没有申请绑定到特定 StorageClass 的 PVC 指定一个默认的存储类 ： 更多详情请参阅 PersistentVolumeClaim 章节。 apiVersion:storage.k8s.io/v1kind:StorageClassmetadata:name:standardprovisioner:kubernetes.io/aws-ebsparameters:type:gp2reclaimPolicy:RetainallowVolumeExpansion:truemountOptions:- debugvolumeBindingMode:Immediate ","date":"2020-11-05","objectID":"/k8s_series_storage/:3:5","tags":["kubernetes"],"title":"Kubernetes系列：存储","uri":"/k8s_series_storage/"},{"categories":["Cloud Native"],"content":"3. 动态供应 面对计算层的多种存储需求，要考虑如何高效且灵活的提供存储服务，于是就有了动态供应的策略。动态供应能按需分配资源，大大减轻了运维工作，是目前最为推荐的一种方式，在很多企业生产环境中都有它的应用。 ","date":"2020-11-05","objectID":"/k8s_series_storage/:4:0","tags":["kubernetes"],"title":"Kubernetes系列：存储","uri":"/k8s_series_storage/"},{"categories":["Cloud Native"],"content":"3.1 如何实现动态供应 显然动态供应比静态供应灵活更多，而且这种方式还解耦了Kubernetes系统的计算层和存储层，更重要的是它给存储供应商提供了可插拔式的开发模型，存储供应商只需要根据这个模型开发相应的卷插件即可为Kubernetes提供存储服务。 有如下三种方法实现卷插件: In-tree Volume Plugin Out-of-tree Provisioner Out-of-tree CSI Driver 第一种，Kubernetes内部代码中实现了一些存储插件，用于支持一些主流网络存储，叫作In-tree Volume Plugin。 第二种 Out-of-tree Provisioner：如果官方的插件不能满足要求，存储供应商可以根据需要去定制或者优化存储插件并集成到Kubernetes系统。 第三种是容器存储接口CSI (Container Storage Interface)，是Kubernetes对外开放的存储接口，实现这个接口即可集成到Kubernetes系统中。CSI特性在刚过去的12月正式GA，同时社区也宣布未来将不再对In tree/Out of tree继续开发，并将已有功能全部迁移到CSI上，所以对于存储供应商和使用者来说，第三种CSI是更推荐的解决方案。 ","date":"2020-11-05","objectID":"/k8s_series_storage/:4:1","tags":["kubernetes"],"title":"Kubernetes系列：存储","uri":"/k8s_series_storage/"},{"categories":["Cloud Native"],"content":"3.2 几种卷插件对比 3.2.1 In-tree Volume Plugin 如上图所示，In-tree Volume Plugin是Kubernetes自带的，属于Kubernetes的一部分，由Kubernetes一起发布和维护，所有存储插件代码都集成在Kubernetes中，第三方存储供应商难以集成。 3.2.2 Out-of-tree Provisioner 其中挂载组件还是复用，只是把Provisioner组件从Kubernetes中挪出来，以供实现定制化或者自定义的高级功能。 创建流程（绿色）：状态更新，监听状态，调用外部供给器，结合后端存储服务，创建一个PV对象，这一步借助外部的Provisioner组件来完成。 挂载流程（橙色）：监听事件，插件挂载卷等操作由原有挂载组件继续负责。 3.2.3 Container Storage Interface (CSI) 一直以来，存储插件的测试、维护等事宜都由Kubernetes社区来完成，即使有贡献者提供协作也不容易合并到主分支发布。另外，存储插件需要随Kubernetes一同发布，如果存储插件存在问题有可能会影响Kubernetes其他组件的正常运行。 鉴于此，Kubernetes和CNCF决定把容器存储进行抽象，通过标准接口的形式把存储部分移到容器编排系统外部去。CSI的设计目的是定义一个行业标准，该标准将使存储供应商能够自己实现，维护和部署他们的存储插件。这些存储插件会以Sidecar Container形式运行在Kubernetes上并为容器平台提供稳定的存储服务。 如上CSI设计图：浅绿色表示从Kubernetes社区中抽离出来且可复用的组件，负责连接XSKY CSI插件（右侧）以及和Kubernetes集群交互： Driver-registrar: 使用 Kubelet注册 CSI 驱动程序的 sidecar 容器，并将 NodeId （通过 GetNodeID 调用检索到 CSI endpoint）添加到 Kubernetes Node API 对象的 annotation 里面； External-provisioner: 监听 Kubernetes PersistentVolumeClaim 对象的 sidecar 容器，并触发对 CSI 端点的 CreateVolume 和DeleteVolume 操作； External-attacher: 可监听 Kubernetes VolumeAttachment 对象并触发 ControllerPublish 和 ControllerUnPublish 操作的 sidecar 容器，负责attache/detache卷到node节点上。 右侧橘黄色表示XSKY实现的存储插件驱动，分别有三个服务： CSI identify: 标志插件服务，并维持插件健康状态； CSI Controller: 创建/删除，attaching/detaching，快照等； CSI Node: attach/mount、umount/detach。 动态供应方式总结：通过对比Kubernetes的In-tree Volume Plugin，以及Out-of-tree Provisioner和CSI三种方式，在对接比较常见的存储时，可以使用不需要改动的In-tree方案，因为开箱即用，但是缺点也非常明显，只支持有限的存储类型，可拓展性较差甚至有版本限制，另外官方宣布以后新特性将不再添加到其中。 相比之下，使用Out-of-tree Provisioner或者CSI则可以实现和Kubernetes的核心组件解耦，并能支持更多的存储类型和高级特性，因而也是推荐使用的一种供应方式。由于后者对编排系统而言是非侵入式插件部署，因而更受存储供应商的青睐。 ","date":"2020-11-05","objectID":"/k8s_series_storage/:4:2","tags":["kubernetes"],"title":"Kubernetes系列：存储","uri":"/k8s_series_storage/"},{"categories":["Cloud Native"],"content":"4. 结论 本章我们介绍kubernetes的存储方式，包括PV、PVC、StorageClass，并说明了动态供应的实现机制。 参考 https://jimmysong.io/kubernetes-handbook/concepts/persistent-volume.html https://kubernetes.io/blog/2019/01/15/container-storage-interface-ga/ https://github.com/kubernetes/community/blob/master/contributors/design-proposals/storage/container-storage-interface.md https://www.xsky.com/tec/5609/ ","date":"2020-11-05","objectID":"/k8s_series_storage/:5:0","tags":["kubernetes"],"title":"Kubernetes系列：存储","uri":"/k8s_series_storage/"},{"categories":["Cloud Native"],"content":"网络","date":"2020-10-30","objectID":"/k8s_series_network/","tags":["kubernetes"],"title":"Kubernetes系列：网络","uri":"/k8s_series_network/"},{"categories":["Cloud Native"],"content":"系列目录 《Kubernetes系列：开篇》 《Kubernetes系列：概述》 《Kubernetes系列：架构》 《Kubernetes系列：容器》 《Kubernetes系列：网络》 《Kubernetes系列：存储》 《Kubernetes系列：Service》 《Kubernetes系列：Ingress》 《Kubernetes系列：OAM》 ","date":"2020-10-30","objectID":"/k8s_series_network/:1:0","tags":["kubernetes"],"title":"Kubernetes系列：网络","uri":"/k8s_series_network/"},{"categories":["Cloud Native"],"content":"1. 介绍 网络是 Kubernetes 的核心部分，不过，Kubernetes本身并不提供网络功能，只是把网络接口开放出来，通过插件的形式实现。这就是CNI(Container Network Interface)。 CNI（Container Network Interface）是 CNCF 旗下的一个项目，由一组用于配置 Linux 容器的网络接口的规范和库组成，同时还包含了一些插件。CNI 仅关心容器创建时的网络分配，和当容器被删除时释放网络资源。通过此链接浏览该项目：https://github.com/containernetworking/cni。 Kubernetes 对所有网络设施的实施，都需要满足以下的基本要求： 节点上的 Pod 可以不通过 NAT 和其他任何节点上的 Pod 通信 节点上的代理(比如：系统守护进程、kubelet)可以和节点上的所有Pod通信 Pod自己的IP就是其他人看到的IP 因此，一个kubernetes网络插件必须要解决下面五个问题： Container-to-Container 网络通信 Pod-to-Pod 网络通信 Pod-to-Service 网络通信 Internet-to-Service 网络通信 Pod IP在集群内唯一 ","date":"2020-10-30","objectID":"/k8s_series_network/:2:0","tags":["kubernetes"],"title":"Kubernetes系列：网络","uri":"/k8s_series_network/"},{"categories":["Cloud Native"],"content":"2. 基本原理 ","date":"2020-10-30","objectID":"/k8s_series_network/:3:0","tags":["kubernetes"],"title":"Kubernetes系列：网络","uri":"/k8s_series_network/"},{"categories":["Cloud Native"],"content":"2.1 Container-to-Container 在kubernetes中，一个Pod是一组Container的组合，并且，Pod内的Container是共享network namespace的，因此该Pod内的Container的ip和mac地址都是相同的，所以它们只需要通过localhost就能跟Pod内其他Container通信了，不过，因为是共享network namespace，因此该Pod内的端口必须是唯一的，不能冲突。 ","date":"2020-10-30","objectID":"/k8s_series_network/:3:1","tags":["kubernetes"],"title":"Kubernetes系列：网络","uri":"/k8s_series_network/"},{"categories":["Cloud Native"],"content":"2.2 Pod-to-Pod 在容器中，容器可以通过network namespace的技术将不同的容器的网络隔离起来，并通过veth pair技术将两个namespace连接起来以进行通信。如下图所示，每个pod都有自己的netns，通过veth pair跟节点的root netns连接起来，这样一来，pod就能直接跟node进行通信了 但是呢，我们又希望pod1跟pod2进行通信，因此，又需要添加额外的技术允许数据包通过root netns传入不同的pod的netns中，比如bridge技术，当然，也有些可以用路由表来实现。 2.2.1 同Node 通过network namespace、veth pair和bridge技术，我们就能实现同node内的pod之间的通信了，下图是一个数据包在不同pod之间的流程图： pod1将数据包通过自己的eth0设备发送到root namespace的veth0上 veth0将数据包转发给cbr0 网桥设备 网桥设备解析正确的网段，使用ARP协议将数据包发送到veth1 veth1将数据包转发给pod2的namespace中的eth0设备上，返回也是一样的操作 2.2.2 跨Node 通常，集群中的每个Node都分配有一个CIDR块，该CIRD块指定了该节点上运行的Pod可用的IP地址。一旦发往CIDR块的流量到达Node，则Node有责任将流量转发到正确的Pod上。在跨Node的Pod上，这里就需要CNI插件具有知道每个Pod的ip在哪个node上的功能，并且有能力将数据包路由到正确的Node上。 Pod1将数据包发往eth0，通过veth0到达了root namespace Node(VM1)通过路由表信息将数据包发往网络中 CNI插件知道目标ip在哪个Node上，将数据包发送给对应的Node(VM2) VM2接收到数据包后，发现目标ip在pod4上，通过veth1将数据包发送到pod4的eth0上 在后续的不同CNI插件中，我们会了解一下各个插件针对跨Node的网络通信的实现机制。 ","date":"2020-10-30","objectID":"/k8s_series_network/:3:2","tags":["kubernetes"],"title":"Kubernetes系列：网络","uri":"/k8s_series_network/"},{"categories":["Cloud Native"],"content":"2.3 Pod-to-Service 和 Internet-to-Service 这个网络通信将在Service章节中细说。 ","date":"2020-10-30","objectID":"/k8s_series_network/:3:3","tags":["kubernetes"],"title":"Kubernetes系列：网络","uri":"/k8s_series_network/"},{"categories":["Cloud Native"],"content":"3. CNI ","date":"2020-10-30","objectID":"/k8s_series_network/:4:0","tags":["kubernetes"],"title":"Kubernetes系列：网络","uri":"/k8s_series_network/"},{"categories":["Cloud Native"],"content":"3.1 接口定义 CNI 的接口中包括以下几个方法： type CNI interface {AddNetworkList (net *NetworkConfigList, rt *RuntimeConf) (types.Result, error) DelNetworkList (net *NetworkConfigList, rt *RuntimeConf) error AddNetwork (net *NetworkConfig, rt *RuntimeConf) (types.Result, error) DelNetwork (net *NetworkConfig, rt *RuntimeConf) error } 该接口只有四个方法，添加网络、删除网络、添加网络列表、删除网络列表。 ","date":"2020-10-30","objectID":"/k8s_series_network/:4:1","tags":["kubernetes"],"title":"Kubernetes系列：网络","uri":"/k8s_series_network/"},{"categories":["Cloud Native"],"content":"3.2 设计考量 CNI 设计的时候考虑了以下问题： 容器运行时必须在调用任何插件之前为容器创建一个新的网络命名空间。 然后，运行时必须确定这个容器应属于哪个网络，并为每个网络确定哪些插件必须被执行。 网络配置采用 JSON 格式，可以很容易地存储在文件中。网络配置包括必填字段，如 name 和 type 以及插件（类型）。网络配置允许字段在调用之间改变值。为此，有一个可选的字段 args，必须包含不同的信息。 容器运行时必须按顺序为每个网络执行相应的插件，将容器添加到每个网络中。 在完成容器生命周期后，运行时必须以相反的顺序执行插件（相对于执行添加容器的顺序）以将容器与网络断开连接。 容器运行时不能为同一容器调用并行操作，但可以为不同的容器调用并行操作。 容器运行时必须为容器订阅 ADD 和 DEL 操作，这样 ADD 后面总是跟着相应的 DEL。 DEL 可能跟着额外的 DEL，但是，插件应该允许处理多个 DEL（即插件 DEL 应该是幂等的）。 容器必须由 ContainerID 唯一标识。存储状态的插件应该使用（网络名称，容器 ID）的主键来完成。 运行时不能调用同一个网络名称或容器 ID 执行两次 ADD（没有相应的 DEL）。换句话说，给定的容器 ID 必须只能添加到特定的网络一次。 ","date":"2020-10-30","objectID":"/k8s_series_network/:4:2","tags":["kubernetes"],"title":"Kubernetes系列：网络","uri":"/k8s_series_network/"},{"categories":["Cloud Native"],"content":"3.3 CNI 插件 CNI 插件必须实现一个可执行文件，这个文件可以被容器管理系统（例如 rkt 或 Kubernetes）调用。 CNI 插件负责将网络接口插入容器网络命名空间（例如，veth 对的一端），并在主机上进行任何必要的改变（例如将 veth 的另一端连接到网桥）。然后将 IP 分配给接口，并通过调用适当的 IPAM 插件来设置与 “IP 地址管理” 部分一致的路由。 3.3.1 参数 CNI 插件必须支持以下操作： 将容器添加到网络 参数： 版本调用者正在使用的 CNI 规范（容器管理系统或调用插件）的版本。 容器 ID由运行时分配的容器的唯一明文标识符。一定不能是空的。 网络命名空间路径要添加的网络名称空间的路径，即 /proc/[pid]/ns/net 或绑定挂载 / 链接。 网络配置描述容器可以加入的网络的 JSON 文档。架构如下所述。 额外的参数这提供了一个替代机制，允许在每个容器上简单配置 CNI 插件。 容器内接口的名称这是应该分配给容器（网络命名空间）内创建的接口的名称；因此它必须符合 Linux 接口名称上的标准限制。 结果： 接口列表根据插件的不同，这可以包括沙箱（例如容器或管理程序）接口名称和 / 或主机接口名称，每个接口的硬件地址以及接口所在的沙箱（如果有的话）的详细信息。 分配给每个接口的 IP 配置分配给沙箱和 / 或主机接口的 IPv4 和 / 或 IPv6 地址，网关和路由。 DNS 信息包含 nameserver、domain、search domain 和 option 的 DNS 信息的字典。 从网络中删除容器 参数： 版本调用者正在使用的 CNI 规范（容器管理系统或调用插件）的版本。 容器 ID，如上所述。 网络命名空间路径，如上定义。 网络配置，如上所述。 额外的参数，如上所述。 上面定义的容器内的接口的名称。 所有参数应与传递给相应的添加操作的参数相同。 删除操作应释放配置的网络中提供的 containerid 拥有的所有资源。 报告版本 参数：无。 结果：插件支持的 CNI 规范版本信息。 {“cniVersion”：“0.3.1”，// 此输出使用的 CNI 规范的版本 “supportedVersions”：[“0.1.0”，“0.2.0”，“0.3.0”，“0.3.1”] // 此插件支持的 CNI 规范版本列表 } CNI 插件的详细说明请参考：CNI SPEC。 3.3.2 IP 分配 作为容器网络管理的一部分，CNI 插件需要为接口分配（并维护）IP 地址，并安装与该接口相关的所有必要路由。这给了 CNI 插件很大的灵活性，但也给它带来了很大的负担。众多的 CNI 插件需要编写相同的代码来支持用户需要的多种 IP 管理方案（例如 dhcp、host-local）。 为了减轻负担，使 IP 管理策略与 CNI 插件类型解耦，我们定义了 IP 地址管理插件（IPAM 插件）。CNI 插件的职责是在执行时恰当地调用 IPAM 插件。 IPAM 插件必须确定接口 IP/subnet，网关和路由，并将此信息返回到 “主” 插件来应用配置。 IPAM 插件可以通过协议（例如 dhcp）、存储在本地文件系统上的数据、网络配置文件的 “ipam” 部分或上述的组合来获得信息。 ","date":"2020-10-30","objectID":"/k8s_series_network/:4:3","tags":["kubernetes"],"title":"Kubernetes系列：网络","uri":"/k8s_series_network/"},{"categories":["Cloud Native"],"content":"4. 主流方案 flannel calico cilium 云厂商(VPC) 因为我自己接触的是通过基于AWS的EKS实现的kubernetes集群，因此先说下aws的CNI插件方案 ","date":"2020-10-30","objectID":"/k8s_series_network/:5:0","tags":["kubernetes"],"title":"Kubernetes系列：网络","uri":"/k8s_series_network/"},{"categories":["Cloud Native"],"content":"4.1 amazon-vpc-cni-k8s 4.1.1 K8S运行在AWS VPC上的目标 Pod联网必须支持与用户从EC2联网中获得的特性相当的高吞吐量和可用性，低延迟和最小抖动 可以使用跟EC2一样的网络安全组 网络操作必须简单安全。用户必须能够应用现有的AWS VPC网络和安全最佳实践，以通过AWS VPC构建Kubernetes集群 只需几秒钟即可设置Pod网络 管理员应能够将群集扩展到2000个节点 4.1.2 方案 为每个Node(ec2)创建多个弹性网络接口(ENIs)，并分配secondary IP 对于每个Pod，选择一个可用的secondary IP，将其分配给Pod，并实现以下功能： 在单个主机上进行Pod到Pod的通信 在不同主机上进行Pod到Pod的通信 允许在Pod和AWS服务进行通信 允许Pod和本地数据中心进行通信 允许Pod和Internet进行通信 在EC2-VPC里，每个实例可以创建多个ENI，每个ENI可以分配多个IP地址。 任何发往这些IP地址之一的数据包，EC2-VPC都会将该数据包传递到实例。 ENI是虚拟网络接口，您可以将其附加到VPC中的实例。 将ENI附加到实例后，将创建一个对应的接口。 主ENI IP地址会自动分配给该接口。 所有辅助地址均未分配，并且由主机所有者决定如何配置它们。 4.1.3 架构 Pod to Pod Inside a Pod IP address # ip addr show 1: lo: \u003cLOOPBACK,UP,LOWER_UP\u003e mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 inet 127.0.0.1/8 scope host lo valid_lft forever preferred_lft forever 3: eth0@if173: \u003cBROADCAST,MULTICAST,UP,LOWER_UP\u003e mtu 9001 qdisc noqueue state UP group default link/ether 6a:f3:a1:ff:38:a8 brd ff:ff:ff:ff:ff:ff link-netnsid 0 inet 172.31.176.184/32 scope global eth0 valid_lft forever preferred_lft forever route # ip route show default via 169.254.1.1 dev eth0 169.254.1.1 dev eth0 scope link static arp # arp -a 172-31-177-243.node-exporter.monitoring.svc.cluster.local (172.31.177.243) at 8e:6b:e1:80:7c:de [ether] on eth0 _gateway (169.254.1.1) at 8e:6b:e1:80:7c:de [ether] PERM on eth0 On Host side ip address # ip addr show 1: lo: \u003cLOOPBACK,UP,LOWER_UP\u003e mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 inet 127.0.0.1/8 scope host lo valid_lft forever preferred_lft forever inet6 ::1/128 scope host valid_lft forever preferred_lft forever 2: eth0: \u003cBROADCAST,MULTICAST,UP,LOWER_UP\u003e mtu 9001 qdisc mq state UP group default qlen 1000 link/ether 02:b1:bf:9a:b2:cb brd ff:ff:ff:ff:ff:ff inet 172.31.177.243/23 brd 172.31.177.255 scope global dynamic eth0 valid_lft 2539sec preferred_lft 2539sec inet6 fe80::b1:bfff:fe9a:b2cb/64 scope link valid_lft forever preferred_lft forever 8: eth1: \u003cBROADCAST,MULTICAST,UP,LOWER_UP\u003e mtu 9001 qdisc mq state UP group default qlen 1000 link/ether 02💿2d:55:75:29 brd ff:ff:ff:ff:ff:ff inet 172.31.177.128/23 brd 172.31.177.255 scope global eth1 valid_lft forever preferred_lft forever inet6 fe80:💿2dff:fe55:7529/64 scope link valid_lft forever preferred_lft forever 173: enic614534eb15@if3: \u003cBROADCAST,MULTICAST,UP,LOWER_UP\u003e mtu 9001 qdisc noqueue state UP group default link/ether 8e:6b:e1:80:7c:de brd ff:ff:ff:ff:ff:ff link-netnsid 3 inet6 fe80::8c6b:e1ff:fe80:7cde/64 scope link valid_lft forever preferred_lft forever 通过路由表控制Pod的出入流量 main route控制进入pod的流量 # ip route show default via 172.31.176.1 dev eth0 169.254.169.254 dev eth0 172.31.176.0/23 dev eth0 proto kernel scope link src 172.31.177.243 172.31.176.184 dev enic614534eb15 scope link # \u003c----- Pod's IP 每个ENI都有自己的路由表，该路由表用于路由Pod的传出流量。 # ip route show table 2 default via 172.31.176.1 dev eth1 172.31.176.1 dev eth1 scope link 需要给pod ip配置策略路由，否则流量无法走到ENI的路由表 # ip rule list 0: from all lookup local 512: from all to 172.31.176.184 lookup main --\u003e 到Pod的流量走默认路由表 1024: from all fwmark 0x80/0x80 lookup main 1536: from 172.31.176.184 lookup 2 --\u003e 从Pod出来的流量走ENI自己的路由表 32766: from all lookup main 32767: from all lookup default 4.1.4 CNI插件执行的操作 创建veth pair，一个放到主机的namespace，一个放到Pod’s namespace ip link add veth-1 type veth peer name veth-1c /* on host namespace */ ip link set veth-1c netns ns1 /* move veth-1c to Pod's namespace ns1 */ ip link set veth-1 up /* bring up veth-1 */ ip netns exec ns1 ip link set veth-1c up /* bring up veth-1c */ 获取分配给实例的secondary IP地址，并在Pod的namespace中执行以下操作： 分配IP给Pod的eth0 添加默认网关和默认路由到Pod的路由表 给默认网关添加静态ARP条目 /* To assign IP address 172.31.176.184 to Pod's namespace ns1 */ ip netns exec ns1 ip addr add 172.31.176.184/32 dev veth-1c /* assign a IP address to veth-1c */ ip netns exec ns1 ip route add 169.254.1.1 dev veth-1c /* add default gateway */ ip netns exec ns1 ip route add default via 169.254.1.1 dev veth-","date":"2020-10-30","objectID":"/k8s_series_network/:5:1","tags":["kubernetes"],"title":"Kubernetes系列：网络","uri":"/k8s_series_network/"},{"categories":["Cloud Native"],"content":"5. 结论 该篇我们了解了k8s网络的实现方式，以及CNI插件具备的功能，并了解了一些主流的CNI网络方案。 参考 https://github.com/containernetworking/cni https://sookocheff.com/post/kubernetes/understanding-kubernetes-networking-model/ https://github.com/aws/amazon-vpc-cni-k8s/blob/master/docs/cni-proposal.md ","date":"2020-10-30","objectID":"/k8s_series_network/:6:0","tags":["kubernetes"],"title":"Kubernetes系列：网络","uri":"/k8s_series_network/"},{"categories":["Cloud Native"],"content":"容器运行时","date":"2020-10-25","objectID":"/k8s_series_container/","tags":["kubernetes"],"title":"Kubernetes系列：容器","uri":"/k8s_series_container/"},{"categories":["Cloud Native"],"content":"系列目录 《Kubernetes系列：开篇》 《Kubernetes系列：概述》 《Kubernetes系列：架构》 《Kubernetes系列：容器》 《Kubernetes系列：网络》 《Kubernetes系列：存储》 《Kubernetes系列：Service》 《Kubernetes系列：Ingress》 《Kubernetes系列：OAM》 ","date":"2020-10-25","objectID":"/k8s_series_container/:1:0","tags":["kubernetes"],"title":"Kubernetes系列：容器","uri":"/k8s_series_container/"},{"categories":["Cloud Native"],"content":"1. 介绍 ","date":"2020-10-25","objectID":"/k8s_series_container/:2:0","tags":["kubernetes"],"title":"Kubernetes系列：容器","uri":"/k8s_series_container/"},{"categories":["Cloud Native"],"content":"1.1 容器技术 LXC（Linux容器）是一种操作系统级虚拟化技术，用于使用单个Linux内核在控制主机上运行多个隔离的Linux系统（容器）。 每个运行的容器都是可重复的； 包含依赖环境在内的标准，意味着无论您在哪里运行它，您都会得到相同的行为。容器将应用程序从底层的主机设施中解耦。 这使得在不同的云或 OS 环境中部署更加容易。 ","date":"2020-10-25","objectID":"/k8s_series_container/:2:1","tags":["kubernetes"],"title":"Kubernetes系列：容器","uri":"/k8s_series_container/"},{"categories":["Cloud Native"],"content":"1.2 容器镜像 容器镜像是一个随时可以运行的软件包， 包含运行应用程序所需的一切：代码和它需要的所有运行时、应用程序和系统库，以及一些基本设置的默认值。 根据设计，容器是不可变的：你不能更改已经运行的容器的代码。 如果有一个容器化的应用程序需要修改，则需要构建包含更改的新镜像，然后再基于新构建的镜像重新运行容器。 ","date":"2020-10-25","objectID":"/k8s_series_container/:2:2","tags":["kubernetes"],"title":"Kubernetes系列：容器","uri":"/k8s_series_container/"},{"categories":["Cloud Native"],"content":"1.3 容器运行时 容器运行环境是负责运行容器的软件。 Kubernetes 支持多个容器运行环境: Docker、 containerd、CRI-O 以及任何实现 Kubernetes CRI (容器运行环境接口)。 ","date":"2020-10-25","objectID":"/k8s_series_container/:2:3","tags":["kubernetes"],"title":"Kubernetes系列：容器","uri":"/k8s_series_container/"},{"categories":["Cloud Native"],"content":"1.4 CRI(容器运行时接口) CRI，一个插件接口，使kubelet可以使用各种容器运行时，而无需重新编译。 CRI由规范/要求(待添加)、protobuf API和用于容器运行时的库组成，以与节点上的kubelet集成。 ","date":"2020-10-25","objectID":"/k8s_series_container/:2:4","tags":["kubernetes"],"title":"Kubernetes系列：容器","uri":"/k8s_series_container/"},{"categories":["Cloud Native"],"content":"2. 为什么使用容器？ 容器提供了一种逻辑打包机制，以这种机制打包的应用可以脱离其实际运行的环境。利用这种脱离，不管目标环境是私有数据中心、公有云，还是开发者的个人笔记本电脑，您都可以轻松、一致地部署基于容器的应用。容器化使开发者和 IT 运营团队的关注点泾渭分明 - 开发者专注于应用逻辑和依赖项，而 IT 运营团队可以专注于部署和管理，不必为具体的软件版本和应用特有的配置等应用细节分心。 我们通过与虚拟机的对比来看下容器的优势 与虚拟机的硬件栈虚拟化不同，容器在操作系统级别进行虚拟化，且可以直接在操作系统内核上运行多个容器。也就是说，容器更轻巧：它们共享操作系统内核，启动速度更快，且与启动整个操作系统相比其占用的内存微乎其微。 一致的环境 容器让开发者可以创建与其他应用相隔离的可预测环境。容器还可以包含应用所需的软件依赖项，比如具体的编程语言运行时版本和其他软件库。从开发者的角度看，无论应用最终部署在什么地方，都可以保证这些条件一致。这一切将转化为生产力的提升：开发者和 IT 运营团队可以减少调试和诊断环境差异所需的时间，将更多的时间用于为用户提供新的功能。而且这也意味着 bug 更少，因为开发者现可在开发和测试环境中做出在生产环境中也适用的假设。 在任何地方运行 容器几乎能在任何地方运行，极大减轻了开发和部署工作量：在 Linux、Windows 和 Mac 操作系统中；在虚拟机或裸机上；在开发者的机器或本地数据中心的机器上；当然还有在公有云上。而 Docker 容器映像格式广受欢迎，则进一步增强了可移植性。无论您希望在什么地方运行软件，都可以使用容器。 隔离 容器会在操作系统级别虚拟化 CPU、内存、存储和网络资源，为开发者提供在逻辑上与其他应用相隔离的沙盒化操作系统接口。 容器的优势 虚拟机的优势 一致的运行时环境 true true 应用沙盒化 true true 占用的存储空间少 true 开销低 true ","date":"2020-10-25","objectID":"/k8s_series_container/:3:0","tags":["kubernetes"],"title":"Kubernetes系列：容器","uri":"/k8s_series_container/"},{"categories":["Cloud Native"],"content":"3. 实现机制 ","date":"2020-10-25","objectID":"/k8s_series_container/:4:0","tags":["kubernetes"],"title":"Kubernetes系列：容器","uri":"/k8s_series_container/"},{"categories":["Cloud Native"],"content":"3.1 资源隔离(namespace) Namespace是对全局系统资源的一种封装隔离，使得处于不同namespace的进程拥有独立的全局系统资源，改变一个namespace中的系统资源只会影响当前namespace里的进程，对其他namespace中的进程没有影响。 目前，Linux内核里面实现了7种不同类型的namespace。 名称 宏定义 隔离内容 Cgroup CLONE_NEWCGROUP Cgroup root directory (since Linux 4.6) IPC CLONE_NEWIPC System V IPC, POSIX message queues (since Linux 2.6.19) Network CLONE_NEWNET Network devices, stacks, ports, etc. (since Linux 2.6.24) Mount CLONE_NEWNS Mount points (since Linux 2.4.19) PID CLONE_NEWPID Process IDs (since Linux 2.6.24) User CLONE_NEWUSER User and group IDs (started in Linux 2.6.23 and completed in Linux 3.8) UTS CLONE_NEWUTS Hostname and NIS domain name (since Linux 2.6.19) 3.1.1 查看namespace 系统中的每个进程都有/proc/[pid]/ns/这样一个目录，里面包含了这个进程所属namespace的信息，里面每个文件的描述符都可以用来作为setns函数(后面会介绍)的参数。 #查看当前bash进程所属的namespace dev@ubuntu:~$ ls -l /proc/$$/ns total 0 lrwxrwxrwx 1 dev dev 0 7月 7 17:24 cgroup -\u003e cgroup:[4026531835] #(since Linux 4.6) lrwxrwxrwx 1 dev dev 0 7月 7 17:24 ipc -\u003e ipc:[4026531839] #(since Linux 3.0) lrwxrwxrwx 1 dev dev 0 7月 7 17:24 mnt -\u003e mnt:[4026531840] #(since Linux 3.8) lrwxrwxrwx 1 dev dev 0 7月 7 17:24 net -\u003e net:[4026531957] #(since Linux 3.0) lrwxrwxrwx 1 dev dev 0 7月 7 17:24 pid -\u003e pid:[4026531836] #(since Linux 3.8) lrwxrwxrwx 1 dev dev 0 7月 7 17:24 user -\u003e user:[4026531837] #(since Linux 3.8) lrwxrwxrwx 1 dev dev 0 7月 7 17:24 uts -\u003e uts:[4026531838] #(since Linux 3.0) 上面每种类型的namespace都是在不同的Linux版本被加入到/proc/[pid]/ns/目录里去的，比如pid namespace是在Linux 3.8才被加入到/proc/[pid]/ns/里面，但这并不是说到3.8才支持pid namespace，其实pid namespace在2.6.24的时候就已经加入到内核了，在那个时候就可以用pid namespace了，只是有了/proc/[pid]/ns/pid之后，使得操作pid namespace更方便了 虽然说cgroup是在Linux 4.6版本才被加入内核，可是在Ubuntu 16.04上，尽管内核版本才4.4，但也支持cgroup namespace，估计应该是Ubuntu将4.6的cgroup namespace这部分代码patch到了他们的4.4内核上。 以ipc:[4026531839]为例，ipc是namespace的类型，4026531839是inode number，如果两个进程的ipc namespace的inode number一样，说明他们属于同一个namespace。这条规则对其他类型的namespace也同样适用。 从上面的输出可以看出，对于每种类型的namespace，进程都会与一个namespace ID关联。 3.1.2 namespace API 和namespace相关的函数只有三个 clone： 创建一个新的进程并把他放到新的namespace中 int clone(int (*child_func)(void *), void *child_stack , int flags, void *arg); /*** flags： 指定一个或者多个上面的CLONE_NEW*（当然也可以包含跟namespace无关的flags）， 这样就会创建一个或多个新的不同类型的namespace， 并把新创建的子进程加入新创建的这些namespace中。 ***/ setns： 将当前进程加入到已有的namespace中 int setns(int fd, int nstype); /*** fd： 指向/proc/[pid]/ns/目录里相应namespace对应的文件， 表示要加入哪个namespace nstype： 指定namespace的类型（上面的任意一个CLONE_NEW*）： 1. 如果当前进程不能根据fd得到它的类型，如fd由其他进程创建， 并通过UNIX domain socket传给当前进程， 那么就需要通过nstype来指定fd指向的namespace的类型 2. 如果进程能根据fd得到namespace类型，比如这个fd是由当前进程打开的， 那么nstype设置为0即可 ***/ unshare: 使当前进程退出指定类型的namespace，并加入到新创建的namespace（相当于创建并加入新的namespace） int unshare(int flags); /*** flags： 指定一个或者多个上面的CLONE_NEW*， 这样当前进程就退出了当前指定类型的namespace并加入到新创建的namespace ***/ clone和unshare的区别 clone和unshare的功能都是创建并加入新的namespace， 他们的区别是： unshare是使当前进程加入新的namespace clone是创建一个新的子进程，然后让子进程加入新的namespace，而当前进程保持不变 3.1.3 其它 当一个namespace中的所有进程都退出时，该namespace将会被销毁。当然还有其他方法让namespace一直存在，假设我们有一个进程号为1000的进程，以ipc namespace为例： 通过mount –bind命令。例如mount –bind /proc/1000/ns/ipc /other/file，就算属于这个ipc namespace的所有进程都退出了，只要/other/file还在，这个ipc namespace就一直存在，其他进程就可以利用/other/file，通过setns函数加入到这个namespace 在其他namespace的进程中打开/proc/1000/ns/ipc文件，并一直持有这个文件描述符不关闭，以后就可以用setns函数加入这个namespace。 ","date":"2020-10-25","objectID":"/k8s_series_container/:4:1","tags":["kubernetes"],"title":"Kubernetes系列：容器","uri":"/k8s_series_container/"},{"categories":["Cloud Native"],"content":"3.2 资源限制(cgroup) cgroup和namespace类似，也是将进程进行分组，但它的目的和namespace不一样，namespace是为了隔离进程组之间的资源，而cgroup是为了对一组进程进行统一的资源监控和限制。 术语cgroup在不同的上下文中代表不同的意思，可以指整个Linux的cgroup技术，也可以指一个具体进程组。 cgroup是Linux下的一种将进程按组进行管理的机制，在用户层看来，cgroup技术就是把系统中的所有进程组织成一颗一颗独立的树，每棵树都包含系统的所有进程，树的每个节点是一个进程组，而每颗树又和一个或者多个subsystem关联，树的作用是将进程分组，而subsystem的作用就是对这些组进行操作。cgroup主要包括下面两部分： subsystem 一个subsystem就是一个内核模块，他被关联到一颗cgroup树之后，就会在树的每个节点（进程组）上做具体的操作。subsystem经常被称作\"resource controller\"，因为它主要被用来调度或者限制每个进程组的资源，但是这个说法不完全准确，因为有时我们将进程分组只是为了做一些监控，观察一下他们的状态，比如perf_event subsystem。到目前为止，Linux支持12种subsystem，比如限制CPU的使用时间，限制使用的内存，统计CPU的使用情况，冻结和恢复一组进程等，后续会对它们一一进行介绍。 hierarchy 一个hierarchy可以理解为一棵cgroup树，树的每个节点就是一个进程组，每棵树都会与零到多个subsystem关联。在一颗树里面，会包含Linux系统中的所有进程，但每个进程只能属于一个节点（进程组）。系统中可以有很多颗cgroup树，每棵树都和不同的subsystem关联，一个进程可以属于多颗树，即一个进程可以属于多个进程组，只是这些进程组和不同的subsystem关联。目前Linux支持12种subsystem，如果不考虑不与任何subsystem关联的情况（systemd就属于这种情况），Linux里面最多可以建12颗cgroup树，每棵树关联一个subsystem，当然也可以只建一棵树，然后让这棵树关联所有的subsystem。当一颗cgroup树不和任何subsystem关联的时候，意味着这棵树只是将进程进行分组，至于要在分组的基础上做些什么，将由应用程序自己决定，systemd就是一个这样的例子。 3.2.1 查看cgroup 可以通过查看/proc/cgroups(since Linux 2.6.24)知道当前系统支持哪些subsystem，下面是一个例子 #subsys_name hierarchy num_cgroups enabled cpuset 11 1 1 cpu 3 64 1 cpuacct 3 64 1 blkio 8 64 1 memory 9 104 1 devices 5 64 1 freezer 10 4 1 net_cls 6 1 1 perf_event 7 1 1 net_prio 6 1 1 hugetlb 4 1 1 pids 2 68 1 从左到右，字段的含义分别是： subsystem的名字 subsystem所关联到的cgroup树的ID，如果多个subsystem关联到同一颗cgroup树，那么他们的这个字段将一样，比如这里的cpu和cpuacct就一样，表示他们绑定到了同一颗树。如果出现下面的情况，这个字段将为0： 当前subsystem没有和任何cgroup树绑定 当前subsystem已经和cgroup v2的树绑定 当前subsystem没有被内核开启 subsystem所关联的cgroup树中进程组的个数，也即树上节点的个数 1表示开启，0表示没有被开启(可以通过设置内核的启动参数“cgroup_disable”来控制subsystem的开启). 3.2.2 使用cgroup cgroup相关的所有操作都是基于内核中的cgroup virtual filesystem，使用cgroup很简单，挂载这个文件系统就可以了。一般情况下都是挂载到/sys/fs/cgroup目录下，当然挂载到其它任何目录都没关系。 这里假设目录/sys/fs/cgroup已经存在，下面用到的xxx为任意字符串，取一个有意义的名字就可以了，当用mount命令查看的时候，xxx会显示在第一列 挂载一颗和所有subsystem关联的cgroup树到/sys/fs/cgroup mount -t cgroup xxx /sys/fs/cgroup 挂载一颗和cpuset subsystem关联的cgroup树到/sys/fs/cgroup/cpuset mkdir /sys/fs/cgroup/cpuset mount -t cgroup -o cpuset xxx /sys/fs/cgroup/cpuset 挂载一颗与cpu和cpuacct subsystem关联的cgroup树到/sys/fs/cgroup/cpu,cpuacct mkdir /sys/fs/cgroup/cpu,cpuacct mount -t cgroup -o cpu,cpuacct xxx /sys/fs/cgroup/cpu,cpuacct 挂载一棵cgroup树，但不关联任何subsystem，下面就是systemd所用到的方式 mkdir /sys/fs/cgroup/systemd mount -t cgroup -o none,name=systemd xxx /sys/fs/cgroup/systemd 3.2.3 查看进程归属cgroup 可以通过查看/proc/[pid]/cgroup(since Linux 2.6.24)知道指定进程属于哪些cgroup。 dev@ubuntu:~$ cat /proc/777/cgroup 11:cpuset:/ 10:freezer:/ 9:memory:/system.slice/cron.service 8:blkio:/system.slice/cron.service 7:perf_event:/ 6:net_cls,net_prio:/ 5:devices:/system.slice/cron.service 4:hugetlb:/ 3:cpu,cpuacct:/system.slice/cron.service 2:pids:/system.slice/cron.service 1:name=systemd:/system.slice/cron.service 每一行包含用冒号隔开的三列，他们的意思分别是 cgroup树的ID， 和/proc/cgroups文件中的ID一一对应。 和cgroup树绑定的所有subsystem，多个subsystem之间用逗号隔开。这里name=systemd表示没有和任何subsystem绑定，只是给他起了个名字叫systemd。 进程在cgroup树中的路径，即进程所属的cgroup，这个路径是相对于挂载点的相对路径。 3.2.4 所有subsystems 目前Linux支持下面12种subsystem cpu (since Linux 2.6.24; CONFIG_CGROUP_SCHED) 用来限制cgroup的CPU使用率。 cpuacct (since Linux 2.6.24; CONFIG_CGROUP_CPUACCT) 统计cgroup的CPU的使用率。 cpuset (since Linux 2.6.24; CONFIG_CPUSETS) 绑定cgroup到指定CPUs和NUMA节点。 memory (since Linux 2.6.25; CONFIG_MEMCG) 统计和限制cgroup的内存的使用率，包括process memory, kernel memory, 和swap。 devices (since Linux 2.6.26; CONFIG_CGROUP_DEVICE) 限制cgroup创建(mknod)和访问设备的权限。 freezer (since Linux 2.6.28; CONFIG_CGROUP_FREEZER) suspend和restore一个cgroup中的所有进程。 net_cls (since Linux 2.6.29; CONFIG_CGROUP_NET_CLASSID) 将一个cgroup中进程创建的所有网络包加上一个classid标记，用于tc和iptables。 只对发出去的网络包生效，对收到的网络包不起作用。 blkio (since Linux 2.6.33; CONFIG_BLK_CGROUP) 限制cgroup访问块设备的IO速度。 perf_event (since Linux 2.6.39; CONFIG_CGROUP_PERF) 对cgroup进行性能监控 net_prio (since Linux 3.3; CONFIG_CGROUP_NET_PRIO) 针对每个网络接口设置cgroup的访问优先级。 hugetlb (since Linux 3.5; CONFIG_CGROUP_HUGETLB) 限制cgroup的huge pages的使用量。 pids (since Linux 4.3; CONFIG_CGROUP_PIDS) 限制一个cgroup及其子孙cgroup中的总进程数。 上面这些subsystem，有","date":"2020-10-25","objectID":"/k8s_series_container/:4:2","tags":["kubernetes"],"title":"Kubernetes系列：容器","uri":"/k8s_series_container/"},{"categories":["Cloud Native"],"content":"3.3 镜像技术 3.3.1 (re)mount 我们知道，namespace可以对系统资源进行隔离，而mount namespace是隔离各个挂载点的，因此在容器内部对文件系统进行mount和umount对其他进程并不影响。但是，新创建的容器是会直接继承宿主机的各个挂载点，因此我们会看到之前宿主机的各个挂载目录。但是对于用户来讲，当我们启动一个新的容器的时候，应该是一个干净的文件系统，不应该把宿主机的文件系统挂载进来，因此，我们在操作mount时应该做一个重新挂载整个根目录的操作。 3.3.2 rootfs 在linux中，可以由chroot或者pivot_root来实现根目录的\"挂载\"。 chroot顾名思义，它的作用就是帮你“change root file system”，即改变进程的根目录到你指定的位置。不过chroot是只改变即将运行的某进程的根目录。 pivot_root主要是把整个系统切换到一个新的root目录，然后去掉对之前rootfs的依赖，以便于可以umount 之前的文件系统（pivot_root需要root权限） 为了能够让容器的这个根目录看起来更“真实”，我们一般会在这个容器的根目录下挂载一个完整操作系统的文件系统，比如 Ubuntu16.04 的 ISO。这样，在容器启动之后，我们在容器里通过执行 “ls /” 查看根目录下的内容，就是 Ubuntu 16.04 的所有目录和文件。 而这个挂载在容器根目录上、用来为容器进程提供隔离后执行环境的文件系统，就是所谓的“容器镜像”。它还有一个更为专业的名字，叫作：rootfs（根文件系统）。 ","date":"2020-10-25","objectID":"/k8s_series_container/:4:3","tags":["kubernetes"],"title":"Kubernetes系列：容器","uri":"/k8s_series_container/"},{"categories":["Cloud Native"],"content":"3.3.3 Union File System 所谓UnionFS就是把不同物理位置的目录合并mount到同一个目录中。 在变更rootfs内容时，我们不可能每次都重新重头到位制作一个完整的rootfs，这样太麻烦。那我们可不可以只对rootfs做增量操作呢？ 而这正是UnionFS的功能。docker正是对这个技术引用才脱颖而出的。 Docker 在镜像的设计中，引入了层（layer）的概念。也就是说，用户制作镜像的每一步操作，都会生成一个层，也就是一个增量 rootfs。 每个image layer和container layer在Docker主机上均表示为/var/lib/docker/中的子目录 ","date":"2020-10-25","objectID":"/k8s_series_container/:4:4","tags":["kubernetes"],"title":"Kubernetes系列：容器","uri":"/k8s_series_container/"},{"categories":["Cloud Native"],"content":"4. CRI(容器运行时接口) ","date":"2020-10-25","objectID":"/k8s_series_container/:5:0","tags":["kubernetes"],"title":"Kubernetes系列：容器","uri":"/k8s_series_container/"},{"categories":["Cloud Native"],"content":"4.1 初衷 每个容器运行时都有自己的优势，许多用户要求Kubernetes支持更多运行时。但是，Kubelet使用声明性的Pod级接口，该接口充当容器运行时的唯一集成点，高级别的声明性接口导致更高的集成和维护成本，并且由于一些原因，还降低了功能速度。因此，CRI就是提供一个通用的接口以实现以下目标： 提高可扩展性：简化容器运行时集成。 提高特征速度。 提高代码可维护性 ","date":"2020-10-25","objectID":"/k8s_series_container/:5:1","tags":["kubernetes"],"title":"Kubernetes系列：容器","uri":"/k8s_series_container/"},{"categories":["Cloud Native"],"content":"4.2 CRI概述 Kubelet使用gRPC框架通过Unix socket与容器运行时(或运行时的CRI shim)进行通信，其中kubelet充当客户端，而CRI shim充当服务器。 protocol buffer API包括两个gRPC服务，即ImageService和RuntimeService。 ImageService提供RPC，用于从repository中提取、检查并删除镜像。 RuntimeService包含RPC，用于管理容器和容器的生命周期以及与容器进行交互的调用(exec/attach/port-forward)。 同时管理镜像和容器(例如Docker和rkt)的整体容器运行时可以通过单个套接字同时提供这两种服务。 ","date":"2020-10-25","objectID":"/k8s_series_container/:5:2","tags":["kubernetes"],"title":"Kubernetes系列：容器","uri":"/k8s_series_container/"},{"categories":["Cloud Native"],"content":"4.3 pod和容器生命周期管理 service RuntimeService { // Sandbox operations. rpc RunPodSandbox(RunPodSandboxRequest) returns (RunPodSandboxResponse) {} rpc StopPodSandbox(StopPodSandboxRequest) returns (StopPodSandboxResponse) {} rpc RemovePodSandbox(RemovePodSandboxRequest) returns (RemovePodSandboxResponse) {} rpc PodSandboxStatus(PodSandboxStatusRequest) returns (PodSandboxStatusResponse) {} rpc ListPodSandbox(ListPodSandboxRequest) returns (ListPodSandboxResponse) {} // Container operations. rpc CreateContainer(CreateContainerRequest) returns (CreateContainerResponse) {} rpc StartContainer(StartContainerRequest) returns (StartContainerResponse) {} rpc StopContainer(StopContainerRequest) returns (StopContainerResponse) {} rpc RemoveContainer(RemoveContainerRequest) returns (RemoveContainerResponse) {} rpc ListContainers(ListContainersRequest) returns (ListContainersResponse) {} rpc ContainerStatus(ContainerStatusRequest) returns (ContainerStatusResponse) {} ... } Pod由具有资源约束的隔离环境中的一组应用程序容器组成。在CRI中，此环境称为PodSandbox。PodSandbox必须遵守Pod资源规范。在v1alpha1 API中，这是通过启动kubelet创建并传递到运行时的pod级cgroup中的所有进程来实现的。 在启动Pod之前，kubelet会调用RuntimeService。RunPodSandbox创建环境。这包括为Pod设置网络连接(例如分配IP)。一旦PodSandbox处于活动状态，就可以独立创建/启动/停止/删除单个容器。要删除pod，kubelet将在停止和删除PodSandbox之前停止并删除容器。 Kubelet负责通过RPC管理容器的生命周期，行使容器生命周期的hook和liveness/readiness检查，同时遵守pod的重新启动策略。 ","date":"2020-10-25","objectID":"/k8s_series_container/:5:3","tags":["kubernetes"],"title":"Kubernetes系列：容器","uri":"/k8s_series_container/"},{"categories":["Cloud Native"],"content":"5. 结论 该篇我们介绍了容器技术的基本实现机制，包含namespace和cgroup进行资源的隔离和限制，通过chroot和unionFS技术实现镜像的功能。之后还介绍了CRI的初衷和实现。 参考 https://github.com/kubernetes/kubernetes/blob/release-1.5/docs/proposals/container-runtime-interface-v1.md https://kubernetes.io/blog/2016/12/container-runtime-interface-cri-in-kubernetes/ https://github.com/kubernetes/community/blob/master/contributors/devel/sig-node/container-runtime-interface.md https://docs.docker.com/storage/storagedriver/aufs-driver/ https://segmentfault.com/a/1190000009732550 https://coolshell.cn/articles/17061.html ","date":"2020-10-25","objectID":"/k8s_series_container/:6:0","tags":["kubernetes"],"title":"Kubernetes系列：容器","uri":"/k8s_series_container/"},{"categories":["Cloud Native"],"content":"架构","date":"2020-10-20","objectID":"/k8s_series_arch/","tags":["kubernetes"],"title":"Kubernetes系列：架构","uri":"/k8s_series_arch/"},{"categories":["Cloud Native"],"content":"系列目录 《Kubernetes系列：开篇》 《Kubernetes系列：概述》 《Kubernetes系列：架构》 《Kubernetes系列：容器》 《Kubernetes系列：网络》 《Kubernetes系列：存储》 《Kubernetes系列：Service》 《Kubernetes系列：Ingress》 《Kubernetes系列：OAM》 一个 Kubernetes 集群由一组被称作节点的机器组成。这些节点上运行 Kubernetes 所管理的容器化应用。集群具有至少一个工作节点。 工作节点托管作为应用负载的组件的 Pod 。控制平面管理集群中的工作节点和 Pod 。 为集群提供故障转移和高可用性，这些控制平面一般跨多主机运行，集群跨多个节点运行。 本文档概述了交付正常运行的 Kubernetes 集群所需的各种组件。 这张图表展示了包含所有相互关联组件的 Kubernetes 集群。 ","date":"2020-10-20","objectID":"/k8s_series_arch/:1:0","tags":["kubernetes"],"title":"Kubernetes系列：架构","uri":"/k8s_series_arch/"},{"categories":["Cloud Native"],"content":"控制平面组件（Control Plane Components） 控制平面的组件对集群做出全局决策(比如调度)，以及检测和响应集群事件（例如，当不满足部署的 replicas 字段时，启动新的 pod）。 控制平面组件可以在集群中的任何节点上运行。 然而，为了简单起见，设置脚本通常会在同一个计算机上启动所有控制平面组件，并且不会在此计算机上运行用户容器。 请参阅构建高可用性集群 中对于多主机 VM 的设置示例。 ","date":"2020-10-20","objectID":"/k8s_series_arch/:2:0","tags":["kubernetes"],"title":"Kubernetes系列：架构","uri":"/k8s_series_arch/"},{"categories":["Cloud Native"],"content":"kube-apiserver API 服务器是 Kubernetes 控制面的组件， 该组件公开了 Kubernetes API。 API 服务器是 Kubernetes 控制面的前端。 Kubernetes API 服务器的主要实现是 kube-apiserver。 kube-apiserver 设计上考虑了水平伸缩，也就是说，它可通过部署多个实例进行伸缩。 你可以运行 kube-apiserver 的多个实例，并在这些实例之间平衡流量。 ","date":"2020-10-20","objectID":"/k8s_series_arch/:2:1","tags":["kubernetes"],"title":"Kubernetes系列：架构","uri":"/k8s_series_arch/"},{"categories":["Cloud Native"],"content":"etcd etcd 是兼具一致性和高可用性的键值数据库，可以作为保存 Kubernetes 所有集群数据的后台数据库。 您的 Kubernetes 集群的 etcd 数据库通常需要有个备份计划。 要了解 etcd 更深层次的信息，请参考 etcd 文档。 ","date":"2020-10-20","objectID":"/k8s_series_arch/:2:2","tags":["kubernetes"],"title":"Kubernetes系列：架构","uri":"/k8s_series_arch/"},{"categories":["Cloud Native"],"content":"kube-scheduler 控制平面组件，负责监视新创建的、未指定运行节点（node）的 Pods，选择节点让 Pod 在上面运行。 调度决策考虑的因素包括单个 Pod 和 Pod 集合的资源需求、硬件/软件/策略约束、亲和性和反亲和性规范、数据位置、工作负载间的干扰和最后时限。 ","date":"2020-10-20","objectID":"/k8s_series_arch/:2:3","tags":["kubernetes"],"title":"Kubernetes系列：架构","uri":"/k8s_series_arch/"},{"categories":["Cloud Native"],"content":"kube-controller-manager 在主节点上运行 控制器 的组件。 从逻辑上讲，每个控制器都是一个单独的进程， 但是为了降低复杂性，它们都被编译到同一个可执行文件，并在一个进程中运行。 这些控制器包括: 节点控制器（Node Controller）: 负责在节点出现故障时进行通知和响应。 副本控制器（Replication Controller）: 负责为系统中的每个副本控制器对象维护正确数量的 Pod。 端点控制器（Endpoints Controller）: 填充端点(Endpoints)对象(即加入 Service 与 Pod)。 服务帐户和令牌控制器（Service Account \u0026 Token Controllers）: 为新的命名空间创建默认帐户和 API 访问令牌. ","date":"2020-10-20","objectID":"/k8s_series_arch/:2:4","tags":["kubernetes"],"title":"Kubernetes系列：架构","uri":"/k8s_series_arch/"},{"categories":["Cloud Native"],"content":"cloud-controller-manager 云控制器管理器是指嵌入特定云的控制逻辑的 控制平面组件。 云控制器管理器允许您链接聚合到云提供商的应用编程接口中， 并分离出相互作用的组件与您的集群交互的组件。 cloud-controller-manager 仅运行特定于云平台的控制回路。 如果你在自己的环境中运行 Kubernetes，或者在本地计算机中运行学习环境， 所部署的环境中不需要云控制器管理器。 与 kube-controller-manager 类似，cloud-controller-manager 将若干逻辑上独立的 控制回路组合到同一个可执行文件中，供你以同一进程的方式运行。 你可以对其执行水平扩容（运行不止一个副本）以提升性能或者增强容错能力。 下面的控制器都包含对云平台驱动的依赖： 节点控制器（Node Controller）: 用于在节点终止响应后检查云提供商以确定节点是否已被删除 路由控制器（Route Controller）: 用于在底层云基础架构中设置路由 服务控制器（Service Controller）: 用于创建、更新和删除云提供商负载均衡器 ","date":"2020-10-20","objectID":"/k8s_series_arch/:2:5","tags":["kubernetes"],"title":"Kubernetes系列：架构","uri":"/k8s_series_arch/"},{"categories":["Cloud Native"],"content":"Node 组件 节点组件在每个节点上运行，维护运行的 Pod 并提供 Kubernetes 运行环境。 ","date":"2020-10-20","objectID":"/k8s_series_arch/:3:0","tags":["kubernetes"],"title":"Kubernetes系列：架构","uri":"/k8s_series_arch/"},{"categories":["Cloud Native"],"content":"kubelet 一个在集群中每个节点（node）上运行的代理。 它保证容器（containers）都 运行在 Pod 中。 kubelet 接收一组通过各类机制提供给它的 PodSpecs，确保这些 PodSpecs 中描述的容器处于运行状态且健康。 kubelet 不会管理不是由 Kubernetes 创建的容器。 ","date":"2020-10-20","objectID":"/k8s_series_arch/:3:1","tags":["kubernetes"],"title":"Kubernetes系列：架构","uri":"/k8s_series_arch/"},{"categories":["Cloud Native"],"content":"kube-proxy kube-proxy 是集群中每个节点上运行的网络代理， 实现 Kubernetes 服务（Service） 概念的一部分。 kube-proxy 维护节点上的网络规则。这些网络规则允许从集群内部或外部的网络会话与 Pod 进行网络通信。 如果操作系统提供了数据包过滤层并可用的话，kube-proxy 会通过它来实现网络规则。否则， kube-proxy 仅转发流量本身。 ","date":"2020-10-20","objectID":"/k8s_series_arch/:3:2","tags":["kubernetes"],"title":"Kubernetes系列：架构","uri":"/k8s_series_arch/"},{"categories":["Cloud Native"],"content":"容器运行时（Container Runtime） 容器运行环境是负责运行容器的软件。 Kubernetes 支持多个容器运行环境: Docker、 containerd、CRI-O 以及任何实现 Kubernetes CRI (容器运行环境接口)。 ","date":"2020-10-20","objectID":"/k8s_series_arch/:3:3","tags":["kubernetes"],"title":"Kubernetes系列：架构","uri":"/k8s_series_arch/"},{"categories":["Cloud Native"],"content":"插件（Addons） 插件使用 Kubernetes 资源（DaemonSet、 Deployment等）实现集群功能。 因为这些插件提供集群级别的功能，插件中命名空间域的资源属于 kube-system 命名空间。 下面描述众多插件中的几种。有关可用插件的完整列表，请参见 插件（Addons）。 ","date":"2020-10-20","objectID":"/k8s_series_arch/:4:0","tags":["kubernetes"],"title":"Kubernetes系列：架构","uri":"/k8s_series_arch/"},{"categories":["Cloud Native"],"content":"DNS 尽管其他插件都并非严格意义上的必需组件，但几乎所有 Kubernetes 集群都应该 有集群 DNS， 因为很多示例都需要 DNS 服务。 集群 DNS 是一个 DNS 服务器，和环境中的其他 DNS 服务器一起工作，它为 Kubernetes 服务提供 DNS 记录。 Kubernetes 启动的容器自动将此 DNS 服务器包含在其 DNS 搜索列表中。 ","date":"2020-10-20","objectID":"/k8s_series_arch/:4:1","tags":["kubernetes"],"title":"Kubernetes系列：架构","uri":"/k8s_series_arch/"},{"categories":["Cloud Native"],"content":"Web 界面（仪表盘） Dashboard 是Kubernetes 集群的通用的、基于 Web 的用户界面。 它使用户可以管理集群中运行的应用程序以及集群本身并进行故障排除。 ","date":"2020-10-20","objectID":"/k8s_series_arch/:4:2","tags":["kubernetes"],"title":"Kubernetes系列：架构","uri":"/k8s_series_arch/"},{"categories":["Cloud Native"],"content":"容器资源监控 容器资源监控 将关于容器的一些常见的时间序列度量值保存到一个集中的数据库中，并提供用于浏览这些数据的界面。 ","date":"2020-10-20","objectID":"/k8s_series_arch/:4:3","tags":["kubernetes"],"title":"Kubernetes系列：架构","uri":"/k8s_series_arch/"},{"categories":["Cloud Native"],"content":"集群层面日志 集群层面日志 机制负责将容器的日志数据 保存到一个集中的日志存储中，该存储能够提供搜索和浏览接口。 ","date":"2020-10-20","objectID":"/k8s_series_arch/:4:4","tags":["kubernetes"],"title":"Kubernetes系列：架构","uri":"/k8s_series_arch/"},{"categories":["Cloud Native"],"content":"概述","date":"2020-10-15","objectID":"/k8s_series_intro/","tags":["kubernetes"],"title":"Kubernetes系列：概述","uri":"/k8s_series_intro/"},{"categories":["Cloud Native"],"content":"系列目录 《Kubernetes系列：开篇》 《Kubernetes系列：概述》 《Kubernetes系列：架构》 《Kubernetes系列：容器》 《Kubernetes系列：网络》 《Kubernetes系列：存储》 《Kubernetes系列：Service》 《Kubernetes系列：Ingress》 《Kubernetes系列：OAM》 ","date":"2020-10-15","objectID":"/k8s_series_intro/:1:0","tags":["kubernetes"],"title":"Kubernetes系列：概述","uri":"/k8s_series_intro/"},{"categories":["Cloud Native"],"content":"1. 介绍 Kubernetes 是一个可移植的、可扩展的开源平台，用于管理容器化的工作负载和服务，可促进声明式配置和自动化。 Kubernetes 拥有一个庞大且快速增长的生态系统。Kubernetes 的服务、支持和工具广泛可用。 名称 Kubernetes 源于希腊语，意为“舵手”或“飞行员”。Google 在 2014 年开源了 Kubernetes 项目。 Kubernetes 建立在 Google 在大规模运行生产工作负载方面拥有十几年的经验 的基础上，结合了社区中最好的想法和实践。 ","date":"2020-10-15","objectID":"/k8s_series_intro/:2:0","tags":["kubernetes"],"title":"Kubernetes系列：概述","uri":"/k8s_series_intro/"},{"categories":["Cloud Native"],"content":"2. 部署模式的发展史 让我们回顾一下为什么 Kubernetes 如此有用。 传统部署时代： 早期，各个组织机构在物理服务器上运行应用程序。无法为物理服务器中的应用程序定义资源边界，这会导致资源分配问题。 例如，如果在物理服务器上运行多个应用程序，则可能会出现一个应用程序占用大部分资源的情况， 结果可能导致其他应用程序的性能下降。 一种解决方案是在不同的物理服务器上运行每个应用程序，但是由于资源利用不足而无法扩展， 并且维护许多物理服务器的成本很高。 虚拟化部署时代： 作为解决方案，引入了虚拟化。虚拟化技术允许你在单个物理服务器的 CPU 上运行多个虚拟机（VM）。 虚拟化允许应用程序在 VM 之间隔离，并提供一定程度的安全，因为一个应用程序的信息 不能被另一应用程序随意访问。 虚拟化技术能够更好地利用物理服务器上的资源，并且因为可轻松地添加或更新应用程序 而可以实现更好的可伸缩性，降低硬件成本等等。 每个 VM 是一台完整的计算机，在虚拟化硬件之上运行所有组件，包括其自己的操作系统。 容器部署时代： 容器类似于 VM，但是它们具有被放宽的隔离属性，可以在应用程序之间共享操作系统（OS）。 因此，容器被认为是轻量级的。容器与 VM 类似，具有自己的文件系统、CPU、内存、进程空间等。 由于它们与基础架构分离，因此可以跨云和 OS 发行版本进行移植。 容器因具有许多优势而变得流行起来。下面列出的是容器的一些好处： 敏捷应用程序的创建和部署：与使用 VM 镜像相比，提高了容器镜像创建的简便性和效率。 持续开发、集成和部署：通过快速简单的回滚（由于镜像不可变性），支持可靠且频繁的 容器镜像构建和部署。 关注开发与运维的分离：在构建/发布时而不是在部署时创建应用程序容器镜像， 从而将应用程序与基础架构分离。 可观察性不仅可以显示操作系统级别的信息和指标，还可以显示应用程序的运行状况和其他指标信号。 跨开发、测试和生产的环境一致性：在便携式计算机上与在云中相同地运行。 跨云和操作系统发行版本的可移植性：可在 Ubuntu、RHEL、CoreOS、本地、 Google Kubernetes Engine 和其他任何地方运行。 以应用程序为中心的管理：提高抽象级别，从在虚拟硬件上运行 OS 到使用逻辑资源在 OS 上运行应用程序。 松散耦合、分布式、弹性、解放的微服务：应用程序被分解成较小的独立部分， 并且可以动态部署和管理 - 而不是在一台大型单机上整体运行。 资源隔离：可预测的应用程序性能。 资源利用：高效率和高密度。 ","date":"2020-10-15","objectID":"/k8s_series_intro/:3:0","tags":["kubernetes"],"title":"Kubernetes系列：概述","uri":"/k8s_series_intro/"},{"categories":["Cloud Native"],"content":"3. 好处 容器是打包和运行应用程序的好方式。在生产环境中，你需要管理运行应用程序的容器，并确保不会停机。 例如，如果一个容器发生故障，则需要启动另一个容器。如果系统处理此行为，会不会更容易？ 这就是 Kubernetes 来解决这些问题的方法！ Kubernetes 为你提供了一个可弹性运行分布式系统的框架。 Kubernetes 会满足你的扩展要求、故障转移、部署模式等。 例如，Kubernetes 可以轻松管理系统的 Canary 部署。 Kubernetes 为你提供： 服务发现和负载均衡 Kubernetes 可以使用 DNS 名称或自己的 IP 地址公开容器，如果进入容器的流量很大， Kubernetes 可以负载均衡并分配网络流量，从而使部署稳定。 存储编排 Kubernetes 允许你自动挂载你选择的存储系统，例如本地存储、公共云提供商等。 自动部署和回滚 你可以使用 Kubernetes 描述已部署容器的所需状态，它可以以受控的速率将实际状态 更改为期望状态。例如，你可以自动化 Kubernetes 来为你的部署创建新容器， 删除现有容器并将它们的所有资源用于新容器。 自动完成装箱计算 Kubernetes 允许你指定每个容器所需 CPU 和内存（RAM）。 当容器指定了资源请求时，Kubernetes 可以做出更好的决策来管理容器的资源。 自我修复 Kubernetes 重新启动失败的容器、替换容器、杀死不响应用户定义的 运行状况检查的容器，并且在准备好服务之前不将其通告给客户端。 密钥与配置管理 Kubernetes 允许你存储和管理敏感信息，例如密码、OAuth 令牌和 ssh 密钥。 你可以在不重建容器镜像的情况下部署和更新密钥和应用程序配置，也无需在堆栈配置中暴露密钥。 ","date":"2020-10-15","objectID":"/k8s_series_intro/:4:0","tags":["kubernetes"],"title":"Kubernetes系列：概述","uri":"/k8s_series_intro/"},{"categories":["Cloud Native"],"content":"4. k8s不是什么？ Kubernetes 不是传统的、包罗万象的 PaaS（平台即服务）系统。 由于 Kubernetes 在容器级别而不是在硬件级别运行，它提供了 PaaS 产品共有的一些普遍适用的功能， 例如部署、扩展、负载均衡、日志记录和监视。 但是，Kubernetes 不是单体系统，默认解决方案都是可选和可插拔的。 Kubernetes 提供了构建开发人员平台的基础，但是在重要的地方保留了用户的选择和灵活性。 Kubernetes： 不限制支持的应用程序类型。 Kubernetes 旨在支持极其多种多样的工作负载，包括无状态、有状态和数据处理工作负载。 如果应用程序可以在容器中运行，那么它应该可以在 Kubernetes 上很好地运行。 不部署源代码，也不构建你的应用程序。 持续集成(CI)、交付和部署（CI/CD）工作流取决于组织的文化和偏好以及技术要求。 不提供应用程序级别的服务作为内置服务，例如中间件（例如，消息中间件）、 数据处理框架（例如，Spark）、数据库（例如，mysql）、缓存、集群存储系统 （例如，Ceph）。这样的组件可以在 Kubernetes 上运行，并且/或者可以由运行在 Kubernetes 上的应用程序通过可移植机制（例如， 开放服务代理）来访问。 不要求日志记录、监视或警报解决方案。 它提供了一些集成作为概念证明，并提供了收集和导出指标的机制。 不提供或不要求配置语言/系统（例如 jsonnet），它提供了声明性 API， 该声明性 API 可以由任意形式的声明性规范所构成。 不提供也不采用任何全面的机器配置、维护、管理或自我修复系统。 此外，Kubernetes 不仅仅是一个编排系统，实际上它消除了编排的需要。 编排的技术定义是执行已定义的工作流程：首先执行 A，然后执行 B，再执行 C。 相比之下，Kubernetes 包含一组独立的、可组合的控制过程， 这些过程连续地将当前状态驱动到所提供的所需状态。 如何从 A 到 C 的方式无关紧要，也不需要集中控制，这使得系统更易于使用 且功能更强大、系统更健壮、更为弹性和可扩展。 ","date":"2020-10-15","objectID":"/k8s_series_intro/:5:0","tags":["kubernetes"],"title":"Kubernetes系列：概述","uri":"/k8s_series_intro/"},{"categories":["Cloud Native"],"content":"5. 结论 ","date":"2020-10-15","objectID":"/k8s_series_intro/:6:0","tags":["kubernetes"],"title":"Kubernetes系列：概述","uri":"/k8s_series_intro/"},{"categories":["Cloud Native"],"content":"开篇","date":"2020-10-10","objectID":"/k8s_series/","tags":["kubernetes"],"title":"Kubernetes系列：开篇","uri":"/k8s_series/"},{"categories":["Cloud Native"],"content":"系列目录 《Kubernetes系列：开篇》 《Kubernetes系列：概述》 《Kubernetes系列：架构》 《Kubernetes系列：容器》 《Kubernetes系列：网络》 《Kubernetes系列：存储》 《Kubernetes系列：Service》 《Kubernetes系列：Ingress》 《Kubernetes系列：OAM》 ","date":"2020-10-10","objectID":"/k8s_series/:1:0","tags":["kubernetes"],"title":"Kubernetes系列：开篇","uri":"/k8s_series/"},{"categories":["Cloud Native"],"content":"1. 介绍 最近整体工作在往云原生和k8s上迁移，想将自己对于k8s的一些学习心得和经验写成一个系列记录下来。 ","date":"2020-10-10","objectID":"/k8s_series/:2:0","tags":["kubernetes"],"title":"Kubernetes系列：开篇","uri":"/k8s_series/"},{"categories":["Cloud Native"],"content":"2. 云计算 云计算是一种按使用量付费的模式，这种模式提供可用的、便捷的、按需的网络访问， 进入可配置的计算资源共享池（资源包括网络，服务器，存储，应用软件，服务），这些资源能够被快速提供，只需投入很少的管理工作，或与服务供应商进行很少的交互。 云计算最基本的特性是：“按使用量付费”、“资源共享池”和多租户隔离。 1.2 云计算的特点 超大规模 云具有相当的规模，Google 云计算已经拥有 100 多万台服务器， Amazon、IBM、微软、Yahoo 等的云均拥有几十万台服务器。企业私有云一般拥有数百上千台服务器。云能赋予用户前所未有的计算能力。 虚拟化 云计算支持用户在任意位置、使用各种终端获取应用服务。所请求的资源来自云，而不是固定的有形的实体。应用在云中某处运行，但实际上用户无需了解、也不用担心应用运行的具体位置。只需要一台笔记本或者一个手机，就可以通过网络服务来实现我们需要的一切，甚至包括超级计算这样的任务。 高可靠性 云使用了数据多副本容错、计算节点同构可互换等措施来保障服务的高可靠性，使用云计算比使用本地计算机可靠。 通用性 云计算不针对特定的应用，在云的支撑下可以构造出千变万化的应用，同一个云可以同时支撑不同的应用运行。 高可扩展性 云的规模可以动态伸缩，满足应用和用户规模增长的需要。 按需服务 云是一个庞大的资源池，你按需购买;云可以像自来水，电，煤气那样计费。 极其廉价 由于云的特殊容错措施可以采用极其廉价的节点来构成云，云的自动化集中式管理使大量企业无需负担日益高昂的数据中心管理成本，云的通用性使资源的利用率较之传统系统大幅提升，因此用户可以充分享受云的低成本优势，经常只要花费几百美元、几天时间就能完成以前需要数万美元、数月时间才能完成的任务。 潜在的危险性 云计算服务除了提供计算服务外，还必然提供了存储服务。但是云计算服务当前垄断在私人机构(企业)手中，而他们仅仅能够提供商业信用。对于政府机构、商业机构(特别像银行这样持有敏感数据的商业机构)对于选择云计算服务应保持足够的警惕。一旦商业用户大规模使用私人机构提供的云计算服务，无论其技术优势有多强，都不可避免地让这些私人机构以数据(信息)的重要性挟制整个社会。 对于信息社会而言，信息是至关重要的。另一方面，云计算中的数据对于数据所有者以外的其他用户云计算用户是保密的，但是对于提供云计算的商业机构而言确实毫无秘密可言。所有这些潜在的危险，是商业机构和政府机构选择云计算服务、特别是国外机构提供的云计算服务时，不得不考虑的一个重要的前提。 1.3 云计算的分类 公有云：只有使用权，使用的时候进行按需付费。但数据放在别人家。数据安全没有保障。而且银行不会使用公有云，金融行业不要使用公有云。公有云的核心属性是共享资源服务。 私有云：自己的机房搭建的云，私有云有局限性，资源固定；数据比较安全。私有云的核心属性是专有资源。 混合云：主要任务放到私有云，临时需要时利用混合云，它将公有云和私有云进行混合匹配，以获得最佳的效果，这种个性的解决方案，达到二既省钱又安全的目的。 1.4 云计算分层 云计算也是层的，大概有以下几种： 传统 IT 基本所有的都需要自行管理，比如：网络、存储、服务器、虚拟化，操作系统、中间件、运行环境、数据、应用等。 IaaS: Infrastructure-as-a-Service（基础设施即服务） IaaS 主要作用是提供虚拟机或者其他资源作为服务提供给用户。 PaaS: Platform-as-a-Service（平台即服务） PaaS, 中文名为平台即服务。如果以传统计算机架构中 “硬件+操作系统/开发工具+应用软件” 的观点来看待，那么云计算的平台层应该提供类似操作系统和开发工具的功能。实际上也的确如此，PaaS 定位于通过互联网为用户提供一整套开发、运行和运行应用软件的支撑平台。就像在个人计算机软件开发模式下，程序员可能会在一台装有 Windows 或 Linux 操作系统的计算机上使用开发工具开发并部署应用软件一样。PaaS 某些时候也叫做中间件，主要作用是提供一个开发和运行平台给用户。 SaaS: Software-as-a-Service（软件即服务） SaaS，软件即服务。简单地说，就是一种通过互联网提供软件服务的软件应用模式。在这种模式下，用户不需要再花费大量投资用于硬件、软件和开发团队的建设，只需要支付一定的租赁费用，就可以通过互联网享受到相应的服务，而且整个系统的维护也由厂商负责。 如果要用一句话来概括 IaaS、PaaS 和 SaaS 的话，那就是：如果把云计算比喻成一部手机，那么 IaaS 就是硬件，你要自己写代码研发系统才能用；PaaS 是手机系统，你要实现什么功能还是要装各种软件；SaaS 就是硬件+系统+软件，你要干什么一句话就能解决。 ","date":"2020-10-10","objectID":"/k8s_series/:3:0","tags":["kubernetes"],"title":"Kubernetes系列：开篇","uri":"/k8s_series/"},{"categories":["Cloud Native"],"content":"3. 虚拟化 2.1 虚拟化概念 虚拟化是通过软件手段对计算机硬件资源镜像整合管理和再分配的一种技术，常用的手段有基于虚拟机的虚拟化和基于容器的虚拟化。 2.2 虚拟化技术分类 2.2.1 按应用场景分类 操作系统虚拟化 应用程序虚拟化 桌面应用虚拟化 存储虚拟化 网络虚拟化 2.2.2 按照应用模式分类 一对多：其中将一个物理服务器划分为多个虚拟服务器，这是典型的服务器整合模式。 多对一：其中整合了多个虚拟服务器，并将它们作为一个资源池，这是典型的网格计算模式。 多对多：将前两种模式结合在一起。 2.2.3 按硬件资源调用模式分类 全虚拟化 全虚拟化，虚拟化操作系统与底层硬件完全隔离。由中间的 Hypervisor 层转化虚拟化客户操作系统对底层硬件的调用代码，全虚拟化无需更改客户端操作系统，并兼容性好。典型代表有：Vmware Workstation、KVM。 半虚拟化 半虚拟化，在虚拟客户操作系统中加入特定的虚拟化指令，通过这些指令可以直接通过 Hypervisor 层调用硬件资源，免除有 Hypervisor 层转换指令的性能开销。半虚拟化的典型代表 Microsoft Hyper-V、Vmware 的 vSphere。 注：针对 IO 层面半虚拟化要比全虚拟化要好，因为磁盘 IO 多一层必定会慢。一般说 IO 就是网络 IO 和磁盘 IO，因为这两个相对而言是比较慢的。 2.3 基于虚拟机（Hypervisor-based）的虚拟化 它通过一个软件层的封装，提供和物理硬件相同的输入输出表现。实现了操作系统和计算机硬件的解耦，将 OS 和计算机间从 1 对 1 变成了多对多（实际上是 1 对多）的关系。该软件层称为虚拟机管理器（VMM / Hypervisor），它可以直接运行在裸机上（Xen、VMware EXSi），也可以运行在操作系统上（KVM、VMware Workstation）。这项技术已经很成熟了,（发展了40 多年），但仍然存在以下几个问题： 在虚拟机上运行了一个完整的操作系统（GuestOS），在其下执行的还有虚拟化层和宿主机操作系统，一定比直接在物理机上运行相同的服务性能差； 有 GuestOS 的存在，虚拟机镜像往往有几个 G 到几十个 G，占用的存储空间大，便携性差； 想要使用更多硬件资源，需要启动一台新的虚拟机。要等待 GuesOS 启动，可能需要几十秒到几分钟不等。 实际使用场景中，我们使用虚拟化技术其实是为了按需分配资源来完成服务的部署和使用，同时对服务所依赖的环境进行隔离，不被其它服务感知或干扰。为此启动一个 GuestOS 并不是必需的，为什么不考虑让多个虚拟机公用一个操作系统内核，只隔离开服务运行环境同时控制服务使用的系统资源呢？基于容器的虚拟化就是这样一种技术。 2.4 基于容器的虚拟化 容器是没有 GuestOS 的轻量级虚拟机，多个容器共享一个 OS 内核，容器中包含需要部署的应用和它依赖的系统环境，容器大小通常只有几十到几百 MB。由于共享操作系统内核，所以容器依赖于底层的操作系统，各个操作系统大都有自己的容器技术和容器工具。 Docker 是一个 Linux 容器管理工具，随着 Docker 的兴起，Linux 容器技术也是当下最时兴的容器虚拟化技术。Linux 容器工具有很多，OpenVZ、LXC、Docker、Rocket、Lmctfy 等等，大都是基于 Linux 内核提供的两个机制：Cgroups（实现资源按需分配）和 Namespace（实现任务隔离）。 2.5 二种虚拟化技术的区别 虚拟机技术已经发展了很多年，虚拟机和虚拟化层间的接口、虚拟机镜像格式等都已经标准化了。相应的管理工具、分布式集群管理工具都有比较完善的解决方案，而容器最近几年才兴起，配套技术和标准还在完善中； 虚拟机由于有 GuestOS 存在，可以和宿主机运行不同 OS，而容器只能支持和宿主机内核相同的操作系统； 虚拟机由于有 VMM 的存在，虚拟机之间、虚拟机和宿主机之间隔离性很好。而容器之间公用宿主机的内核，共享系统调用和一些底层的库，隔离性相对较差； 容器比虚拟机明显更轻量级，对宿主机操作系统而言，容器就跟一个进程差不多。因此容器有着更快的启动速度（秒级甚至更快），更高密度的存储和使用（镜像小）、更方便的集群管理等优点。同时由于没有 GuestOS 存在，在容器中运行应用和直接在宿主机上几乎没有性能损失，比虚拟机明显性能上有优势。 ","date":"2020-10-10","objectID":"/k8s_series/:4:0","tags":["kubernetes"],"title":"Kubernetes系列：开篇","uri":"/k8s_series/"},{"categories":["Cloud Native"],"content":"4. 容器 实际使用场景中，我们使用虚拟化技术其实是为了按需分配资源来完成服务的部署和使用，同时对服务所依赖的环境进行隔离，不被其它服务感知或干扰。为此启动一个GuestOS并不是必需的，为什么不考虑让多个虚拟机公用一个操作系统内核，只隔离开服务运行环境同时控制服务使用的系统资源呢？基于容器的虚拟化就是这样一种技术。其中最著名的就是Docker了 Docker是以Docker容器为资源分割和调度的基本单位，封装整个运行时环境，为开发者和系统管理员设计的，用于构建、发布、和运行分布式应用的平台。它是一个跨平台，可移植并且简单易用的容器解决方案。 容器技术的好处 持续部署与测试 容器消除了线上线下的环境差异，保证了应用生命周期的环境一致性和标准化。开发人员使用镜像实现标准开发环境的构建，开发完成后通过封装着完整环境和应用的镜像进行迁移。由此，测试和运维人员可以直接部署软件镜像来进行测试和发布，大大简化了持续集成、测试和发布的过程。 跨平台支持 容器带来的最大好处之一就是其适配性，越来越多的云平台都支持容器，用户再也无需担心应平台的捆绑，同时也能让应用多平台混合部署成为可能。目前支持容器的IaaS平台包括但不限于亚马逊平台（AWS)、Google云平台（GCP）、微软云平台（Azure）、OpenStack等，还包括如Chef、Puppet、Ansible等配置管理工具。 环境标准化和版本控制 基于容器提供的环境一致性和标准化，你可以使用Git等工具对容器镜像进行版本控制，相比于代码的版本控制来说，你还能够对整个应用运行环境实现版本控制，一旦出现故障可以快速回滚。相比以前的虚拟机镜像，容器压缩和备份速度更快，镜像启动也像启动一个普通进行一样快速。 高资源利用率与隔离 容器没有管理程序的额外开销，与底层共享操作系统，性能更加优良，系统负载更低，在同等条件下可以更充分地利用系统资源。同时，容器拥有不错的资源隔离与限制能力，可以精确地对应用分配CPU和内存等资源，保证了应用间不会相互影响。 容器跨平台与镜像 linux容器虽然早在Linux 2.6版本内核已经存在，但是缺少容器的跨平台性，难以推广。容器在原有Linux容器的基础上大胆革新，为容器设定了一整套标准化的配置方法，将应用依赖的运行环境打包成镜像，真正实现了“构建一次，到处运行”的理念，大大提高了容器的跨平台性。 易于理解且易用 Docker的英文原意是集装箱码头工人，标志是鲸鱼运送一大堆集装箱，集装箱就是容器，生动好记，易于理解。一个开发者可以在15分钟内入门Docker并进行安装和部署，这是容器史上的一次飞跃。因为它的易用性，有更多的人开始关注容器技术，加速了容器标准化的步伐。 应用镜像仓库 Docker官方构建了一个镜像仓库，组织和管理形式类似于GitHub,其上已累积了成千上万的镜像，因为Docker的跨平台适配性，相当于用户提供了一个非常有用的应用商店，所有人都可以自由地下载为服务组件，这为开发者提供了巨大便利。 ","date":"2020-10-10","objectID":"/k8s_series/:5:0","tags":["kubernetes"],"title":"Kubernetes系列：开篇","uri":"/k8s_series/"},{"categories":["Cloud Native"],"content":"5. Kubernetes 真正的生产型应用会涉及多个容器。这些容器必须跨多个服务器主机进行部署。容器安全性需要多层部署，因此可能会比较复杂。因此出现了kubernetes、swarm、mesos等编排软件，最终kubernetes实现了大一统。 Kubernetes 可以提供所需的编排和管理功能，以便您针对这些工作负载大规模部署容器。借助 Kubernetes 编排功能，您可以构建跨多个容器的应用服务、跨集群调度、扩展这些容器，并长期持续管理这些容器的健康状况。有了 Kubernetes，您便可切实采取一些措施来提高 IT 安全性。 Kubernetes 还需要与联网、存储、安全性、遥测和其他服务整合，以提供全面的容器基础架构。 当然，这取决于您如何在您的环境中使用容器。Linux 容器中的基本应用将它们视作高效、快速的虚拟机。一旦把它部署到生产环境或扩展为多个应用，您显然需要许多托管在相同位置的容器来协同提供各种服务。随着这些容器的累积，您运行环境中容器的数量会急剧增加，复杂度也随之增长。 Kubernetes 通过将容器分类组成 “容器集” （pod），解决了容器增殖带来的许多常见问题容器集为分组容器增加了一个抽象层，可帮助您调用工作负载，并为这些容器提供所需的联网和存储等服务。Kubernetes 的其它部分可帮助您在这些容器集之间达成负载平衡，同时确保运行正确数量的容器，充分支持您的工作负载。 如果能正确实施 Kubernetes，再辅以其它开源项目（例如 Atomic 注册表、Open vSwitch、heapster、OAuth 以及 SELinux），您就能够轻松编排容器基础架构的各个部分。 ","date":"2020-10-10","objectID":"/k8s_series/:6:0","tags":["kubernetes"],"title":"Kubernetes系列：开篇","uri":"/k8s_series/"},{"categories":["Cloud Native"],"content":"6. 结论 ","date":"2020-10-10","objectID":"/k8s_series/:7:0","tags":["kubernetes"],"title":"Kubernetes系列：开篇","uri":"/k8s_series/"},{"categories":["DevOps"],"content":"SRE介绍","date":"2020-07-29","objectID":"/devops_series_sre/","tags":["devops"],"title":"DevOps系列：SRE","uri":"/devops_series_sre/"},{"categories":["DevOps"],"content":"系列目录 《DevOps系列：开篇》 《DevOps系列：概述》 《DevOps系列：CMDB》 《DevOps系列：CI/CD》 《DevOps系列：监控》 《DevOps系列：SRE》 ","date":"2020-07-29","objectID":"/devops_series_sre/:1:0","tags":["devops"],"title":"DevOps系列：SRE","uri":"/devops_series_sre/"},{"categories":["DevOps"],"content":"困局 计算机软件系统离开人通常是无法自主运行的，那要如何去运维一个日趋复杂的大型分布式计算机系统呢？ ","date":"2020-07-29","objectID":"/devops_series_sre/:2:0","tags":["devops"],"title":"DevOps系列：SRE","uri":"/devops_series_sre/"},{"categories":["DevOps"],"content":"Dev/Ops分离团队模型 雇佣系统管理员(sysadmin)运维复杂的计算机系统是行业内一直以来的普遍做法，系统管理员的工作主要在于应对系统中产生的各种需要人工干预的事件，以及来自业务部门的变更需求。但随着系统变得复杂，组件越来越多，流量不断上升，相关的事件和变更需求也会越来越多，就需要招聘更多的系统管理员。系统管理员的日常工作和研发工程师的相差甚远，通常归属于两个不同的部门，开发部(Dev)和运维部(Ops)。也就是Dev/Ops分离团队模型。 但是这个模型存在两个无法避免的问题。 直接成本。随着系统复杂度的增加，部署规模的扩大，团队的大小基本与系统负载成线性相关，共同成长。 间接成本。即研发团队和运维团队之间的沟通成本。研发团队想要\"随时随地发布新功能，没有任何阻拦\"，运维团队想要”一旦一个东西在生产环境中正常工作了，就不要再进行任何改动“。本质来说，两个团队的目标是互相矛盾的。 ","date":"2020-07-29","objectID":"/devops_series_sre/:2:1","tags":["devops"],"title":"DevOps系列：SRE","uri":"/devops_series_sre/"},{"categories":["DevOps"],"content":"解决之道 ","date":"2020-07-29","objectID":"/devops_series_sre/:3:0","tags":["devops"],"title":"DevOps系列：SRE","uri":"/devops_series_sre/"},{"categories":["DevOps"],"content":"DevOps DevOps（开发 Development 与运维 Operations 的组合词）是一种文化、一场运动或实践，强调在自动化软件交付流程及基础设施变更过程中，软件开发人员与其他信息技术（IT）专业人员彼此之间的协作与沟通。它旨在建立一种文化与环境，使构建、测试、软件发布得以快速、频繁以及更加稳定地进行。 ","date":"2020-07-29","objectID":"/devops_series_sre/:3:1","tags":["devops"],"title":"DevOps系列：SRE","uri":"/devops_series_sre/"},{"categories":["DevOps"],"content":"SRE SRE可以理解为DevOps的一种实践，SRE基本是在进行由运维团队完成的工作，但是雇佣具有软件专业知识的工程师，通过创造软件系统的方式来维护系统运行并替代传统模型中的人工操作。本质上，SRE是在用软件工程的思维和方法论，通过设计、构建自动化工具来完成以前由系统管理员人工操作完成的任务。 SRE方法论 1. 确保长期关注研发工作 SRE团队应将运维工作限制在50%以内，并将剩余时间投入到研发项目上 2. 在保障SLO的前提下最大化迭代速度 错误预算，任何产品都不是，也不应该做到100%可靠，部门建立起一个合理的可靠性目标，错误预算等于”1-可靠性目标“，通过错误预算来最大化新功能上线的速度，同时保障服务质量。 3. 监控系统 监控系统是SRE团队监控服务质量和可用性的一个主要手段。一个监控系统应该只有三类输出： 紧急警报(alert)，意味着收到警报的用户需要立即执行某种操作 工单(ticket)，意味着接受工单的用户应该执行某种操作，但是并发立即执行。 日志(logging)，平时没有人需要关注日志信息，但是日志信息依然被收集起来以备调试和事后分析时使用 4. 应急事件处理 可靠性是MTTF(平均失败时间)和MTTR(平均恢复时间)的函数。评价一个团队将系统恢复到正常情况的最有效的指标，就是MTTR。 任何需要人工操作的事情都只会延长恢复时间。但有时候人工介入不可避免时，可以通过事先预案并且将最佳方法记录到”运维手册“上来降低MTTR。 5. 变更管理 变更管理的最佳实践是使用自动化来完成以下几个项目： 采用渐进式发布机制 迅速而准确地检测到问题的发生 当问题发生时，安全迅速的回滚 6. 需要预测和容量规划 需要预测和容量规划是保障一个业务有足够的容量和冗余度去服务预测中的未来需要。 容量规划有几个必需步骤： 必须有一个准确的自然增长需求预测模型，需求预测的时间应该超过资源获取的时间 规划中必须有准确的非自然增长的需求来源的统计 必须有周期性压力测试，以便准确地将系统原始资源信息与业务容量对应起来。 7. 效率与性能 高效地利用各种资源是任何赢利性服务都要关心的，一个服务的利用率指标通常依赖于这个服务的工作方式以及对容量的配置与部署上。如果能通过密切关注一个服务的容量配置策略，进而改进其资源利用率，可以有效地降低系统的总成本。 ","date":"2020-07-29","objectID":"/devops_series_sre/:3:2","tags":["devops"],"title":"DevOps系列：SRE","uri":"/devops_series_sre/"},{"categories":["DevOps"],"content":"成为SRE ","date":"2020-07-29","objectID":"/devops_series_sre/:4:0","tags":["devops"],"title":"DevOps系列：SRE","uri":"/devops_series_sre/"},{"categories":["DevOps"],"content":"技能要求 – TCP/IP网络模型 (OSI模型) – Unix/Linux 系统 – Unix/Linux 系统管理 – 数据结构与算法 – 编程语言 – SQL和数据库管理 – 人员管理 – 项目管理 ","date":"2020-07-29","objectID":"/devops_series_sre/:4:1","tags":["devops"],"title":"DevOps系列：SRE","uri":"/devops_series_sre/"},{"categories":["DevOps"],"content":"能力等级 0 – 对于相关的技术领域还不熟悉。 1 – 可以读懂这个领域的基础知识。 2 – 可以实现一些小的改动，清楚基本的原理，并能够在简单的指导下自己找到更多的细节。 3 – 基本精通这个技术领域，完全不需要别人的帮助。 4 – 对这个技术领域非常的熟悉和舒适，可以应对和完成所有的日常工作。 ​ 4 – 1 对于软件领域 – 有能力开发中等规模的程序，能够熟练和掌握并使用所有的语言特性，而不是需要翻书，并且能够找到所有的冷知识。 ​ 4 – 2 对于系统领域 – 掌握网络和系统管理的很多基础知识，并能够掌握一些内核知识以运维一个小型的网络系统，包括恢复、调试和能解决一些不常见的故障。 5 – 对于该技术领域有非常底层的了解和深入的技能。 6 – 能够从零开发大规模的程序和系统，掌握底层和内在原理，能够设计和部署大规模的分布式系统架构。 7 – 理解并能利用高级技术，以及相关的内在原理，并可以从根本上自动化大量的系统管理和运维工作。 8 – 对于一些边角和晦涩的技术、协议和系统工作原理有很深入的理解和经验。能够设计，部署并负责非常关键以及规模很大的基础设施，并能够构建相应的自动化设施。 9 – 能够在该技术领域出一本经典的书。并和标准委员会的人一起工作制定相关的技术标准和方法。 10 – 在该领域写过一本书，被业内尊为专家，并是该技术的发明人。 ","date":"2020-07-29","objectID":"/devops_series_sre/:4:2","tags":["devops"],"title":"DevOps系列：SRE","uri":"/devops_series_sre/"},{"categories":["DevOps"],"content":"Roadmap 参考 Google SRE 运维解密 Google SRE Workbook Roadmap ","date":"2020-07-29","objectID":"/devops_series_sre/:4:3","tags":["devops"],"title":"DevOps系列：SRE","uri":"/devops_series_sre/"},{"categories":["DevOps"],"content":"DevOps成熟度模型","date":"2020-07-26","objectID":"/devops_model/","tags":["devops","translation"],"title":"[译]DevOps成熟度模型——讲解","uri":"/devops_model/"},{"categories":["DevOps"],"content":"原文链接：DevOps Maturity Model – Explained 水平有限，本文不免存在遗漏或错误之处。如有疑问，请查阅原文。 DevOps通过改善团队在方法链和工作流程中的运作和合作方式，改变了IT行业。实际上，根据最近的一项调查，有63％的公司报告说在采用DevOps之后其软件部署质量得到了改善。 到目前为止，大部分的公司已经在其软件开发过程中完成了一些DevOps实施阶段。但是，尽管有几家公司在采用DevOps方面受益匪浅，但许多公司仍未充分发挥其潜力。 关于DevOps选择的最常见错误仍然存在，即“将其作为旅程或目标”。 这就是“ DevOps成熟度模型”的用处。阅读以下文章，深入了解什么是DevOps成熟度模型以及它如何为您提供帮助。 ","date":"2020-07-26","objectID":"/devops_model/:0:0","tags":["devops","translation"],"title":"[译]DevOps成熟度模型——讲解","uri":"/devops_model/"},{"categories":["DevOps"],"content":"1. 理解DevOps成熟度 通过描述，DevOps成熟度被定义为一种模式，该模式确定组织在DevOps课程中的位置以及决定要执行的操作以获得期望的结果。 理解将DevOps“作为一个连续的旅程，而不是一个目的地”，对于管理DevOps的成熟度至关重要。 DevOps成熟度设计通过双方和组织方面的不断培训来管理增长。更多的才能和技能，将有更大的能力来处理规模和复杂性问题。 ","date":"2020-07-26","objectID":"/devops_model/:1:0","tags":["devops","translation"],"title":"[译]DevOps成熟度模型——讲解","uri":"/devops_model/"},{"categories":["DevOps"],"content":"2. DevOps成熟度所需的能力 ","date":"2020-07-26","objectID":"/devops_model/:2:0","tags":["devops","translation"],"title":"[译]DevOps成熟度模型——讲解","uri":"/devops_model/"},{"categories":["DevOps"],"content":"2.1 文化与策略 DevOps必须被视为一种文化驱动的计划，该计划吸引了不同的团队，将他们推向共同的目标。向DevOps的过渡涉及在一系列方法和框架的支持下，以及组织工作文化的变化。因此，这需要适当的计划和全面的程序。 ","date":"2020-07-26","objectID":"/devops_model/:2:1","tags":["devops","translation"],"title":"[译]DevOps成熟度模型——讲解","uri":"/devops_model/"},{"categories":["DevOps"],"content":"2.2 自动化 自动化是DevOps方法中持续交付和持续部署工具的代码。通过自动化执行的任务，自动化流程有助于DevOps系列产品的开发，实验和生产，从而节省了时间并提高了资源效率。 ","date":"2020-07-26","objectID":"/devops_model/:2:2","tags":["devops","translation"],"title":"[译]DevOps成熟度模型——讲解","uri":"/devops_model/"},{"categories":["DevOps"],"content":"2.3 结构与过程 这是DevOps文化的最重要方面。对于同一地点的DevOps和团队而言，协作和共享至关重要，或者不同地点的DevOps和团队需要加入工具和资源才能达成共同的目标。 根据《福布斯》的一项研究，组织通常会在DevOps课程的一部分中找到以下步骤： 潜意识的不足：组织忽视了学习DevOps及其好处 有意识但是无能：即使经过一些工业化的DevOps课程，即使12-18个月后，组织仍然看到孤立的方法。 意识强：经过四年的DevOps课程和可靠的自动化，组织专注于团队之间的协作和简化分配机制。 潜意识技能：这里的组织都徘徊在结构化框架，深入协作，有效共享的实际方法上 ","date":"2020-07-26","objectID":"/devops_model/:2:3","tags":["devops","translation"],"title":"[译]DevOps成熟度模型——讲解","uri":"/devops_model/"},{"categories":["DevOps"],"content":"3. DevOps成熟度模型需要什么？ 完整的DevOps成熟度模型通过三种方式定义DevOps成熟度： 评估现代技能水平 确定增长措施 概述实现DevOps目标的步骤 与这三个级别一致，DevOps成熟度模块支持跨表单，数据和基础架构级别的开发，部署和测试阶段的成熟度： ","date":"2020-07-26","objectID":"/devops_model/:3:0","tags":["devops","translation"],"title":"[译]DevOps成熟度模型——讲解","uri":"/devops_model/"},{"categories":["DevOps"],"content":"3.1 应用的DevOps成熟度 这通过从开发到生产阶段的代码开发安全性来定义DevOps的成熟度。为了实现这一点，需要将构建，测试，代码覆盖，安全扫描和监控作为部署管道的自动化元素。 ","date":"2020-07-26","objectID":"/devops_model/:3:1","tags":["devops","translation"],"title":"[译]DevOps成熟度模型——讲解","uri":"/devops_model/"},{"categories":["DevOps"],"content":"3.2 数据的DevOps成熟度 这通过使用DataOps进行课程来自动完成对数据的更改和自动验证功能的能力来定义DevOps的成熟度。 ","date":"2020-07-26","objectID":"/devops_model/:3:2","tags":["devops","translation"],"title":"[译]DevOps成熟度模型——讲解","uri":"/devops_model/"},{"categories":["DevOps"],"content":"3.3 基础设施的DevOps成熟度 这可以通过简化与自动化相关的基础架构处理技能，简化并促进自助服务以存储环境以及其他业务的能力来定义DevOps的成熟度。 总结而言，DevOps成熟度模型包括五个转换阶段： 阶段1：初始阶段 处理了Dev和Ops部门的传统设置。 阶段2：管理 创新心态的起源集中在Dev的活动和Ops的初始自动化中，并强调协作。 阶段3：定义 组织范围内的变化始于既定的规则和安全的自动化。 阶段4：测量 对目标和自动化有更好的了解，并伴随着不断的发展。 阶段5：优化 行动可见，团队差距消失，员工获得荣誉。 尽管这5个步骤构成了整个DevOps成熟度模型，但企业必须在每个阶段不断检查其成熟度，并最终识别中心区域和实现总体任务的方式。 ","date":"2020-07-26","objectID":"/devops_model/:3:3","tags":["devops","translation"],"title":"[译]DevOps成熟度模型——讲解","uri":"/devops_model/"},{"categories":["DevOps"],"content":"4. DevOps成熟度模型应包含哪些内容？ DevOps成熟度模型的每个步骤都需要保留几个参数，以建立组织的DevOps成熟度级别。这些项目理想地定义了组织在DevOps旅程中的发展方式。他们是： 设计与输出的完成数最好应具有较高的投资回报率 安全部署的百分比应胜过已损坏的部署 从经历之时起，奇妙的事件/故障的平均恢复时间（MTTR）应该为零或尽可能低 领先一代，从代码开发到部署的返回，应该是相当的 部署数量限制了新代码部署的频率 阶段性方法和上述参数代表组织的DevOps成熟度成功。 ","date":"2020-07-26","objectID":"/devops_model/:4:0","tags":["devops","translation"],"title":"[译]DevOps成熟度模型——讲解","uri":"/devops_model/"},{"categories":["DevOps"],"content":"5. DevOps成熟度与安全性息息相关 DevOps的成熟度与DevOps的安全性息息相关。随着组织在DevOps旅程中的发展，竞争优势已成为至关重要的市场，需要更快的发布周期，而数字发现则需要大力宣传。 这就是安全性测试开始变得更加严格的地方，这就是为什么DevOps文化要求重新考虑安全系统。 最终，组织必须将安全性作为其DevOps方法的重要组成部分，并使它更接近所有应用程序开发平台。 DevOps专家与安全人员合作，在软件开发生命周期的所有阶段中，以成熟度级别进行新的安全集成。 这可以通过主动的DevSecOps实施来发生。通过检查暴露的资源，诸如容器化之类的解决方案还可以在任何程度上连续参与标签问题。 ","date":"2020-07-26","objectID":"/devops_model/:5:0","tags":["devops","translation"],"title":"[译]DevOps成熟度模型——讲解","uri":"/devops_model/"},{"categories":["DevOps"],"content":"6. DevOps成熟度的业务优势 提供组织的DevOps职位的完整情况，DevOps成熟度指南具有多种业务优势： 更快的开发灵活性 挖掘事件的能力 确定实现领域 可扩展性增强 运营表现 交付活动增加 品质提升 DevOps成熟度模型中包含更多此类商品，该模型使您能够观察到完整的DevOps潜力。 ","date":"2020-07-26","objectID":"/devops_model/:6:0","tags":["devops","translation"],"title":"[译]DevOps成熟度模型——讲解","uri":"/devops_model/"},{"categories":["DevOps"],"content":"7. 结论 DevOps成熟度改善了您的整体组织工作流程，提高了发行率，并缩短了上市时间，从而为您提供了一个优势！ ","date":"2020-07-26","objectID":"/devops_model/:7:0","tags":["devops","translation"],"title":"[译]DevOps成熟度模型——讲解","uri":"/devops_model/"},{"categories":["DevOps"],"content":"监控系统","date":"2020-07-20","objectID":"/devops_series_mon/","tags":["devops"],"title":"DevOps系列：监控","uri":"/devops_series_mon/"},{"categories":["DevOps"],"content":"系列目录 《DevOps系列：开篇》 《DevOps系列：概述》 《DevOps系列：CMDB》 《DevOps系列：CI/CD》 《DevOps系列：监控》 《DevOps系列：SRE》 ","date":"2020-07-20","objectID":"/devops_series_mon/:1:0","tags":["devops"],"title":"DevOps系列：监控","uri":"/devops_series_mon/"},{"categories":["DevOps"],"content":"为什么需要监控系统？ ","date":"2020-07-20","objectID":"/devops_series_mon/:2:0","tags":["devops"],"title":"DevOps系列：监控","uri":"/devops_series_mon/"},{"categories":["DevOps"],"content":"两个场景 场景一 技术部门上线了一个新项目，系统宕机了，客户在访问时发现无法访问，客户 A 不知道如何处理，放弃了访问，客户 B 知道客服系统，告知了运营人员，运营人员自己访问后也发现无法访问，于是通知了测试人员，再由测试人员通知线上项目的负责人，由负责人来进行故障恢复。 整个流程中，能处理故障的人成了最后知道故障的人。 场景二 用户反馈访问某个系统很慢，通知技术人员排查问题，由于系统涉及的组件很多，技术人员没办法立即知道问题出在哪里，于是技术人员只能通过自己把整个数据流走完的方式来排查问题： 1、由入口开始排查问题，先确认网络是否丢包，延时是否过高，发现无异常。 2、于是排查服务所在机器的负载情况，以及服务相关日志 (未必有记录)，也无异常。 3、排查代码发现有做 sql 查询，于是根据 sql 手动到数据库执行，发现 sql 执行很慢。 4、于是排查数据库所在机器的负载情况，发现 cpu 一直处在 100% 状态，是数据库进程造成的。 5、通过查询相关执行 sql 发现有某个 sql 在执行复杂查询导致了 cpu 使用率一直很高，从而影响了其他 sql 查询。 极端情况下，技术人员可能需要把所有相关组件都排查一遍，才能发现问题出在哪里。 ","date":"2020-07-20","objectID":"/devops_series_mon/:2:1","tags":["devops"],"title":"DevOps系列：监控","uri":"/devops_series_mon/"},{"categories":["DevOps"],"content":"场景解决方案 开头提到的两个场景应该是大部分技术人员都会碰到的问题，场景一是故障出现到故障处理的耗时问题，场景二是故障处理到故障恢复的耗时问题。 场景一的解决方式，可以由一个脚本或者一个系统，定时收集客户访问的 url 的返回状态码，如果出现错误的状态码达到一定次数，就发送邮件或者短信给到对应的负载人。 场景二的解决方式，可以由一个系统，定时收集所有组件的相关信息，然后通过聚合和数据展示，来提供一个全局的问题查看功能。 解决上面两种场景的系统就是监控系统。 ","date":"2020-07-20","objectID":"/devops_series_mon/:2:2","tags":["devops"],"title":"DevOps系列：监控","uri":"/devops_series_mon/"},{"categories":["DevOps"],"content":"为什么要监控 监控一个系统有多个原因，一般包含如下几项 分析趋势 数据库多大，增长速度如何？日活用户的增长速度？ 数据对比 增加了 redis 缓存后，访问速度较没增加前如何？这周和上周的访问速度有什么差异？ 告警 有东西故障了，或者即将故障，需要有人处理它。 构建仪表盘 仪表盘应该可以回答有关服务的一些基本问题，通常会包含常见的指标 临时性回溯分析 请求延迟刚刚大幅增加，有没有其他的现象同时发生？ ","date":"2020-07-20","objectID":"/devops_series_mon/:2:3","tags":["devops"],"title":"DevOps系列：监控","uri":"/devops_series_mon/"},{"categories":["DevOps"],"content":"建立监控系统 ","date":"2020-07-20","objectID":"/devops_series_mon/:3:0","tags":["devops"],"title":"DevOps系列：监控","uri":"/devops_series_mon/"},{"categories":["DevOps"],"content":"监控系统基本组件 一个监控系统一般包含下面几个组件： Agent/collector Agent/collector用于定时收集各种需要的指标信息，可以是脚本、程序、埋点。 Server Server用于接收采集回来的指标信息，进行聚合、存储，供后续查询使用。 Dashboard Dashboard用于展示历史指标信息 Alert Alert用于计算告警规则，发送告警的操作 ","date":"2020-07-20","objectID":"/devops_series_mon/:3:1","tags":["devops"],"title":"DevOps系列：监控","uri":"/devops_series_mon/"},{"categories":["DevOps"],"content":"监控系统的工作流程 一个监控系统的工作流程一般如下： 数据采集 安装客户端，可以是脚本、程序、埋点, 定时采集各种需要的数据。 数据接收、存储 Push方式 监控系统提供接口供客户端定时上报数据 Pull方式 客户端提供接口供监控系统定时拉取数据 数据处理 告警 根据一定规则计算采集回来的指标数据，设置阈值，当达到阈值后发送告警。 展示 提供一个仪表板，用来展示采集回来的数据 分析 针对采集回来的数据进行定制化的数据分析 ","date":"2020-07-20","objectID":"/devops_series_mon/:3:2","tags":["devops"],"title":"DevOps系列：监控","uri":"/devops_series_mon/"},{"categories":["DevOps"],"content":"Metric 配置监控时，我们首要面对的是监控数据如何采集的问题。一般我们可以把监控指标分为两类：业务指标和资源指标。 业务指标 业务指标通过衡量有用的输出来指示系统的运行状况。一般包括以下几个 吞吐量 成功率 错误 性能(延迟) 一个Web server的业务指标例子 Subtype Description Value throughput requests per second 312 success 上个周期响应状态码为2xx的百分比 99.1 error 上个周期响应状态码为5xx的百分比 0.1 performance 采集周期内的平均响应时间 0.4 资源指标 资源指标一般包括以下几个： 利用率 饱和度 错误 可用性 一些通用资源的指标例子 Resource Utilization Saturation Errors Availability Disk IO % time that device was busy wait queue length # device errors % time writable Memory % of total memory capacity in use swap usage N/A (not usually observable) N/A Microservice average % time each request-servicing thread was busy # enqueued requests # internal errors such as caught exceptions % time service is reachable Database average % time each connection was busy # enqueued queries # internal errors, e.g. replication errors % time database is reachable Four Golden Signals Four Golden Signals 是 Google 针对大量分布式监控的经验总结，4 个黄金指标可以在服务级别帮助衡量终端用户体验、服务中断、业务影响等层面的问题。主要关注与以下四种类型的指标： 延迟：服务请求所需时间。 记录用户所有请求所需的时间，重点是要区分成功请求的延迟时间和失败请求的延迟时间 流量：监控当前系统的流量，用于衡量服务的容量需求。 流量对于不同类型的系统而言可能代表不同的含义。例如，在 HTTP REST API 中, 流量通常是每秒 HTTP 请求数； 错误：监控当前系统所有发生的错误请求，衡量当前系统错误发生的速率。 对于失败而言有些是显式的 (比如, HTTP 500 错误)，而有些是隐式 (比如，HTTP 响应 200，但实际业务流程依然是失败的)。 对于一些显式的错误如 HTTP 500 可以通过在负载均衡器 (如 Nginx) 上进行捕获，而对于一些系统内部的异常，则可能需要直接从服务中添加钩子统计并进行获取。 饱和度：衡量当前服务的饱和度。 主要强调最能影响服务状态的受限制的资源。 例如，如果系统主要受内存影响，那就主要关注系统的内存状态，如果系统主要受限与磁盘 I/O，那就主要观测磁盘 I/O 的状态。因为通常情况下，当这些资源达到饱和后，服务的性能会明显下降。同时还可以利用饱和度对系统做出预测，比如，“磁盘是否可能在 4 个小时候就满了”。 ","date":"2020-07-20","objectID":"/devops_series_mon/:3:3","tags":["devops"],"title":"DevOps系列：监控","uri":"/devops_series_mon/"},{"categories":["DevOps"],"content":"Alert 报警可以让一个系统发生故障或即将发生故障时主动通知相应的人员，一个紧急报警的处理会占用对应人员的宝贵时间，如果无效信息过多，分析和修复问题课鞥呢会变慢，故障时间也会相应的延长，因此一个高效的报警系统应该能提供足够的信息，并且误报率非常低。 在管理大规模集群的情况下，究竟有多少报警量才是合理的呢？ Google SRE每周只有十条报警，如果超过十条，说明没有把无效报警过滤掉（Google SRE仅负责SLA要求为99.99%的服务）。 那么怎么减少报警量呢？ 这就需要对报警进行优化了。 报警优化 1. 报警值班和报警升级 基于值班表，每天安排两人进行值班处理报警，将值班压力从全团队压缩在两人范围内，从而让团队能够有足够的时间和人力进行优化工作。 同时，为了避免两个值班人员都没有响应报警，可以使用报警升级功能，如果一个报警在5min内值班人员均未响应，或者15min内未处理完毕，或者有严重故障发生，都可以将报警进行升级，通告团队其他成员协助处理。 2. 建立报警等级 Google SRE的实践则是将监控系统的输出分为三类，报警、工单和记录。 SRE的要求是所有的故障级别的报警，都必须是接到报警，有明确的非机械重复的事情要做，且必须马上就得做，才能叫做故障级别的报警。其他要么是工单，要么是记录。 3. 故障自愈 重启作为单机预案，在很多业务线，可以解决至少50%的报警。没有响应，重启试试，请求异常，重启试试，资源占用异常，重启试试，各种问题，重启都屡试不爽。 换言之，针对简单场景具有明确处置方案的报警，自动化是一个比较好的解决方案，能够将人力从大量重复的工作中解放出来。 自动化处理报警的过程中，需要注意以下问题： 自动化处理比例不能超过服务的冗余度（默认串行处理最为稳妥）； 不能对同一个问题在短时间内重复多次地自动化处理（不断重启某个机器上的特定进程）； 在特定情况下可以在全局范围内快速终止自动化处理机制； 尽量避免高危操作（如删除操作、重启服务器等操作）； 每次执行操作都需要确保上一个操作的结果和效果收集分析完毕（如果一个服务重启需要10min）。 4. 持续优化TOP3的报警 2/8定律，80%的报警可能来自20%的指标，对报警数过多的报警进行持续优化，可以减少大量的报警。 5. 基于时间段分而治之 从冗余度角度来分析，如果在流量峰值有20%的冗余度，那么在流量低谷，冗余度至少为50%。 基于冗余度的变换，相应的监控策略的阈值，随机也应该发生一系列的变化。 举例来说，在高峰期，可能一个服务故障20%的实例，就必须介入处理的话，那么在低谷期，可能故障50%的实例，也不需要立即处理，依赖于报警自动化处理功能，逐步修复即可。 6. 报警周期优化，避免瞬报 在监控趋势图中，会看到偶发的一些毛刺或者抖动，这些毛刺和抖动，就是造成瞬报的主要原因。 这些毛刺和抖动，至多定义为异常，而非服务故障，因此应该以非紧急的通知方式进行。 7. 提前预警，防患于未然 对于很多有趋势规律的场景，可以通过提前预警的方式，降低问题的紧迫程度和严重性。 8. 日常巡检 提前预警面向的是有规律的场景，而日常巡检，还可以发现那些没有规律的隐患。 9. 比例为主，绝对值为辅 线上机器的规格不同，如果从绝对值角度进行监控，则无法适配所有的机器规格，势必会产生大量无意义的报警。 10. Code Review 前人埋坑，后人挖坑。在解决存量问题的情况下，不对增量问题进行控制，那报警优化，势必会进入螺旋式缓慢上升的过程，这对于报警优化这类项目来说，无疑是致命的。 通过对新增监控的Code Review，可以让团队成员快速达成一致认知，从而避免监控配置出现千人千面的情况出现。 11. 沉淀标准和最佳实践 仅仅做Code Review还远远不够，一堆人开会，面对一行监控配置，大眼瞪小眼，对不对，为什么不对，怎么做更好？大家没有一个标准，进而会浪费很多时间来进行不断的讨论。 这时候，如果有一个标准，告诉大家什么是好，那么就有了评价标准，很多事情就比较容易做了。 标准本身也是需要迭代和进步的，因此大家并不需要担心说我的标准不够完美。 基于标准，再给出一些最佳的监控时间，那执行起来，就更加容易了。 12. 彻底解决问题不等于自动处理问题 自动化处理问题不等于解决问题，掩耳盗铃也不等于解决问题，什么叫做解决问题，只有是找到问题的根本原因，并消灭之，才能确保彻底解决问题，轻易不会再次发生。 参考 Google SRE 运维解密 datadog monitoring 101 摆脱无效报警？十年运维监控报警优化经验总结 ","date":"2020-07-20","objectID":"/devops_series_mon/:3:4","tags":["devops"],"title":"DevOps系列：监控","uri":"/devops_series_mon/"},{"categories":["DevOps"],"content":"CI/CD","date":"2020-07-15","objectID":"/devops_series_cicd/","tags":["devops"],"title":"DevOps系列：CI/CD","uri":"/devops_series_cicd/"},{"categories":["DevOps"],"content":"系列目录 《DevOps系列：开篇》 《DevOps系列：概述》 《DevOps系列：CMDB》 《DevOps系列：CI/CD》 《DevOps系列：监控》 《DevOps系列：SRE》 ","date":"2020-07-15","objectID":"/devops_series_cicd/:1:0","tags":["devops"],"title":"DevOps系列：CI/CD","uri":"/devops_series_cicd/"},{"categories":["DevOps"],"content":"1. 介绍 CI/CD 是一种通过在应用开发阶段引入自动化来频繁向客户交付应用的方法。CI/CD 的核心概念是持续集成、持续交付和持续部署。作为一个面向开发和运营团队的解决方案，CI/CD 主要针对在集成新代码时所引发的问题（亦称：“集成地狱”）。 具体而言，CI/CD 可让持续自动化和持续监控贯穿于应用的整个生命周期（从集成和测试阶段，到交付和部署）。这些关联的事务通常被统称为“CI/CD 管道”，由开发和运维团队以敏捷方式协同支持。 ","date":"2020-07-15","objectID":"/devops_series_cicd/:2:0","tags":["devops"],"title":"DevOps系列：CI/CD","uri":"/devops_series_cicd/"},{"categories":["DevOps"],"content":"1.1 CI 是什么？CI 和 CD 有什么区别？ 缩略词 CI / CD 具有几个不同的含义。CI/CD 中的“CI”始终指持续集成，它属于开发人员的自动化流程。成功的 CI 意味着应用代码的新更改会定期构建、测试并合并到共享存储库中。该解决方案可以解决在一次开发中有太多应用分支，从而导致相互冲突的问题。 CI/CD 中的“CD”指的是持续交付和/或持续部署，这些相关概念有时会交叉使用。两者都事关管道后续阶段的自动化，但它们有时也会单独使用，用于说明自动化程度。 持续交付通常是指开发人员对应用的更改会自动进行错误测试并上传到存储库（如 GitHub 或容器注册表），然后由运维团队将其部署到实时生产环境中。这旨在解决开发和运维团队之间可见性及沟通较差的问题。因此，持续交付的目的就是确保尽可能减少部署新代码时所需的工作量。 持续部署（另一种“CD”）指的是自动将开发人员的更改从存储库发布到生产环境，以供客户使用。它主要为了解决因手动流程降低应用交付速度，从而使运维团队超负荷的问题。持续部署以持续交付的优势为根基，实现了管道后续阶段的自动化。 CI/CD 既可能仅指持续集成和持续交付构成的关联环节，也可以指持续集成、持续交付和持续部署这三项构成的关联环节。更为复杂的是，有时“持续交付”也包含了持续部署流程。 归根结底，我们没必要纠结于这些语义，您只需记得 CI/CD 其实就是一个流程（通常形象地表述为管道），用于实现应用开发中的高度持续自动化和持续监控。因案例而异，该术语的具体含义取决于 CI/CD 管道的自动化程度。许多企业最开始先添加 CI，然后逐步实现交付和部署的自动化（例如作为云原生应用的一部分）。 ","date":"2020-07-15","objectID":"/devops_series_cicd/:2:1","tags":["devops"],"title":"DevOps系列：CI/CD","uri":"/devops_series_cicd/"},{"categories":["DevOps"],"content":"1.2 CI 持续集成（Continuous Integration） 现代应用开发的目标是让多位开发人员同时处理同一应用的不同功能。但是，如果企业安排在一天内将所有分支源代码合并在一起（称为“合并日”），最终可能造成工作繁琐、耗时，而且需要手动完成。这是因为当一位独立工作的开发人员对应用进行更改时，有可能会与其他开发人员同时进行的更改发生冲突。如果每个开发人员都自定义自己的本地集成开发环境（IDE），而不是让团队就一个基于云的 IDE 达成一致，那么就会让问题更加雪上加霜。 持续集成（CI）可以帮助开发人员更加频繁地（有时甚至每天）将代码更改合并到共享分支或“主干”中。一旦开发人员对应用所做的更改被合并，系统就会通过自动构建应用并运行不同级别的自动化测试（通常是单元测试和集成测试）来验证这些更改，确保这些更改没有对应用造成破坏。这意味着测试内容涵盖了从类和函数到构成整个应用的不同模块。如果自动化测试发现新代码和现有代码之间存在冲突，CI 可以更加轻松地快速修复这些错误。 进一步了解技术细节 ","date":"2020-07-15","objectID":"/devops_series_cicd/:2:2","tags":["devops"],"title":"DevOps系列：CI/CD","uri":"/devops_series_cicd/"},{"categories":["DevOps"],"content":"1.3 CD 持续交付（Continuous Delivery） 完成 CI 中构建及单元测试和集成测试的自动化流程后，持续交付可自动将已验证的代码发布到存储库。为了实现高效的持续交付流程，务必要确保 CI 已内置于开发管道。持续交付的目标是拥有一个可随时部署到生产环境的代码库。 在持续交付中，每个阶段（从代码更改的合并，到生产就绪型构建版本的交付）都涉及测试自动化和代码发布自动化。在流程结束时，运维团队可以快速、轻松地将应用部署到生产环境中。 ","date":"2020-07-15","objectID":"/devops_series_cicd/:2:3","tags":["devops"],"title":"DevOps系列：CI/CD","uri":"/devops_series_cicd/"},{"categories":["DevOps"],"content":"1.4 CD 持续部署（Continuous Deployment） 对于一个成熟的 CI/CD 管道来说，最后的阶段是持续部署。作为持续交付——自动将生产就绪型构建版本发布到代码存储库——的延伸，持续部署可以自动将应用发布到生产环境。由于在生产之前的管道阶段没有手动门控，因此持续部署在很大程度上都得依赖精心设计的测试自动化。 实际上，持续部署意味着开发人员对应用的更改在编写后的几分钟内就能生效（假设它通过了自动化测试）。这更加便于持续接收和整合用户反馈。总而言之，所有这些 CI/CD 的关联步骤都有助于降低应用的部署风险，因此更便于以小件的方式（而非一次性）发布对应用的更改。不过，由于还需要编写自动化测试以适应 CI/CD 管道中的各种测试和发布阶段，因此前期投资还是会很大。 ","date":"2020-07-15","objectID":"/devops_series_cicd/:2:4","tags":["devops"],"title":"DevOps系列：CI/CD","uri":"/devops_series_cicd/"},{"categories":["DevOps"],"content":"2. 构建CI/CD https://www.infoq.cn/article/wht0wfmdrrbu-dtkh1xp https://gitbook.cn/gitchat/column/5aa795539c3cf94d49162f03/topic/5aaa1abe0bb9e857450e7a9e ","date":"2020-07-15","objectID":"/devops_series_cicd/:3:0","tags":["devops"],"title":"DevOps系列：CI/CD","uri":"/devops_series_cicd/"},{"categories":["DevOps"],"content":"CI/CD 与 DevOps 学习笔记 DevOps 是为了应对软件发布频率越来越高与传统的“瀑布型”开发模式之间的矛盾提出的，CI/CD 是 Devops 的基础。CI/CD 将 Dev、QA、Ops 连接在一起，就是所谓的“开发运维一体化”，通过高度自动化的流程保证频繁且可靠的发布。 CI/CD 基本流程 从开发提交代码，到上线，整个过程都是属于 CI/CD 的范畴。 CI 意思是 持续构建，指的是从提交代码到最终生成的可以发布的应用，应用可以是 jar 包，也可以指 tar 包，甚至是 docker 镜像。 CI 的基本流程包括，提交代码、对代码的监控、编译打包、基本测试，发布包。 CD 意思是 持续交付/持续部署，持续部署比持续交付多强调了一点，即deploy的自动化。两者的区别实践起来并不重要，主要流程指将持续构建的产物，快速部署到测试环境进行测试，测试完毕后进行评审后即可自动化发布、部署到现网。 CI 具体分析 CI——Confinuous Integration，持续集成。 集成的意思是，将开发者的代码迅速集成到整个代码项目中，与历史的代码、他人的代码集成。传统的开发模式中，一个项目，分成几部分，大家分别开发，最后发布前，将所有的代码合到一起然后进行迭代测试直到发布。在 CI 流程中，每个人随时将自己的代码合入到主干，并且合入能够很快得到反馈，这种反馈不止是基于修改部分的代码，而是基于整个代码仓库的测试。这种模式下，问题可以被更早的发现，尤其是那些在集成测试中容易出现的问题。 持续的意思，可以理解为，强调的是整个过程是持续不断的。提交代码 -\u003e 监控到代码提交并触发测试（也可能是定时的，比如每天一次）-\u003e反馈结果到开发，整个过程持续贯穿在代码开发的整个流程中。 CI 流程中的测试，一般不同于 CD 流程中的测试。在 CI 流程中，比如在提交代码阶段，一般会有 commit hook，对提交的代码进行基本的静态检查，代码提交之后，一般会有产物的构建流程。在测试中，常常包含的是基本的自动化测试、单元测试以及基本的集成测试。 但是这些也取决于具体的场景的决策，这种决策需要兼顾的是两个方面： 时效性，在 CI 阶段的测试一般来讲应该是可以比较快的反馈到开发者的，否则就会降低开发的效率 问题出现的可能性，应该思考在 CI 阶段比较容易出现、常出现的问题是哪些，这些问题可以尽量放到该阶段，但是一般是一些基础的测试，复杂的测试可以放到 CD 中去做。 CD 具体分析 CD 的输入是 CI 的输出，也叫 CI 的产物（Artifact），它可能是 jar 包、war包、tar包、image 等等，CD 流程从归档地址（Artifact Repository） 获取产物。 CD 的主要过程是部署产出物到测试环境，进行集成的全面的测试，并且最终得到一个通过了所有测试的、可以被评审发布的产物。 在 CD 的测试中，尽管各个厂商叫法不同，但是可以将其基本分为三种环境，分别是测试环境、预发环境、生产环境。团队可以根据自身实际情况，定义三个环境的部署基准。 在 CD 流程中的测试应该是全面覆盖的，包括但不限于单元测试、接口自动化测试、集成自动化测试、UI测试、性能压测以及整个的端到端的测试。 经过了所有的测试的发布包，经过评审发布，就可以部署到现网，如果部署到现网也做到自动化，这个就叫 Continuous Deployment，也就是 持续部署。 持续部署必须拥有回滚的能力。 CI/CD 工具 CI/CD 整体是自动化的，同时由于多个任务并行/串行构成了流水线，Jenkins 提供的就是自动化流水线的能力以及丰富的插件的能力。 Jenkins 基本是事实上的标准，容器化目前可以作为补充来完善或者加速整个流程。但是同时也有一些容器化的工具开始挑战 Jenkins，比如 Drone。 Jenkins 主要在于它强大的生态，几乎可以搞定整个 CI/CD 流程的各个方面。Jenkins 可以安装很多插件，必要时写一个插件也不算太难——比如通过插件可以监控代码仓库的提交。 Jenkins 1.0 中开发者基本就是依靠 shell 脚本来构建一条流水线，在 Jenkins 2.0 中有了 pipeline as code，从而可以将发布的流程也纳入到版本管理中，可以很方便的复制迁移工程。 除了 Jenkins 之外，也可以使用 gitlab 的 CI/CD 流程工具，学会了使用其中一个，另一个也就比较容易上手了，比如知道了如何在 Jenkins 中添加 slave，那么自然就知道 gitlab 中添加 Runner 是干啥的了。gitlab 对代码的托管的集成肯定是更好的，但是 Jenkins 的源码管理插件也已经足够好用了。 Docker 提供了另外一种产物的可能，比如之前发布的包是 jar 包，使用了 docker 之后，可以发布一个 docker 镜像。将产物从打包好了的代码变成处理好了的镜像，就可以解决“测试环境明明运行的好好的，现网却挂了”这种环境导致的问题。 当然，也可以在其它具体的任务场景下使用 docker，比如使用 docker 作为构建机器，好处依旧是配置更加简单，启动停止销毁也变得容易。 另外，kubernates 也可以在 CI/CD 中发挥巨大的作用，比如使用 Jenkins 对接 Kubernates 集群，可以完成 master 挂掉立即重启一个 master的操作，也可以 解决slave 需求峰值低谷差异大造成资源浪费的问题。 但是比较明显的是，容器化能做的远远不止这些，未来的 CI/CD 会跟容器化会联系地更加紧密，几乎是毫无疑问的事情。 总结 CI/CD 可以认为是 DevOps 工作流的基础，它完成了应用的快速且高质量的发布。 持续集成 完成了提交代码到产出物的快速迭代。 持续交付 完成了整个测试流程的快速迭代。 持续部署 自动化了生产环境的部署，并且定义了灰度发布、回滚等能力及策略。 持续集成和持续交付的分界点是 Artiface Repository 中的产出物。 持续交付和持续部署的分界点是评审后的 Release 的产出物。 整个流程是一直在不断 running 的，因此叫做“持续”。 构建CICD工具链 源码管理： git、svn CI 平台：Jenkins、gitlab ci 代码静态分析工具：比如 pylint 单元测试：开发者应该写单元测试 集成测试：集成自动化用例 测试环境：最好可以自动化，支持多版本部署验证 软件发布平台 全覆盖测试工具，UI测试工具、压测工具等 预发环境 自动化部署工具 资料及扩展 阿里巴巴如何基于 Kubernetes 实践 CI/CD Jenkins 用户手册 Getting started with Gitlab CI/CD CICD 方案分析 基于 Jenkins 和 Docker 搭建持续交付流水线 持续集成与交付解决方案 基于 Docker 的 CI/CD 流水线实践 解读阿里 CI/CD、DevOps、分层自动化技术 ","date":"2020-07-15","objectID":"/devops_series_cicd/:4:0","tags":["devops"],"title":"DevOps系列：CI/CD","uri":"/devops_series_cicd/"},{"categories":["DevOps"],"content":"2.2 工具 https://jenkins-zh.cn/wechat/articles/2020/01/2020-01-10-the-complete-ci-cd-collection-tutorials/ ","date":"2020-07-15","objectID":"/devops_series_cicd/:4:1","tags":["devops"],"title":"DevOps系列：CI/CD","uri":"/devops_series_cicd/"},{"categories":["DevOps"],"content":"3. 结论 ","date":"2020-07-15","objectID":"/devops_series_cicd/:5:0","tags":["devops"],"title":"DevOps系列：CI/CD","uri":"/devops_series_cicd/"},{"categories":["DevOps"],"content":"CMDB","date":"2020-07-10","objectID":"/devops_series_cmdb/","tags":["devops"],"title":"DevOps系列：CMDB","uri":"/devops_series_cmdb/"},{"categories":["DevOps"],"content":"系列目录 《DevOps系列：开篇》 《DevOps系列：概述》 《DevOps系列：CMDB》 《DevOps系列：CI/CD》 《DevOps系列：监控》 《DevOps系列：SRE》 ","date":"2020-07-10","objectID":"/devops_series_cmdb/:1:0","tags":["devops"],"title":"DevOps系列：CMDB","uri":"/devops_series_cmdb/"},{"categories":["DevOps"],"content":"1. 介绍 CMDB是组织使用的ITIL术语，用于组织存储有关硬件和软件资产的信息（通常称为配置项[CI]）。CMDB提供了一种了解组织的关键资产及其关系的方法，例如信息系统，资产的上游来源或依存关系以及资产的下游目标。 用比较通俗的语言解释，CMDB可以存储并自动发现整个IT网络上的各种信息，比如一个IT网络上有多少台服务器、多少存储、设备的品牌、资产编号、维护人员、所属部门、服务器上运营什么操作系统、操作系统的版本、操作系统上有哪些应用、每个应用的版本等等，此外，CMDB还有一个非常重要的功能，即存储不同资源之间的依赖关系，如果网络上某个节点出现问题，通过CMDB，可以判断因此受到影响的业务。 ","date":"2020-07-10","objectID":"/devops_series_cmdb/:2:0","tags":["devops"],"title":"DevOps系列：CMDB","uri":"/devops_series_cmdb/"},{"categories":["DevOps"],"content":"2. 云原生时代的CMDB设计 ","date":"2020-07-10","objectID":"/devops_series_cmdb/:3:0","tags":["devops"],"title":"DevOps系列：CMDB","uri":"/devops_series_cmdb/"},{"categories":["DevOps"],"content":"2.1 传统时代的CMDB 通常，我们在建设运维的基础管理平台时，通常要做的事情： 1、把服务器、网络、IDC、机柜、存储、配件等这几大维度先定下来； 2、这些硬件的属性确定下来，比如服务器就会有 SN 序列号、IP 地址、厂商、硬件配置（如 CPU、内存、硬盘、网卡、PCIE、BIOS）、维保信息等等；网络设备如交换机也会有厂商、型号、带宽等等； 3、以上信息之间的关联关系，或者叫拓扑关系。比如服务器所在机柜，虚拟机所在的宿主机、机柜所在 IDC 等简单关系，复杂一点就会有核心交换机、汇聚交换机、接入交换机以及机柜和服务器之间的级联关系，这个就相对复杂一些 4、其实应该是 3.5 步，在上面信息的梳理过程中肯定就会遇到一些规划问题，比如，IP 地址段的规划，xx 网段用于 DB，xx 网段用于大数据、xx 网段用于业务应用等等，再比如同步要做的还有 xx 机柜用于做虚拟化宿主机、xx 机柜只放 DB 机器等等。 以上信息梳理清楚，通过 ER 建模工具进行数据建模，再将以上的信息固化到 DB 中，一个资源层面的信息管理平台就基本成型了。但是，信息固化不是目的，也没有价值，只有信息动态流转起来才有价值（跟货币一样）。接下来我们可以做的事情： 1、基于这些信息进行流程规范的建设，比如服务器的上线、下线、维修、装机等流程。同时，流程过程中状态的变更要同步管理起来。 2、拓扑关系的可视化和动态展示，比如交换机与服务器之间的级联关系、状态（正常 or 故障）的展示等，这样可以很直观的关注到资源节点的状态。 至此，从资源维度的信息梳理，以及基于这些信息的平台和流程规范建设也算是基本成型了。这个时候，以服务器简单示例，我们的视角是下面这样的： ","date":"2020-07-10","objectID":"/devops_series_cmdb/:3:1","tags":["devops"],"title":"DevOps系列：CMDB","uri":"/devops_series_cmdb/"},{"categories":["DevOps"],"content":"2.2 云时代的CMDB 云和k8s的出现以及DevOps的普及，开始了以应用为中心的CMDB构建 上面说明了 CMDB 的基础信息部分，如果从传统的 SA 运维模式，这些信息已经足够，但是从应用运维的角度，这些就远远不够了。这时我们就需要一个非常非常重要的句柄——应用名，*或者叫应用标示。这时，应用运维里面最最重要的一条联系也就产生了——*“应用名—IP“的关联关系。（注：这里也可以是定义的其它的唯一主机标示，如主机名、容器 ID 等等，因为我们使用的方式是 IP，所以这里就以 IP 示例） 之所以说应用名和应用名-IP 关联关系非常重要，是因为它的影响力不仅仅在运维内部，而是会一直延伸整个技术架构上，后面的文章，我们介绍到的所有的平台和系统建设，都会跟这两个概念有关。 CMDB 是 IP 为标示的资源管理维度，有了应用名之后，我们后面就是以应用为视角的管理维度了。首先看一下应用会涉及到的信息： 1、应用基础信息，如应用责任人、应用的 Git 地址等 2、应用部署涉及的基础软件包，如语言包（Java、C++、GO 等）、Web 容器（Tomcat、JBoss 等）、Web 服务器（Apache、Nginx 等）、基础组件（各种 agent，如日志、监控、系统维护类的 tsar 等） 3、应用部署涉及的目录，如运维脚本目录、日志目录、应用包目录、临时目录等 4、应用运行涉及的各项脚本和命令，如启停脚本、健康监测脚本 5、应用运行时的参数配置，如 Java 的 jvm 参数，特别重要的是 GC 方式、新生代、老生代、永生代的堆内存大小配置等 6、应用运行的端口号 7、应用日志的输出规范 我们梳理完上述信息后就会发现，这些信息跟 CMDB 里面的资源信息是完全两个维度的东西，所以从信息管理维度上讲，资源配置和应用配置分开会更清晰，解耦之后也更容易管理。 **好了，按照上面 CMDB 说的套路，梳理完成后，就是要进行信息的建模和数据的固化，这时就有了我们的——应用配置管理。**再往后，就是基于应用配置管理的流程规范和工具平台的建设，这就涉及到我们经常说的持续集成 \u0026发布 \u0026交付，监控、稳定性平台、成本管理等等。 从应用的视角，我们配置管理，应该是下面这样一个视图（简单示例，不是完整的）： ","date":"2020-07-10","objectID":"/devops_series_cmdb/:3:2","tags":["devops"],"title":"DevOps系列：CMDB","uri":"/devops_series_cmdb/"},{"categories":["DevOps"],"content":"3. 构建以应用为中心的CMDB https://cloud.tencent.com/developer/news/675404 ","date":"2020-07-10","objectID":"/devops_series_cmdb/:4:0","tags":["devops"],"title":"DevOps系列：CMDB","uri":"/devops_series_cmdb/"},{"categories":["DevOps"],"content":"3. 结论 CMDB 是运维的基石，但是要发挥更大的价值，光有基础是不够的，我们要把更多的精力放到上层的应用和价值服务上，所以我才会讲应用才是运维的核心。我们可以看到，如果仅仅基于 CMDB 的资源信息作自动化，最多只能做出自动化的硬件资源采集、自动化装机、网络-硬件拓扑关系生成等资源层面的工具，这些工具只会在运维层面产生价值，离业务还很远，就更谈不上能给业务带来什么价值了。但是基于应用这一层去做，就可以做很多事情，比如持续集成和发布、持续交付、弹性扩缩容、稳定性平台、成本控制等等，这些事情，带来的价值就会大大不同，这些后续会一个个介绍出来。 ","date":"2020-07-10","objectID":"/devops_series_cmdb/:5:0","tags":["devops"],"title":"DevOps系列：CMDB","uri":"/devops_series_cmdb/"},{"categories":["DevOps"],"content":"概述","date":"2020-07-05","objectID":"/devops_series_intro/","tags":["devops"],"title":"DevOps系列：概述","uri":"/devops_series_intro/"},{"categories":["DevOps"],"content":"系列目录 《DevOps系列：开篇》 《DevOps系列：概述》 《DevOps系列：CMDB》 《DevOps系列：CI/CD》 《DevOps系列：监控》 《DevOps系列：SRE》 ","date":"2020-07-05","objectID":"/devops_series_intro/:1:0","tags":["devops"],"title":"DevOps系列：概述","uri":"/devops_series_intro/"},{"categories":["DevOps"],"content":"1. 介绍 DevOps大概起源于08到09年之间，最初的目的是要打破开发与运维之间的壁垒，2010年，The Agile Admin博客发布了《What is DevOps》给出了一个详细的DevOps定义，算是对DevOps有了一个初步的定义。不过，我这里还是引用一下wiki的定义： DevOps（开发 Development 与运维 Operations 的组合词）是一种文化、一场运动或实践，强调在自动化软件交付流程及基础设施变更过程中，软件开发人员与其他信息技术（IT）专业人员彼此之间的协作与沟通。它旨在建立一种文化与环境，使构建、测试、软件发布得以快速、频繁以及更加稳定地进行。 ","date":"2020-07-05","objectID":"/devops_series_intro/:2:0","tags":["devops"],"title":"DevOps系列：概述","uri":"/devops_series_intro/"},{"categories":["DevOps"],"content":"2. 优势 传统的瀑布模型会带来很多沟通上的问题和成本，DevOps实际上是要来解决这些问题的。我们分别从工程上和角色上的优势来说一说 ","date":"2020-07-05","objectID":"/devops_series_intro/:3:0","tags":["devops"],"title":"DevOps系列：概述","uri":"/devops_series_intro/"},{"categories":["DevOps"],"content":"2.1 基于DevOps的工程上的优势 DevOps的主要好处是可以更快地交付质量大大提高的软件。 根据行业的不同，可能还会有其他好处。大部分软件工程采用DevOps具有如下优势： 提升了可靠性 更快速的软件更新 减少故障恢复时间 更好的用户体验 更加高效 减少失败 降低风险 更好的质量 更短的开发周期 提升产品交付时间 提升稳定性 节省成本 ","date":"2020-07-05","objectID":"/devops_series_intro/:3:1","tags":["devops"],"title":"DevOps系列：概述","uri":"/devops_series_intro/"},{"categories":["DevOps"],"content":"2.2 基于DevOps的角色上的优势 2.2.1 不同角色的痛苦 在IT角色中，大部分的痛苦都来自于缺乏沟通和无聊的重复工作。 DevOps旨在解决这些问题。 2.2.2 优势 如果采用DevOps后，不同的角色能获得不同的好处： 开发 在没有采用DevOps之前，开发人员可能需要一遍又一遍的完成类似构建和部署的相同的任务。非常消耗时间。 借助DevOps和自动化，可以消除繁琐的重复任务！将这些耗时的项目排除在外，这样就有更多的时间进行开发。 运维 软件开发完成后交由运维人员进行发布维护，但是因为缺少沟通，运维人员不清楚具体的功能和变更，当出现问题时，解决问题将花费更长的时间。也因为这个情况，为了保持环境的稳定性，变更变得小心翼翼。 使用DevOps后，自动化和持续集成允许在不威胁稳定性的情况下交付新功能。这样运维人员报告的计划外的工作和返工时间减少了22%。这主要是因为运维人员与开发人员的交流增加了。 更好的代码、共享的代码库和更稳定的线上环境使工作更加轻松。 产品 当产品和应用需要更长的时间才能投入生产环境时，对于产品经理来说令人难以接受。特别是在软件有错误的时候，时间会更加的长。 而DevOps鼓励协作环境，通过不断的沟通交流，以及小步快跑的方式，软件部署频率提高了46倍，变更准备的时间缩短了440倍！ 系统管理员 当软件有错误，但是反馈不及时以及可见性低时，系统管理员就很难进行工作。 而Devops鼓励沟通，沟通可以带来更好的产品和更好的系统，这样可以让管理更简单，并且自动化可以减少人为错误，从而将故障发生的概率降低3倍。 DevOps还可以提高整个软件开发过程的可见性。当能够检测到错误，找到错误的根源并找到原因时，解决问题就变得容易。 DevOps可使故障恢复速度提高96倍。 测试 当测试人员看不到问题的源头和愿意时，就很令人沮丧。 DevOps可以更快地解决问题。提高可见度和沟通对于解决问题至关重要。工程师可以使用实时数据来解决问题并了解应用程序更改的影响。当出现问题时，解决方案实施越早越好。如果错误太深，则将很难修复。 质量保障 确保产品和系统完好无损是质量保障的工作。但这并不意味着大家会喜欢充满错误的软件和流程。 借助DevOps，团队成员可以共同开发出更好的产品，并且自动化可以减少容易避免的人为错误。这样可以让错误更少。在存在错误的地方，由于持续集成和持续交付以及它们的频繁微小更改，它们更小而且更容易被修复。DevOps用户报告的修复安全问题的时间减少了50％，故障恢复速度提高了96倍。 客服 曾经在服务行业工作过的任何人，无论是在餐馆，零售店还是客户服务部门，都知道与心怀不满的客户打交道的痛苦。当您的系统出现故障和错误时，客户也会不满意。 DevOps减少了错误，这意味着更快乐的客户。客服仍然会收到有投诉的客户打来的电话，但是他们之间的联系可能会越来越少。另外，与他们反复遇到相同的问题相比，他们将更加了解。更具协作性的环境意味着您的工作更加轻松。 用户 产品变更的意义是什么？当然是为了改善用户体验。因为我们简化了流程，这就意味着我们将有更多时间为客户和客户进行更多改进。 DevOps通过改进流程和应用程序使最终用户的体验更加一致。总体而言，使交互更加有趣。 ","date":"2020-07-05","objectID":"/devops_series_intro/:3:2","tags":["devops"],"title":"DevOps系列：概述","uri":"/devops_series_intro/"},{"categories":["DevOps"],"content":"3. 如何实施？ 要让 DevOps 切实有效，您必须首先建立一套开发人员与运营部门通力协作的文化和理念。这对于该策略能否取得成功至关重要。该策略使得两个团队之间能够进行更好的沟通，从而激发创新。在打破了局限性的组织中，您可以建立一个集成环境。在该环境中，您能够反复测试并改进软件代码，然后实施一套连续发布计划以部署经过增强的软件。 您可以快速交付高质量的新产品和服务，因此客户满意度和用户体验将得到提升。通过使用内置了机器学习和算法的工具来执行连续监控和响应，任务（工作流程）可以自动触发，无需人工干预。 通过收集客户反馈和分析，您可以快速将这些信息融入到您的业务规划和未来产品开发中。然后，您可以从头开始再次启动 DevOps 循环。但这一次，您可以借助从客户那里学习并验证的知识来改进协作开发过程，并开始优化。 通过继续遵循 DevOps 方法，公司可以逐步建立一套优化的生态系统，其中包含相互作用的组件、用于优化开发的优秀实践以及用于维持高质量的既定标准。 DevOps的关键在于持续改进并且循环： 持续计划 协作开发 持续测试 持续集成 持续交付 持续反馈 持续监控 持续响应 ","date":"2020-07-05","objectID":"/devops_series_intro/:4:0","tags":["devops"],"title":"DevOps系列：概述","uri":"/devops_series_intro/"},{"categories":["DevOps"],"content":"4. 结论 该篇文章，我们介绍了DevOps的定义，说明了采用DevOps后的优势，以及在大方向上要如何进行DevOps的实施。接来下，我们将从技术层面来说明如何构建DevOps。 参考： https://devops.com/different-organizations-different-devops-outcomes/ https://dzone.com/articles/the-benefits-of-devops-by-role https://www.juniper.net/cn/zh/products-services/what-is/devops/ ","date":"2020-07-05","objectID":"/devops_series_intro/:5:0","tags":["devops"],"title":"DevOps系列：概述","uri":"/devops_series_intro/"},{"categories":["DevOps"],"content":"开篇","date":"2020-07-01","objectID":"/devops_series/","tags":["devops"],"title":"DevOps系列：开篇","uri":"/devops_series/"},{"categories":["DevOps"],"content":"系列目录 《DevOps系列：开篇》 《DevOps系列：概述》 《DevOps系列：CMDB》 《DevOps系列：CI/CD》 《DevOps系列：监控》 《DevOps系列：SRE》 ","date":"2020-07-01","objectID":"/devops_series/:1:0","tags":["devops"],"title":"DevOps系列：开篇","uri":"/devops_series/"},{"categories":["DevOps"],"content":"1. 介绍 在Ops领域工作也接近8年了，个人也经历了从人工Ops–\u003e工具Ops–\u003e平台Ops的转变，最近想把自己的一些DevOps相关的学习和经验记录下来，整理成一个系列，算是对自己这么多年的工作经验的一个总结，也想对之后的发展想法做下自己的规划和判断。 ","date":"2020-07-01","objectID":"/devops_series/:2:0","tags":["devops"],"title":"DevOps系列：开篇","uri":"/devops_series/"},{"categories":["DevOps"],"content":"2. 当我们谈论Ops，我们在谈论什么 ","date":"2020-07-01","objectID":"/devops_series/:3:0","tags":["devops"],"title":"DevOps系列：开篇","uri":"/devops_series/"},{"categories":["DevOps"],"content":"2.1 从开发模型说起 早期的软件开发模型一般采用瀑布式开发模型，该模型将软件过程划分成几个阶段，从需求到设计、开发、测试和运维，它的理念是软件开发的规模越来越大，必须以一种工程管理的方式来定义每个阶段，以及相应的交付产物和交付标准，以期通过一种重流程，重管控，按照计划一步步推进整个项目的交付过程。Ops处于软件交付的末端。那么，具体的Ops是什么呢？ ","date":"2020-07-01","objectID":"/devops_series/:3:1","tags":["devops"],"title":"DevOps系列：开篇","uri":"/devops_series/"},{"categories":["DevOps"],"content":"2.2 什么是运维(Ops) 运维(Ops)， 通常指IT运维（IT Operations）， 是指通过一系列步骤和方法，管理与维护线上服务（Online Service）或者产品 （Product）的过程。 运维有着非常广泛的定义，在不同的公司不同的阶段代表不同的职责与定位，没有一个统一的标准。尤其是随着互联网的发展，运维的含义也在逐渐互联网化。互联网运维通常属于技术部门，与研发、测试、系统管理同为互联网产品的技术支撑，这种划分在国内和国外以及大小公司之间都会多少有一些不同。运维的重点在于系统运行的各种环境，从机房、网络、存储、物理机、虚拟机这些基础的架构，到数据库、中间件平台、云平台、大数据平台，偏重的也不是编程，而是对这类平台的使用和管理。运维的水平可以成为衡量一个公司（IT公司）技术实力的标准。随着软件行业和规模的不断发展，Ops也在不断的改进。 ","date":"2020-07-01","objectID":"/devops_series/:3:2","tags":["devops"],"title":"DevOps系列：开篇","uri":"/devops_series/"},{"categories":["DevOps"],"content":"2.3 Ops的发展历程 2.3.1 人工阶段 在这个阶段，所有运维问题，基本靠人工操作完成。这种情况下，系统规模不大，遇到的问题相对简单，大多集中在硬件、网络和系统层面，所以有一定操作系统或网络维护经验的人就可以搞定。 2.3.2 脚本\u0026工具阶段 一般绝大多数企业都会很快从第一阶段过渡到第二阶段，因为上一阶段的大量重复繁琐的操作，完全可以转化为脚本来实现，而不是每次都去敲一堆类似的命令。 早期的运维主要以各种 shell 为主，所以很多运维如果会 shell 编写一些批处理脚本，就会很有竞争力了。再往后，我们大家所熟知的 Perl、Ruby、Python 等动态语言也被广泛应用于脚本工具的实现，特别是一些逻辑和场景相对复杂的自动化实现。不同的工具也被开发出现，例如ansible、chef、puppet等。 2.3.3 流程\u0026工具阶段 在该阶段，要面临更加复杂化的场景实现，比如做一次业务部署，运维同学可能要安装服务器，做系统配置变更，安装软件包、启停进程，然后再负载均衡上配置服务等等。这时，就需要有一个流程将一个个的脚本功能串联起来，同时还要有一些脚本执行结果校验及判断的过程。 所以，这就对流程和工具平台有了更大的诉求。同时，在一些 IT 化比较早的行业，如电信运营商和金融行业，由于对变更过程的严格控制，这就需要更加科学和规范的管理措施，所以会引入 ITIL 这样 IT 服务管理体系，对整个 IT 系统及其变更进行管控。 2.3.4 运维平台阶段(DevOps) 从该阶段开始，随着业务复杂度和体量的增加，为了完成企业对于效率、稳定和成本的要求，倒逼着整个业务和技术架构发生转变，例如服务化和分布式等技术，而在这样新的技术体系下，运维所面临的场景复杂度也急剧上升，原有的运维技能如操作系统维护、系统配置、脚本编写已经完全满足不了要求。同时，由于软件系统复杂度的提升，也需要运维投入更多的精力去关注业务软件架构和应用服务上。也正是在这种要求下，运维人员不再是运维工作唯一参与的角色，每个在软件交付过程中需要参与的角色都将参与到DevOps中来，用于提升效率。 2.3.5 智能运维阶段(AIOps) 个人理解这个是未来阶段，需要大厂来完成，因为从 AI 的角度，AIOps 有三个方面的充要条件：机器学习算法、计算能力如 GPU、海量数据。那么海量数据这个条件基本是只有大厂才能具备了。 ","date":"2020-07-01","objectID":"/devops_series/:3:3","tags":["devops"],"title":"DevOps系列：开篇","uri":"/devops_series/"},{"categories":["DevOps"],"content":"3. DevOps ","date":"2020-07-01","objectID":"/devops_series/:4:0","tags":["devops"],"title":"DevOps系列：开篇","uri":"/devops_series/"},{"categories":["DevOps"],"content":"3.1 从开发模型的转变说起 3.1.1 瀑布式开发模型 随着市场环境和用户需求变化的不断加速，瀑布式开发模式这种按部就班的方式有一个严重的潜在问题。 软件开发活动需要在项目一开始就确定项目目标、范围以及实现方式，而这个时间点往往是我们对用户和市场环境信息了解最少的时候，这样做出来的决策往往带有很大的不确定性，很容易导致项目范围不断变更，计划不断延期，交付上线时间不断推后，最后的结果是，即便我们投入了大量资源，却难以达到预期的效果。 从业界巨头 IBM 的统计数字来看，有 34% 的新 IT 项目延期交付，将近一半的应用系统因为缺陷导致线上回滚，这是一件多么令人沮丧的事情。 3.1.2 敏捷式开发模型 基于这种问题，敏捷的思潮开始盛行。它的核心理念是，既然我们无法充分了解用户的真实需求是怎样的，那么不如将一个大的目标不断拆解，把它变成一个个可交付的小目标，然后通过不断迭代，以小步快跑的方式持续开发。 与此同时，将测试工作从研发末端的一个独立环节注入整个开发活动中，对开发交付的内容进行持续验证，保证每次可交付的都是一个可用的功能集合，并且由于质量内建在研发环节中，交付功能的质量也是有保障的。 说到底，敏捷源于开发实践，敏捷的应用使得开发和测试团队抱团取暖。可是问题又来了，开发和测试团队发现，不管研发的速度变得多快，在软件交付的另一端，始终有一群人在冷冰冰地看着他们，一句“现在没到发布窗口”让多少新开发的功能倒在了上线的门槛上。 毕竟，无论开发了多少天才的功能，如果没有经过运维环节的部署上线，并最终发布给真实用户，那么这些功能其实并没有什么用。 3.1.3 DevOps模型 在传统模式下，度量开发团队效率的途径就是看开发完成了多少需求。于是，开发为了达成绩效目标，当然也是为了满足业务需求，不断地堆砌新功能，却很少有时间认真思考这些功能的可运维性和可测试性，只要需求状态流转到开发完成就万事大吉了。 而对于运维团队而言，他们的考核指标却是系统的稳定性、可用性和安全性。但现代 IT 系统是如此复杂，以至于每一次的上线发布都是一场战役，整个团队如临大敌，上线失败的焦虑始终如影随形。 很多时候，我们并不知道上线之后会发生什么，只能按照部署手册一步步操作，完成之后就听天由命。所以，每逢大促活动，就会有各种“拜服务器教”的照片广为流传。 另一方面，在无数次被开发不靠谱的功能缺陷蹂躏得体无完肤之后，运维团队意识到，变更才是影响他们绩效目标的最大敌人。于是，预先设立的上线窗口就成了运维团队的自留地，不断抬高的上线门槛也使得开发团队的交付变成了不可能完成的任务，最后，“互相伤害”就成了这个故事注定的结局。 因此，DevOps应运而生，也就是说，DevOps 最开始想要打破的就是开发和运维之间的对立和隔阂。DevOps旨在建立一种文化与环境，使构建、测试、软件发布得以快速、频繁以及更加稳定地进行。 虽然DevOps从一开始是想要促进开发和运维的协作，但是慢慢发现，其实在整个软件交付过程中，不仅只有开发和运维，业务也是重要的一环。 比方说，如果业务制定了一个不靠谱的需求，那么无论开发和运维怎样协作，得到的终究是一个不靠谱的结果，以及对人力的浪费。可是业务并不清楚用户的真实情况，于是运维团队慢慢转向运营团队，他们需要持续不断地把线上的真实数据和用户行为及时地反馈给需求团队，来帮助需求团队客观评估需求的价值，并及时作出有利于产品发展的调整，这样一来，业务也被引入到了 DevOps 之中，甚至诞生了 BizDevOps 这样一个专门的词汇。 那么，既然沟通协作放之四海皆准，安全也开始积极地参与进来。安全不再是系统上线发布之后的“定时炸弹”，而是介入到整个软件开发过程中，在每个过程中注入安全反馈机制，来帮助团队在第一时间应对安全风险，那么，对于安全团队来说，DevSecOps 就成了他们眼中的 DevOps。 这样的例子比比皆是，包括职能部门、战略部门等，都纷纷加入其中，使得 DevOps 由最开始的点，扩展为线，再到面，不断发展壮大。每个人都参与其中，这使得 DevOps 成了每一个 IT 从业人员都需要学习和了解的知识和技能体系。 ","date":"2020-07-01","objectID":"/devops_series/:4:1","tags":["devops"],"title":"DevOps系列：开篇","uri":"/devops_series/"},{"categories":["DevOps"],"content":"结论 本篇文章，我们讲述了Ops的相关内容以及发展历程，然后讲到DevOps，我们最终要解决的是软件在企业中的效率、稳定和成本问题。接下来的文章，我们会针对DevOps说一些技术上的实现细节。 参考 [译]当我们谈论Ops时，我们在谈论什么 运维十年回顾：当前很多新技术的本质都是在解决运维问题 DevOps发展历程 ","date":"2020-07-01","objectID":"/devops_series/:5:0","tags":["devops"],"title":"DevOps系列：开篇","uri":"/devops_series/"},{"categories":["Golang"],"content":"并发","date":"2020-04-07","objectID":"/go_series_conc/","tags":["go"],"title":"Go系列：并发","uri":"/go_series_conc/"},{"categories":["Golang"],"content":"系列目录 《Go系列：内存管理》 《Go系列：调度器》 《Go系列：并发》 ","date":"2020-04-07","objectID":"/go_series_conc/:1:0","tags":["go"],"title":"Go系列：并发","uri":"/go_series_conc/"},{"categories":["Golang"],"content":"1. 介绍 ","date":"2020-04-07","objectID":"/go_series_conc/:2:0","tags":["go"],"title":"Go系列：并发","uri":"/go_series_conc/"},{"categories":["Golang"],"content":"2. 面向并发的内存模型 Go内存模型指定了一种条件，在这种条件下，可以保证在一个goroutine中读取变量可以观察到在不同goroutine中写入同一变量所产生的值。 程序如果修改被多个协程同时访问的数据，那么必须串行化这些访问操作。 为了保证串行化访问，可以使用golang的channel操作或者使用sync和sync/atomic包中的同步原语来保护数据。 ","date":"2020-04-07","objectID":"/go_series_conc/:3:0","tags":["go"],"title":"Go系列：并发","uri":"/go_series_conc/"},{"categories":["Golang"],"content":"2.1 Happens Before 在单个goroutine中，读取和写入的行为必须像它们按照程序指定的顺序执行一样。也就是说，仅当重新排序不会改变语言规范所定义的该goroutine中的行为时，编译器和处理器才可以对单个goroutine中执行的读取和写入进行重新排序。由于此重新排序，一个goroutine观察到的执行顺序可能不同于另一个goroutine所察觉到的执行顺序。例如，如果一个goroutine执行a = 1； b = 2；另一个可能会在b的更新值之前观察b的更新值。 为了指定读取和写入的要求，我们定义happens Before，Go程序中执行内存操作的部分顺序。如果事件e1happens before事件e2，那么我们说e2happens aftere1。同样的，如果e1不happens beforee2并且e1也不happens aftere2，那么我们说e1和e2happens concurrently。 在单个goroutine中，事前发生顺序是程序表示的顺序。 如果同时满足以下两个条件，则允许对变量v的读r观察对v的写w： r does not happen before w. There is no other write w' to v that happens after w but before r. 为了保证变量v的读取r观察到对v的特定写入w，请确保w是唯一允许r观察的写入。也就是说，如果同时满足以下两个条件，则保证r遵守w： w happens before r. Any other write to the shared variable v either happens before w or after r. 这对条件比第一个条件强。它要求没有其他写入与w或r同时发生。 在单个goroutine中，没有并发性，因此这两个定义是等效的：read r观察最近写入w到v的值。当多个goroutine访问共享变量v时，它们必须使用同步事件来建立事件发生-在确保读取遵守期望的写入的条件之前。 用v的类型的零值初始化变量v的行为与在内存模型中的写操作相同。 大于单个机器字的值的读取和写入将以未指定的顺序充当多个机器字大小的操作。 ","date":"2020-04-07","objectID":"/go_series_conc/:3:1","tags":["go"],"title":"Go系列：并发","uri":"/go_series_conc/"},{"categories":["Golang"],"content":"2.2 同步 2.2.1 Initialization 程序初始化在单个goroutine中运行，但是该goroutine可能会创建其他同时运行的goroutine。 如果包p导入了包q，则完成q的init函数happen beforep的任何一个的开始。 函数main.main的启动 happen after所有init函数均已完成。 2.2.2 Goroutine creation 开始新goroutine的go语句 happen before该goroutine的执行开始。 2.2.3 Goroutine destruction 无法保证goroutine的退出 happen before 程序中的任何事件 2.2.4 Channel communication channel通信是goroutine之间同步的主要方法。通常在不同的goroutine中，将特定通道上的每个发送与该通道上的相应接收进行匹配。 一个在channel上的发送happen before他从该频道收到的相应信息完成。 channel关闭happen before由于通道关闭，接收返回零值的接收 来自无缓冲channel的接收happen before他在该channel上的发送完成了。 第k次从初始化空间为C的channel的接收happens before第k+C次往channel完成发送。 2.2.5 Locks sync包实现了两种锁数据类型，sync.Mutex和sync.RWMutex。 对于任何sync.Mutex或sync.RWMutex的锁变量l和两个描述次数的n和m（n \u003c m），调用第n次l.Unlock()happens before调用第m次l.Lock()的返回。 对于sync.RWMutex变量l的任意调用l.RLock，l.RLock阻塞直到n次调用l.UnLock，并且n次l.RUnlock happens before 第n+1次调用l.Lock。 2.2.6 Once 同步包为使用一次类型的多个goroutines进行初始化提供了一种安全的机制。多个线程可以执行一次。对于一个特定的f执行Do（f），但是只有一个将运行f（），而另一个调用将阻塞直到f（）返回。 单个从once.Do(f)调用f() happen before任何调用once.Do(f)的返回 ","date":"2020-04-07","objectID":"/go_series_conc/:3:2","tags":["go"],"title":"Go系列：并发","uri":"/go_series_conc/"},{"categories":["Golang"],"content":"调度器","date":"2020-04-07","objectID":"/go_series_scheduler/","tags":["go"],"title":"Go系列：调度器","uri":"/go_series_scheduler/"},{"categories":["Golang"],"content":"系列目录 《Go系列：内存管理》 《Go系列：调度器》 《Go系列：并发》 ","date":"2020-04-07","objectID":"/go_series_scheduler/:1:0","tags":["go"],"title":"Go系列：调度器","uri":"/go_series_scheduler/"},{"categories":["Golang"],"content":"1. 介绍 随着技术的不断发展，CPU也在不断发展，出现了多处理器、多核心、CPU缓存、NUMA架构等概念。为了最大化利用CPU的计算能力，软件也在不断发展，出现了并发和并行等概念。而为了支持并发和并行，就需要调度器，用于处理计算任务在不同CPU上的计算。我们主要从系统调度和语言层面的调度来说明。 ","date":"2020-04-07","objectID":"/go_series_scheduler/:2:0","tags":["go"],"title":"Go系列：调度器","uri":"/go_series_scheduler/"},{"categories":["Golang"],"content":"2. OS Scheduler 我们的的程序只是一系列机器指令，需要依次执行。为此，操作系统使用了线程的概念，线程的工作就是负责说明并按顺序执行分配给它的指令集。执行将不断进行，直到没有更多指令可以执行。 在操作系统上，我们运行的每个程序都会创建一个进程，并且为每个进程分配一个初始线程。线程具有创建更多线程的能力。所有这些不同的线程彼此独立运行，并且调度决策是在线程级别而不是在进程级别做出的。线程可以同时运行(并发，每个任务运行在同一个的core上)，也可以并行运行(并行，每个任务运行在不同core上同时运行)。线程还维护自己的状态，以允许在本地安全和独立地执行指令。 如果存在可以执行的线程时，OS Scheduler负责确保core不处于空闲状态。它还必须产生一种幻想，即所有可以执行的线程正在同时执行。在创建这种幻想的过程中，Scheduler需要优先运行优先级较高的线程，而不是运行优先级较低的线程。但是，具有较低优先级的线程并无法节省执行时间。Scheduler还需要通过做出快速而明智的决策来最大程度地减少调度延迟。 ","date":"2020-04-07","objectID":"/go_series_scheduler/:3:0","tags":["go"],"title":"Go系列：调度器","uri":"/go_series_scheduler/"},{"categories":["Golang"],"content":"2.1 执行指令 程序计数器(PC)有时也称为指令指针(IP)，它使线程可以跟踪要执行的下一条指令。在大多数处理器中，PC指向下一条指令，而不是当前指令。 ","date":"2020-04-07","objectID":"/go_series_scheduler/:3:1","tags":["go"],"title":"Go系列：调度器","uri":"/go_series_scheduler/"},{"categories":["Golang"],"content":"2.2 Thread状态 Waiting，这意味着线程已经停止执行并需要等待某些操作才能继续。这可能是由于诸如等待硬件(磁盘，网络)，操作系统(系统调用)或同步调用(原子，互斥体)之类的原因。这些类型的延迟是导致性能下降的根本原因。 Runnable ，这意味着线程需要获得cpu时间，这样它可以执行其分配的机器指令。如果您有很多需要cpu时间的线程，则线程必须等待更长的时间才能获得cpu时间。而且，随着更多线程争夺cpu时间，任何给定线程获得的cpu时间都将缩短。这种类型的调度延迟也可能是性能下降的原因 Executing，这意味着线程已放置在core上并正在执行其机器指令。与应用程序相关的工作已经完成。这是每个人都想要的状态。 ","date":"2020-04-07","objectID":"/go_series_scheduler/:3:2","tags":["go"],"title":"Go系列：调度器","uri":"/go_series_scheduler/"},{"categories":["Golang"],"content":"2.3 工作类型 CPU-Bound，这项工作永远不会造成线程可能处于等待状态的情况。这是不断进行计算的工作。计算Pi到第N位的线程将是CPU-Bound的。 IO-Bound，这项工作导致线程进入等待状态。这项工作包括请求通过网络访问资源或对操作系统进行系统调用。需要访问数据库的线程将是IO-Bound。我将包括同步事件(互斥量，原子)等导致线程进入等待状态的事件都归为此类。 ","date":"2020-04-07","objectID":"/go_series_scheduler/:3:3","tags":["go"],"title":"Go系列：调度器","uri":"/go_series_scheduler/"},{"categories":["Golang"],"content":"2.4 上下文切换 抢占式调度 首先，这意味着在任何给定时间选择要运行的线程时，调度程序都是不可预测的。线程优先级和事件(例如在网络上接收数据)一起使得无法确定调度程序将选择做什么以及何时执行。 其次，这意味着您绝不能基于自己幸运的经历但不能保证每次都发生的某些感知行为来编写代码。让自己思考很容易，因为我已经看到这种情况以1000次相同的方式发生，这是有保证的行为。如果需要在应用程序中确定性，则必须控制线程的同步和编排。 在内核上交换线程的物理行为称为上下文切换。当调度程序从core中拉出一个excuting线程并将其替换为可runnable线程时，就会发生上下文切换。从运行队列中选择的线程将进入excuting状态。被拉出的线程可以移回runnable状态(如果它仍具有运行能力)或waitting状态(如果由于IO-Bound类型的请求而被替换)。 上下文切换被认为是昂贵的，因为在core上和在core外交换线程都需要时间。上下文切换期间的延迟等待时间量取决于不同的因素，但花费约1000到1500纳秒的时间并非没有道理。虑到硬件应该能够合理地(平均)在每核每纳秒执行12条指令，上下文切换可能会花费大约12000至18k的延迟指令。本质上，您的程序在上下文切换期间将失去执行大量指令的能力。 如果您有一个专注于IO-Bound工作的程序，那么上下文切换将是一个优势。一旦一个线程进入等待状态，另一个处于可运行状态的线程就会代替它。这使核心始终可以工作。这是调度的最重要方面之一。如果有工作要做(线程处于可运行状态)，请不要让core闲置。 如果您的程序专注于CPU-Bound工作，那么上下文切换将成为性能噩梦。由于Thead总是有工作要做，因此上下文切换将阻止该工作的进行。这种情况与IO-Bound工作负载形成鲜明对比。 ","date":"2020-04-07","objectID":"/go_series_scheduler/:3:4","tags":["go"],"title":"Go系列：调度器","uri":"/go_series_scheduler/"},{"categories":["Golang"],"content":"2.5 少即是多 制定调度决策时，scheduler还需要考虑和处理更多的事情。您可以控制在应用程序中使用的线程数。当要考虑的线程更多，并且发生IO-Bound工作时，就会出现更多的混乱和不确定性行为。任务需要更长的时间来计划和执行。 这就是为什么游戏规则是“少即是多”的原因。处于runnable状态的线程越少，意味着获得调度的时间越少，并且每个线程随着时间的流逝会花费更多的时间。更多线程处于runnable状态意味着每个线程随着时间流逝的时间更少。这意味着随着时间的流逝，您完成的工作也更少了。 ","date":"2020-04-07","objectID":"/go_series_scheduler/:3:5","tags":["go"],"title":"Go系列：调度器","uri":"/go_series_scheduler/"},{"categories":["Golang"],"content":"2.6 寻找平衡 您需要在拥有的core数量与为应用程序获得最佳吞吐量所需的线程数量之间找到平衡。在管理这种平衡时，线程池是一个很好的答案。 如果您的服务正在执行许多不同类型的工作该怎么办？这可能会产生不同且不一致的延迟。也许它还会创建许多需要处理的不同的系统级事件。不可能找到一个在所有不同工作负荷下始终有效的魔术数字。当涉及到使用线程池来调整服务的性能时，找到正确的一致配置会变得非常复杂。 ","date":"2020-04-07","objectID":"/go_series_scheduler/:3:6","tags":["go"],"title":"Go系列：调度器","uri":"/go_series_scheduler/"},{"categories":["Golang"],"content":"2.7 CPU缓存 从主存储器访问数据具有很高的延迟成本（〜100至〜300个时钟周期），以致处理器和内核具有本地缓存，以使数据保持在需要它的硬件线程附近。从缓存访问数据的成本要低得多（约3至40个时钟周期），具体取决于要访问的缓存。今天，性能的一个方面是关于如何有效地将数据输入处理器以减少这些数据访问延迟。编写改变状态的多线程应用程序需要考虑缓存系统的机制。 使用cache line在处理器和主存储器之间交换数据。缓存行是在主内存和缓存系统之间交换的64字节内存块。每个内核都会获得所需的任何高速缓存行的副本，这意味着硬件使用值语义。这就是为什么多线程应用程序中的内存突变会造成性能方面的噩梦。 当多个并行运行的线程正在访问同一数据值或什至彼此接近的数据值时，它们将在同一高速缓存行上访问数据。在任何内核上运行的任何线程都将获得该同一缓存行的副本。 如果给定核心上的一个线程更改了其缓存行的副本，则必须借助硬件的魔力，将同一缓存行的所有其他副本标记为脏。当线程尝试对脏的缓存行进行读写访问时，需要主存储器访问（〜100至〜300个时钟周期）才能获取缓存行的新副本. 也许在2核处理器上这没什么大不了，但是如果32核处理器并行运行32个线程，所有访问和变异数据都在同一缓存行上，那又如何呢？带有两个分别具有16个内核的物理处理器的系统又如何呢？由于处理器间通信增加了延迟，因此情况将变得更糟。该应用程序将遍历内存，性能将非常糟糕，并且很可能您将不明白为什么。 这称为高速缓存一致性问题，还引入了错误共享之类的问题。当编写将改变共享状态的多线程应用程序时，必须考虑缓存系统. ","date":"2020-04-07","objectID":"/go_series_scheduler/:3:7","tags":["go"],"title":"Go系列：调度器","uri":"/go_series_scheduler/"},{"categories":["Golang"],"content":"2.8 调度决策方案 想象一下，我要您根据我给您的高级信息编写操作系统调度程序。考虑一下您必须考虑的一种情况。请记住，这是调度程序在做出调度决策时必须考虑的许多有趣的事情之一。 您启动您的应用程序，并创建了主线程并在核心1上执行该线程。随着该线程开始执行其指令，由于需要数据，因此正在检索缓存行。线程现在决定为某些并发处理创建一个新线程。这是问题。 创建线程并准备就绪后，调度程序应： 上下文切换核心1的主线程？这样做可以提高性能，因为此新线程需要与已缓存的数据相同的机会非常好。但是主线程无法获得其全部时间片。 线程是否等待核心1可用，等待主线程的时间片完成？线程未运行，但是一旦开始，将消除获取数据的延迟。 线程是否正在等待下一个可用core？这意味着将清除，检索和复制所选核心的缓存行，从而导致延迟。但是，线程将启动得更快，并且主线程可以完成其时间片。 玩得开心吗？在做出调度决策时，OS调度程序需要考虑这些有趣的问题。幸运的是，对于每个人来说，我都不是一个。我只能告诉您的是，如果有一个空闲的内核，它将被使用。您希望线程可以在运行时运行。 ","date":"2020-04-07","objectID":"/go_series_scheduler/:3:8","tags":["go"],"title":"Go系列：调度器","uri":"/go_series_scheduler/"},{"categories":["Golang"],"content":"3. Go Scheduler ","date":"2020-04-07","objectID":"/go_series_scheduler/:4:0","tags":["go"],"title":"Go系列：调度器","uri":"/go_series_scheduler/"},{"categories":["Golang"],"content":"3.1 进程、线程和协程 进程， 线程， 协程， ","date":"2020-04-07","objectID":"/go_series_scheduler/:4:1","tags":["go"],"title":"Go系列：调度器","uri":"/go_series_scheduler/"},{"categories":["Golang"],"content":"3.2 程序开始 Go程序为主机上标识的每个虚拟core分配了逻辑处理器(P)。如果您的处理器每个物理核心具有多个硬件线程(超线程)，则每个硬件线程将作为虚拟core呈现给您的Go程序。 每个P都分配有一个OS线程(M)。“M\"代表machine。该线程仍由操作系统管理，并且操作系统仍负责将线程放置在内核上执行。 每个Go程序还会获得一个初始Goroutine(G)，这是Go程序的“线程”。Goroutine本质上是一个协程，但是它是Go，因此我们将字母\"C\"替换为\"G”，然后得到单词Goroutine。您可以将Goroutines视为应用程序级线程，并且它们在许多方面类似于OS线程。就像OS线程在上下文中打开和关闭core一样，Goroutine在上下文中打开和关闭M。 最后一个难题是运行队列。Go调度程序中有两个不同的运行队列：全局运行队列(GRQ)和本地运行队列(LRQ)。每个P都有一个LRQ，该LRQ管理分配给在P上下文中执行的Goroutine。这些Goroutine轮流在上下文中切换分配给该P的M。GRQ用于尚未分配给P的Goroutine。有一个将Goroutines从GRQ转移到LRQ的过程，我们将在后面讨论。 ","date":"2020-04-07","objectID":"/go_series_scheduler/:4:2","tags":["go"],"title":"Go系列：调度器","uri":"/go_series_scheduler/"},{"categories":["Golang"],"content":"3.3 协作式调度 Go Scheduler是Go运行时的一部分，并且Go运行时已内置到您的应用程序中。这意味着Go Scheduler在内核上方的用户空间中运行。 Go Scheduler的当前实现不是抢占式调度器，而是协作式调度器。成为协作调度器味着调度器需要在代码的安全点发生的定义明确的用户空间事件，以制定调度决策。 Go协作调度器的出色之处在于它看起来和感觉都是抢先的。您无法预测Go Scheduler将要执行的操作。这是因为该协作调度器的决策权不掌握在开发人员手中，而在于Go运行时。Go Scheduler视为抢占式调度器很重要，并且由于该调度器是不确定的，因此这并不是一件容易的事。 ","date":"2020-04-07","objectID":"/go_series_scheduler/:4:3","tags":["go"],"title":"Go系列：调度器","uri":"/go_series_scheduler/"},{"categories":["Golang"],"content":"3.4 Goroutine状态 Waiting: 这意味着Goroutine已停止并等待某些东西才能继续。这可能是由于诸如等待操作系统(系统调用)或同步调用(原子和互斥操作)之类的原因。这些类型的延迟是导致性能下降的根本原因。 Runnable: 这意味着Goroutine需要获得在M上的执行时间，因此它可以执行其分配的指令。如果您有很多需要M时间的Goroutine，那么Goroutine必须等待更长的时间才能获得时间。而且，随着更多Goroutine争夺时间，任何给定Goroutine所获得的时间都将缩短。这种类型的调度延迟也可能是性能下降的原因。 Executing: 这意味着Goroutine已放置在M上并正在执行其指令。与应用程序相关的工作已经完成。这就是每个人都想要的。 ","date":"2020-04-07","objectID":"/go_series_scheduler/:4:4","tags":["go"],"title":"Go系列：调度器","uri":"/go_series_scheduler/"},{"categories":["Golang"],"content":"3.5 上下文切换 Go Scheduler需要定义明确的用户空间事件，这些事件发生在代码中的安全点处，以便从上下文进行切换。函数调用对于Go调度程序的运行状况至关重要。今天(使用Go 1.11或更低版本)，如果运行任何未进行函数调用的紧密循环，则将导致调度程序和垃圾回收中的延迟。在合理的时间内进行函数调用至关重要。 Go程序中发生了四类事件，这些事件使计划程序可以做出计划决策。这并不意味着它将永远在这些事件之一中发生。这意味着调度器会获得机会。 使用关键字go，关键字go是创建Goroutine的方式。一旦创建了新的Goroutine，它将为调度器提供做出调度决策的机会。 垃圾回收，由于GC使用自己的Goroutine集合运行，因此这些Goroutine需要M上的时间才能运行。这导致GC造成很多调度混乱。但是，调度器对于Goroutine所做的事情非常聪明，它将利用该情报做出明智的决策。这个明智的决定是在GC中将要触摸堆的Goroutine与不触摸堆的Goroutine进行上下文切换。当GC运行时，将制定许多计划决策。 系统调用，如果Goroutine进行了导致Goroutine阻塞M的系统调用，则有时调度器能够将Goroutine上下文切换到M之外，并在上下文中将新Goroutine切换到该M上。但是，有时需要一个新的M来继续执行在P中排队的Goroutine。在下一节中将更详细地说明其工作原理。 同步与编排，如果原子，互斥或channel操作调用将导致Goroutine阻塞，则调度器可以进行上下文切换运行新的Goroutine。一旦Goroutine可以再次运行，就可以对其重新排队，并最终在M上进行上下文切换。 ","date":"2020-04-07","objectID":"/go_series_scheduler/:4:5","tags":["go"],"title":"Go系列：调度器","uri":"/go_series_scheduler/"},{"categories":["Golang"],"content":"3.6 异步系统调用 当您正在运行的OS能够异步处理系统调用时，可以使用称为网络轮询器的东西来更有效地处理系统调用。这是通过在各个操作系统中使用kqueue(MacOS)，epoll(Linux)或iocp(Windows)来完成的。 我们今天使用的许多操作系统都可以异步处理基于网络的系统调用。这是网络轮询器的名称，这是因为它的主要用途是处理网络操作。通过使用网络轮询器进行网络系统调用，调度器可以防止Goroutine在进行这些系统调用时阻止M。这有助于使M保持可用以执行P的LRQ中的其他Goroutine，而无需创建新的M。这有助于减少OS上的调度负载。 例子： 基本调度图，Goroutine-1正在M上执行，并且还有3个Goroutine在LRQ中等待以获取其在M上的时间。网络轮询器闲置无事可做 Goroutine-1希望进行网络系统调用，因此Goroutine-1被移至网络轮询器并处理了异步网络系统调用。将Goroutine-1移至网络轮询器后，M现在可用于执行与LRQ不同的Goroutine。在这种情况下，Goroutine-2在M上进行了上下文切换。 网络轮询器完成了异步网络系统调用，并将Goroutine-1移回了P的LRQ中。一旦Goroutine-1可以在M上上下文切换回去，它负责的Go相关代码就可以再次执行。这里最大的好处是，执行网络系统调用不需要额外的M。 ","date":"2020-04-07","objectID":"/go_series_scheduler/:4:6","tags":["go"],"title":"Go系列：调度器","uri":"/go_series_scheduler/"},{"categories":["Golang"],"content":"3.7 同步系统调用 当Goroutine想要进行无法异步完成的系统调用时，会发生什么情况？在这种情况下，无法使用网络轮询器，并且进行系统调用的Goroutine将阻止M。不幸的是，但是无法阻止这种情况的发生。无法异步进行的系统调用的一个示例是基于文件的系统调用。如果使用的是CGO，则在其他情况下，调用C函数也会阻塞M。让我们逐一介绍同步系统调用（例如文件I / O）会导致M阻塞的情况。 再次显示了我们的基本调度图，但是这次Goroutine-1将进行一次同步系统调用，该调用将阻塞M1。 调度器可以识别Goroutine-1导致M阻塞。此时，调度器将M1与P分离，而阻塞Goroutine-1仍处于连接状态。然后，调度器会引入一个新的M2来为P服务。这时，可以从LRQ中选择Goroutine-2，并在M2上进行上下文切换。如果由于先前的交换已存在M，则此过渡比必须创建新的M更快。 Goroutine-1进行的阻止系统调用完成。此时，Goroutine-1可以移回LRQ并再次由P服务。如果这种情况需要再次发生，则将M1放在一边以备将来使用。 ","date":"2020-04-07","objectID":"/go_series_scheduler/:4:7","tags":["go"],"title":"Go系列：调度器","uri":"/go_series_scheduler/"},{"categories":["Golang"],"content":"3.8 工作窃取 计划程序的另一个方面是，它是一种可以窃取工作的计划程序。这有助于在某些方面保持调度效率。首先，您想要的最后一件事是M进入等待状态，因为一旦发生这种情况，操作系统将上下文M切换到core。这意味着，即使有一个Goroutine处于可运行状态，P也无法完成任何工作，直到M在上下文中切换回Core为止。窃取工作还有助于在所有P上平衡Goroutine，从而更好地分配工作并更高效地完成工作。 例子： 我们有一个多线程Go程序，其中两个P分别为四个Goroutine和GRQ中的一个Goroutine提供服务。如果P的服务之一迅速地执行其所有Goroutine，会怎样？ P1没有更多的Goroutines可以执行。但是在P2的LRQ和GRQ中都有可运行状态的Goroutine。这是P1需要窃取工作的时刻。窃取工作的规则如下。 runtime.schedule() { // only 1/61 of the time, check the global runnable queue for a G. // if not found, check the local queue. // if not found, // try to steal from other Ps. // if not, check the global runnable queue. // if not found, poll network. } 因此，根据清单2中的这些规则，P1需要在其LRQ中检查P2中的Goroutines，并取其发现结果的一半。 Goroutine的一半来自P2，现在P1可以执行这些Goroutine。 如果P2完成其所有Goroutine的服务并且P1的LRQ中没有剩余，该怎么办？ P2完成了所有工作，现在需要偷一些东西。首先，它将查看P1的LRQ，但找不到任何Goroutine。接下来，将查看GRQ。在那里它将找到Goroutine-9。 P2从GRQ窃取Goroutine-9，并开始执行工作。所有这些偷窃工作的最大好处是，它可以让M保持忙碌而不会闲着。内部，这种窃取工作被认为是在旋转M。这种旋转还有其他好处，JBD在她的工作窃取博客文章中很好地解释了这一点。 ","date":"2020-04-07","objectID":"/go_series_scheduler/:4:8","tags":["go"],"title":"Go系列：调度器","uri":"/go_series_scheduler/"},{"categories":["Golang"],"content":"4. 结论 这里讲了OS Scheduler和Go Scheduler的相关的实现原理。 参考 OS Scheduler Go Scheduler ","date":"2020-04-07","objectID":"/go_series_scheduler/:5:0","tags":["go"],"title":"Go系列：调度器","uri":"/go_series_scheduler/"},{"categories":["Golang"],"content":"内存管理","date":"2020-04-01","objectID":"/go_series_mem/","tags":["go"],"title":"Go系列：内存管理","uri":"/go_series_mem/"},{"categories":["Golang"],"content":"系列目录 《Go系列：内存管理》 《Go系列：并发》 ","date":"2020-04-01","objectID":"/go_series_mem/:1:0","tags":["go"],"title":"Go系列：内存管理","uri":"/go_series_mem/"},{"categories":["Golang"],"content":"1.介绍 内存管理是控制和协调软件应用程序访问计算机内存的方式的过程。 这是软件工程中一个严肃的话题，它使一些人感到困惑，并且对某些人来说是一个黑盒。 ","date":"2020-04-01","objectID":"/go_series_mem/:2:0","tags":["go"],"title":"Go系列：内存管理","uri":"/go_series_mem/"},{"categories":["Golang"],"content":"2. 内存管理 当软件在计算机上的目标操作系统上运行时，它需要访问计算机RAM（随机存取存储器）以执行以下操作： 加载自己需要执行的字节码 存储执行的程序使用的数据值和数据结构 加载程序执行所需的所有运行时系统 当软件程序使用内存时，除了用于加载字节码的空间外，还有两个内存区域，即stack内存和heap内存。 ","date":"2020-04-01","objectID":"/go_series_mem/:3:0","tags":["go"],"title":"Go系列：内存管理","uri":"/go_series_mem/"},{"categories":["Golang"],"content":"2.1 Stack Stack用于静态内存分配，顾名思义，它是一个后进先出（LIFO）堆栈（将其视为盒子堆栈）。 由于这种性质，由于不需要查找，因此从堆栈中存储和检索数据的过程非常快，您只需从其最上面的块中存储和检索数据即可。 但这意味着存储在堆栈中的任何数据都必须是有限且静态的（数据大小在编译时是已知的）。 这是函数的执行数据作为堆栈帧存储的位置（因此，这是实际的执行堆栈）。每个帧都是一个空间块，用于存储该功能所需的数据。例如，每当一个函数声明一个新变量时，它就被“压入”到栈顶块中。然后，每次退出函数时，都会清除最顶层的块，从而清除该函数压入堆栈的所有变量。由于此处存储的数据的静态性质，可以在编译时确定这些值。 多线程应用程序每个线程可以有一个堆栈。 堆栈的内存管理非常简单明了，并且由操作系统完成。 存储在堆栈中的典型数据是局部变量（值类型或原语，原语常量），指针和函数框。 这是您会遇到堆栈溢出错误的地方，因为与堆相比，堆栈的大小受到限制。 对于大多数语言，可以存储在堆栈中的值的大小是有限制的。 ","date":"2020-04-01","objectID":"/go_series_mem/:3:1","tags":["go"],"title":"Go系列：内存管理","uri":"/go_series_mem/"},{"categories":["Golang"],"content":"2.2 Heap 堆用于动态内存分配，与堆栈不同，程序需要使用指针在堆中查找数据（将其视为大型的多级库）。 它比堆栈慢，因为查找数据的过程涉及更多，但它可以存储比堆栈更多的数据。 这意味着可以在此处存储具有动态大小的数据。 堆在应用程序的线程之间共享。 由于其动态特性，堆管理起来比较棘手，这是大多数内存管理问题的起因，这也是该语言自动内存管理解决方案的起因。 存储在堆中的典型数据是全局变量，引用类型（如对象，字符串，映射和其他复杂数据结构）。 如果您的应用程序尝试使用比分配的堆更多的内存，这就是您遇到内存不足错误的地方（尽管这里还有许多其他因素在起作用，例如GC，压缩）。 通常，对堆中可以存储的值的大小没有限制。当然，为应用程序分配多少内存是有上限的。 ","date":"2020-04-01","objectID":"/go_series_mem/:3:2","tags":["go"],"title":"Go系列：内存管理","uri":"/go_series_mem/"},{"categories":["Golang"],"content":"2.3 为什么如此重要 与硬盘驱动器不同，RAM不是无限的。 如果程序继续消耗内存而不释放内存，最终它将耗尽内存并崩溃，甚至使操作系统崩溃。 因此，软件程序不仅会继续使用自己喜欢的RAM，还会导致其他程序和进程的内存不足。 因此，大多数编程语言都没有提供让软件开发人员解决此问题的方法，而是提供了执行自动内存管理的方法。 当我们谈论内存管理时，我们主要是在谈论管理堆内存。 ","date":"2020-04-01","objectID":"/go_series_mem/:3:3","tags":["go"],"title":"Go系列：内存管理","uri":"/go_series_mem/"},{"categories":["Golang"],"content":"2.4 垃圾回收 通过释放未使用的内存分配来自动管理堆内存。 GC是现代语言中最常见的内存管理之一，该过程通常以一定的间隔运行，因此可能会导致较小的开销，称为暂停时间。JVM（Java / Scala / Groovy / Kotlin），JavaScript，C＃，Golang，OCaml和Ruby是默认情况下使用垃圾回收进行内存管理的一些语言。 Mark \u0026 Sweep GC: 也称为跟踪GC。它通常采用两阶段算法，首先将仍被引用为“活动”的对象标记为标记，然后在下一个阶段中释放未激活的对象的内存。 Reference counting GC:在这种方法中，每个对象都获得一个引用计数，该引用计数随对它的引用的更改而增加或减少，并且当计数变为零时将进行垃圾回收。它不是首选，因为它不能处理循环引用。 ","date":"2020-04-01","objectID":"/go_series_mem/:3:4","tags":["go"],"title":"Go系列：内存管理","uri":"/go_series_mem/"},{"categories":["Golang"],"content":"3. Golang内存管理 Go是一种静态类型化和编译的语言，例如C / C ++和Rust。Go应用程序二进制文件中嵌入了一个小型运行时程序，可以处理诸如垃圾收集，调度和并发之类的语言功能。 ","date":"2020-04-01","objectID":"/go_series_mem/:4:0","tags":["go"],"title":"Go系列：内存管理","uri":"/go_series_mem/"},{"categories":["Golang"],"content":"3.1 Go内部存储器结构 Go Runtime schedules Goroutines (G) onto Logical Processors (P) for execution. Each P has a machine (M). 每个Go程序进程都由操作系统（OS）分配了一些虚拟内存，这是该进程可以访问的总内存。虚拟内存中使用的实际内存称为常驻集。 TCMalloc(Thread-Caching Malloc)，Go自己的内存分配器就是以此为模型的。 3.1.1 Page Heap(mheap) Go在这里存储动态数据（在编译时无法计算大小的任何数据）。这是最大的内存块，这是进行垃圾回收（GC）的地方。 驻留集分为每个8KB的页面，并由一个全局mheap对象管理。 Large objects(Object of Size \u003e 32kb) are allocated directly from mheap. These large requests come at an expense of central lock, so only one P’s request can be served at any given point of time. mheap管理分为以下不同结构的页面： mspan:mspan是管理mheap中的内存页面的最基本结构。这是一个双向链接列表，其中包含起始页的地址，跨度大小类和跨度中的页面数。像TCMalloc一样，Go还将内存页按大小分为67个不同类的块，大小从8个字节开始，最高到32 KB，如下图所示 每个span存在两次，一个用于带指针的对象（扫描类），另一个用于不带指针的对象（noscan类）。这在GC期间很有帮助，因为无需遍历noscan跨度即可查找活动对象。 mcentral：mcentral将相同大小类别的spans分组在一起。每个mcentral包含两个mspanList： empty：非空闲对象或spans将以双链表的spans缓存在mcache中。当一个span释放后，它将移至非空列表。 non-empty：具有空闲对象的spans的双链接列表。当从mcentral请求一个新的span时，它将从非空列表中获取该span并将其移到空列表中。 当mcentral没有任何可用跨度时，它会要求mheap重新运行页面。 arena:堆内存在分配的虚拟内存中根据需要增长和缩小。需要更多内存时，mheap将从虚拟内存中拉出一块64MB（用于64位体系结构）的arena块。pages在此处映射到spans。 mcache: 这是一个非常有趣的构造。mcache是提供给P（逻辑处理器）以存储小对象（对象大小\u003c= 32Kb）的内存缓存。尽管它类似于线程堆栈，但它是堆的一部分，用于动态数据。 mcache包含适用于所有类大小的mspan的scan和noscan类型。Goroutine可以从mcache获取内存而没有任何锁，因为P一次只能有一个G。因此，这是更有效的。如果需要，mcache会从mcentral请求新的span。 3.1.2 Stack 这是堆栈存储区，每个Goroutine（G）都有一个堆栈。在这里存储了静态数据，包括功能框架，静态结构，原始值和指向动态结构的指针。这与分配给P的mcache不同。 ","date":"2020-04-01","objectID":"/go_series_mem/:4:1","tags":["go"],"title":"Go系列：内存管理","uri":"/go_series_mem/"},{"categories":["Golang"],"content":"3.2 Go内存使用 现在我们已经清楚了内存的组织方式，让我们看看Go在执行程序时如何使用Stack和Heap。 让我们使用下面的Go程序，该代码并未针对正确性进行优化，因此可以忽略诸如不必要的中间变量之类的问题，因此，重点是可视化堆栈和堆内存的使用情况。 package main import \"fmt\" type Employee struct { name string salary int sales int bonus int } const BONUS_PERCENTAGE = 10 func getBonusPercentage(salary int) int { percentage := (salary * BONUS_PERCENTAGE) / 100 return percentage } func findEmployeeBonus(salary, noOfSales int) int { bonusPercentage := getBonusPercentage(salary) bonus := bonusPercentage * noOfSales return bonus } func main() { var john = Employee{\"John\", 5000, 5, 0} john.bonus = findEmployeeBonus(john.salary, john.sales) fmt.Println(j 与许多垃圾回收语言相比，Go的一个主要区别是许多对象直接在程序堆栈上分配。Go编译器使用一种称为escape analysis 的过程来查找其生命周期在编译时已知的对象，并将它们分配在堆栈上，而不是在垃圾收集的堆内存中。在编译过程中，Go进行了逃逸分析，以确定哪些内容可以放入堆栈（静态数据），哪些内容需要放入堆（动态数据）。我们可以在编译期间通过运行带有-gcflags'-m’标志的go build来查看此详细信息。对于上面的代码，它将输出如下内容： $ go build -gcflags '-m' gc.go # command-line-arguments temp/gc.go:14:6: can inline getBonusPercentage temp/gc.go:19:6: can inline findEmployeeBonus temp/gc.go:20:39: inlining call to getBonusPercentage temp/gc.go:27:32: inlining call to findEmployeeBonus temp/gc.go:27:32: inlining call to getBonusPercentage temp/gc.go:28:13: inlining call to fmt.Println temp/gc.go:28:18: john.bonus escapes to heap temp/gc.go:28:13: io.Writer(os.Stdout) escapes to heap temp/gc.go:28:13: main []interface {} literal does not escape \u003cautogenerated\u003e:1: os.(*File).close .this does not escape 让我们将其形象化。单击幻灯片，然后使用箭头键向前/向后移动，以查看如何执行上述程序以及如何使用堆栈和堆存储器： https://speakerdeck.com/deepu105/golang-memory-usage-stack-vs-heap 如你看到的： main函数保存在stack上的\"main frame\"中 每个函数调用都以frame-block的形式添加到堆栈存储器中 所有静态变量（包括参数和返回值）都保存在堆栈上的功能框内 无论类型如何，所有静态值都直接存储在堆栈中。这也适用于全局范围 所有在堆上创建并使用堆栈指针从堆栈中引用的动态类型。大小小于32Kb的对象将进入P的mcache。这也适用于全局范围 具有静态数据的结构将保留在堆栈上，直到在该位置将任何动态值添加到该结构为止 从当前函数调用的函数被推入栈顶 当函数返回时，其框架从堆栈中删除 旦主过程完成，堆上的对象将不再具有来自Stack的指针，并成为孤立对象 如您所见，堆栈是由操作系统自动管理的，而不是由Go本身自动管理的。因此，我们不必担心堆栈。另一方面，Heap并不是由操作系统自动管理的，并且由于其最大的内存空间并保存动态数据，因此它可能呈指数增长，从而导致我们的程序随着时间的推移而耗尽内存。随着时间的流逝，它也变得支离破碎，使应用程序变慢。这是垃圾收集进来的地方。 ","date":"2020-04-01","objectID":"/go_series_mem/:4:2","tags":["go"],"title":"Go系列：内存管理","uri":"/go_series_mem/"},{"categories":["Golang"],"content":"3.3 Go内存管理 Go的内存管理包括在需要内存时自动分配内存，在不再需要内存时进行垃圾回收。这是由标准库完成的。与C / C ++不同，开发人员不必处理它，并且Go进行的基础管理得到了很好的优化和高效。 3.3.1 内存分配 许多采用垃圾收集的编程语言都使用代内存结构来使收集高效，同时进行压缩以减少碎片。正如我们前面所看到的，Go在这里采用了不同的方法，Go在构造内存方面有很大的不同。Go使用线程本地缓存来加快小型对象分配的速度，并保持扫描/非扫描范围以加快GC的速度。这种结构以及整个过程避免了碎片，从而在GC期间无需紧凑。让我们看看这种分配是如何进行的。 Go根据对象的大小决定对象的分配过程，并分为三类： Tiny(size \u003c 16B): 小于16个字节的对象是使用mcache的微型分配器分配的。这是高效的，并且在单个16字节块上完成了多个微小分配。 Small(size 16B ~ 32KB): 在运行G的P的mcache上，将大小在16字节到32 KB之间的对象分配给相应的大小类（mspan）。 无论是小型分配还是小型分配，如果mspan的列表为空，则分配器将从mheap中获取大量页面供mspan使用。 Large(size \u003e 32KB): 大小大于32 KB的对象直接分配到相应的mheap大小级别上。如果mheap为空或没有足够大的页面运行，则它将从OS中分配一组新的页面（至少1MB）。 3.3.2 垃圾回收 现在我们知道Go如何分配内存，让我们看看它如何自动收集Heap内存，这对于应用程序的性能非常重要。当程序尝试在堆上分配的内存大于可用内存时，我们会遇到内存不足错误。管理不当的堆也可能导致内存泄漏。 Go通过垃圾回收管理堆内存。简单来说，它释放了孤立对象（即不再从堆栈中直接或间接（通过另一个对象中的引用）引用的对象）使用的内存，从而为创建新对象腾出了空间。 从版本1.12开始，Golang使用了非世代的并发三色标记和清除收集器。收集过程大致如下所示，由于版本之间的差异，我不想赘述。有时间，可以看下这篇 当完成一定百分比（GC百分比）的堆分配并且收集器执行不同的工作阶段时，该过程开始： Mark Setup (Stop the world): 当GC启动时，收集器将打开写屏障，以便可以在下一个并发阶段保持数据完整性。此步骤需要非常小的暂停，因为每个正在运行的Goroutine都会暂停以启用此功能，然后继续。 Marking (Concurrent): 一旦打开写屏障，就会使用25％的可用CPU容量与应用程序并行开始实际的标记过程。保留相应的P，直到标记完成。这是使用专用Goroutines完成的。在这里，GC标记了活动堆中的值（从任何活动Goroutine的堆栈中引用）。当采集花费更长的时间时，该过程可以从应用程序中使用活动的Goroutine来辅助标记过程。这称为标记辅助。 Mark Termination (Stop the world):标记完成后，将暂停每个活动的Goroutine，并关闭写屏障并开始执行清理任务。GC还会在此处计算下一个GC目标。完成此操作后，保留的P会释放回应用程序。 Sweeping (Concurrent):完成收集并尝试分配后，清除过程将开始从未标记为活动的堆中回收内存。扫描的内存量与分配的内存量同步。 让我们在一个Goroutine中看到它们的实际作用。为了简洁起见，将对象的数量保持较小。单击幻灯片，然后使用箭头键向前/向后移动以查看该过程： 我们正在查看一个Goroutine，实际过程是对所有活动Goroutine进行的。首先打开写屏障。 标记过程选择GC根并将其着色为黑色，并以深度优先的树状方式遍历该指针，将遇到的每个对象标记为灰色 当它到达无扫描范围内的对象时，或者当对象不再有指针时，它将完成根操作并拾取下一个GC根对象 当它到达无扫描范围内的对象时，或者当对象不再有指针时，它将完成根操作并拾取下一个GC根对象 扫描完所有GC根之后，它将拾取灰色对象，并以类似方式继续遍历其指针 当不再有灰色物体留下时，标记过程完成，并且写入屏障被关闭 分配开始时将进行扫描 他有一些制止世界的过程，但通常情况下它可以忽略不计，这是非常快的。对象的着色发生在跨度的gcmarkBits属性中 ","date":"2020-04-01","objectID":"/go_series_mem/:4:3","tags":["go"],"title":"Go系列：内存管理","uri":"/go_series_mem/"},{"categories":["Golang"],"content":"4. 结论 这里我们讲述了什么是内存管理，以及Go在内存管理上的操作。 参考 https://deepu.tech/memory-management-in-programming/ ","date":"2020-04-01","objectID":"/go_series_mem/:5:0","tags":["go"],"title":"Go系列：内存管理","uri":"/go_series_mem/"},{"categories":["Nginx"],"content":"Nginx匹配机制总结","date":"2020-02-06","objectID":"/nginx_match/","tags":["nginx","linux\"","application"],"title":"Nginx匹配机制总结","uri":"/nginx_match/"},{"categories":["Nginx"],"content":"背景 Nginx是一个当前主流的HTTP服务器和反向代理服务器，很多做WEB相关的同学基本都会用到，很多云厂商的七层负载均衡器也基本都是基于nginx实现的，个人在工作过程也算是经常接触，这篇文章主要想总结一下nginx的匹配机制，主要分为两块，一块是server的匹配，一块是location的匹配。 ","date":"2020-02-06","objectID":"/nginx_match/:1:0","tags":["nginx","linux\"","application"],"title":"Nginx匹配机制总结","uri":"/nginx_match/"},{"categories":["Nginx"],"content":"Server匹配机制 配置过nginx的都知道，在一个http模块中是可以配置多个server模块的，并且多个server模块是可以配置相同的监听端口的，下面是一个简单的server配置例子： server { listen 80; server_name example.org www.example.org; ... } server { listen 80; server_name example.net www.example.net; ... } server { listen 80; server_name example.com www.example.com; ... } 当我们对nginx发起http请求后，nginx会拿到http请求中对应的 \"Host\" 头部跟server模块中的server_name进行匹配，根据匹配的server结果进入具体的server模块处理http请求。那么，它具体的匹配机制是怎样的呢？ 首先，我们先简单了解下nginx内部server的相关结构， 其中listen和server_name在配置文件中的写法有： listen(可带default_server标识) ip:port ip(监听80端口) port(监听所有地址) server_name www.example.com(完整域名) *.example.com(带通配符开头的域名) www.example.*(带通配符结尾的域名) ~^(www.)?(.+)$(正则写法的域名) 代码中的具体结构： /************************************************************************************* 伪结构体示例 (port) --\u003e address(ip:port) --\u003e server(example.com) --\u003e server(example.net) 一个server模块的唯一标识是由address(listen配置)和server(server_name配置)组成 *************************************************************************************/ /* address 结构体，具有相同的ip:port */ struct ngx_http_addr_conf_s { /* default_server 存储的是listen配置里带default_server标识的server， 若没有就为顺序中的第一个server */ ngx_http_core_srv_conf_t *default_server; ngx_http_virtual_names_t *virtual_names; unsigned ssl:1; unsigned http2:1; unsigned proxy_protocol:1; }; /* virtual_name结构体，存储hash_combined和正则写法的server_name */ typedef struct { ngx_hash_combined_t names; ngx_uint_t nregex; ngx_http_server_name_t *regex; } ngx_http_virtual_names_t; /* hash_combined结构体，存储完成域名、通配符开头、通配符结尾的server_name */ typedef struct { ngx_hash_t hash; ngx_hash_wildcard_t *wc_head; ngx_hash_wildcard_t *wc_tail; } ngx_hash_combined_t; 通过结构体，我们来说明下server的匹配规则： host是否匹配virtual_names中的names中的完整域名(hash)，若是则返回 host是否匹配virtual_name中的names中的通配符开头的域名(wc_head)，若是则返回 host是否匹配virtual_name中的names中的通配符结尾的域名(wc_tail)，若是则返回 host是否匹配virtual_name中的正则写法的域名(regex)，若是则返回 返回default_server 具体示例如下： #精确匹配，第一优先级 server { listen 80; server_name www.test.com; } #通配符开头匹配，第二优先级， server { listen 80; server_name *.test.com; } #通配符结尾匹配，第三优先级 server { listen 80; server_name www.test.*; } #正则匹配，第三优先级 server { listen 80; server_name ~^(www.)?(.+)$; } #default，没找到对应host，则以此优先 server { listen 80 defalut_server; server_name _; } #若没有加defalut_server，则第一个server为defalut_server server { listen 80; server_name _; } ","date":"2020-02-06","objectID":"/nginx_match/:2:0","tags":["nginx","linux\"","application"],"title":"Nginx匹配机制总结","uri":"/nginx_match/"},{"categories":["Nginx"],"content":"Location匹配机制 一个server模块可以配置多个location，nginx根据URI来进行匹配， lication的写法有以下几种： = /uri 精确匹配 ^~ /uri 非正则前缀匹配 ~ 或者 ~* /uri 正则匹配 /uri 前缀匹配 整个location匹配机制如下： 针对所有前缀字符串测试URI(包括精确匹配、非正则前缀匹配、前缀匹配中的字符串) uri等于精确匹配中的字符串，停止搜索 最长(最相似)前缀字符串如果为非正则前缀匹配(带^~)，则停止正则搜索 保存最长(最相似)的前缀字符串 按顺序进行uri和正则匹配测试，有一个匹配成功后就停止搜索 如果都没有，就使用最长(最相似)的前缀匹配 额外说明： 最长(最相似)前缀字符串的测试阶段，非正则前缀匹配匹配、前缀匹配的优先级是一致的，谁的长度长，谁优先。优先前缀匹配的前提必须是前缀字符串非正则前缀匹配的长度大于前缀匹配的长度，这个很多网站都是直接写成了非正则前缀匹配是第二优先级，没有说明前提条件。 具体示例如下： #精确匹配，第一优先级 location = /test { } #前缀匹配，最低优先级，长度优先 location /test/aa { } #非正则前缀匹配，最长前缀下为第二优先级(特殊条件下) location ^~ /test { } #正则匹配，第三优先级,顺序优先 # ~ : 区分大小写 # ~* : 不区分大小写 location ~* ^/test { } ","date":"2020-02-06","objectID":"/nginx_match/:3:0","tags":["nginx","linux\"","application"],"title":"Nginx匹配机制总结","uri":"/nginx_match/"},{"categories":["Nginx"],"content":"参考 https://nginx.org/en/docs/http/request_processing.html https://nginx.org/en/docs/http/ngx_http_core_module.html#location https://docs.nginx.com/nginx/admin-guide/web-server/web-server/ https://www.codedump.info/post/20190212-nginx-http-config/ ","date":"2020-02-06","objectID":"/nginx_match/:4:0","tags":["nginx","linux\"","application"],"title":"Nginx匹配机制总结","uri":"/nginx_match/"},{"categories":["Linux"],"content":"谈谈文件描述符","date":"2019-12-29","objectID":"/file_descriptor/","tags":["linux"],"title":"谈谈文件描述符","uri":"/file_descriptor/"},{"categories":["Linux"],"content":"概念 wiki解释，文件描述符在形式上是一个非负整数。实际上，它是一个索引值，指向内核为每一个进程所维护的该进程打开文件的记录表。当程序打开一个现有文件或者创建一个新文件时，内核向进程返回一个文件描述符。在程序设计中，一些涉及底层的程序编写往往会围绕着文件描述符展开。 一个文件描述符是一个数字，唯一标识一个在计算机的操作系统打开的文件。它描述了数据资源，以及如何访问该资源。 当程序要求打开文件（或其他数据资源，例如网络套接字）时，内核： 授予访问权限。 在全局文件表中创建一个条目。 向软件提供该条目的位置。 该描述符是唯一的非负整数。系统上每个打开的文件至少存在一个文件描述符。 ","date":"2019-12-29","objectID":"/file_descriptor/:1:0","tags":["linux"],"title":"谈谈文件描述符","uri":"/file_descriptor/"},{"categories":["Linux"],"content":"细节 对于内核，所有打开的文件均由文件描述符引用。文件描述符是一个非负数。当我们打开现有文件或创建新文件时，内核将文件描述符返回到进程。当我们想读取或写入文件时，我们用文件描述符标识文件。 每个Linux进程（也许是守护程序除外）都应该具有三个标准的POSIX文件描述符： POSIX常数名称 文件描述符 描述 STDIN_FILENO 0 标准输入 STDOUT_FILENO 1 标准输出 STDERR_FILENO 2 标准误差 有三个“系统文件表”：有一个文件描述符表，它将文件描述符（小整数）映射到打开的文件表中的条目。打开文件表中的每个条目（除其他事项外）还包含文件偏移量和指向内存中inode表的指针。在打开的文件表中，每个open（）调用都有一个文件表条目，如果文件描述符是dup（）ed或fork（）ed，则共享该条目。 我们使用来自维基百科的示例来显示这些表的工作方式。这是一张照片： 单个进程的文件描述符，文件表和索引节点表。请注意，多个文件描述符可以引用相同的文件表条目（例如，由于dup系统调用），并且多个文件表条目可以依次引用同一个索引节点（如果已多次打开；则该表之所以仍然简化，是因为它通过文件名来表示索引节点，即使索引节点可以具有多个名称也是如此。文件描述符3没有引用文件表中的任何内容，表明它已关闭。 理解具体情况，需要了解由内核维护的 3 个数据结构： 进程级 文件描述符表 ( file descriptor table ) 系统级 打开文件表 ( open file table ) 文件系统 i-node表 ( i-node table ) 这 3 个数据结构之间的关系如图所示： ","date":"2019-12-29","objectID":"/file_descriptor/:2:0","tags":["linux"],"title":"谈谈文件描述符","uri":"/file_descriptor/"},{"categories":["Linux"],"content":"文件描述符表 内核为每个进程维护一个 文件描述符表 ，该表每一条目都记录了单个文件描述符的相关信息，包括： 控制标志 ( flags )，目前内核仅定义了一个，即 close-on-exec 打开文件描述体指针 ","date":"2019-12-29","objectID":"/file_descriptor/:2:1","tags":["linux"],"title":"谈谈文件描述符","uri":"/file_descriptor/"},{"categories":["Linux"],"content":"打开文件表 内核对所有打开的文件维护一个系统级别的 打开文件描述表 ( open file description table )，简称 打开文件表 。 表中条目称为 打开文件描述体 ( open file description )，存储了与一个打开文件相关的全部信息，包括： 文件偏移量 ( file offset )，调用 read() 和 write() 更新，调用 lseek() 直接修改 访问模式 ，由 open() 调用设置，例如：只读、只写或读写等 i-node 对象指针 ","date":"2019-12-29","objectID":"/file_descriptor/:2:2","tags":["linux"],"title":"谈谈文件描述符","uri":"/file_descriptor/"},{"categories":["Linux"],"content":"i-node 表 每个文件系统会为存储于其上的所有文件(包括目录)维护一个 i-node 表，单个 i-node 包含以下信息： 文件类型 ( file type )，可以是常规文件、目录、套接字或 FIFO 访问权限 文件锁列表 ( file locks ) 文件大小 等等 i-node 存储在磁盘设备上，内核在内存中维护了一个副本，这里的 i-node 表为后者。 副本除了原有信息，还包括： 引用计数 (从打开文件描述体)、所在 设备号 以及一些临时属性，例如文件锁。 ","date":"2019-12-29","objectID":"/file_descriptor/:2:3","tags":["linux"],"title":"谈谈文件描述符","uri":"/file_descriptor/"},{"categories":["Linux"],"content":"参数优化 ","date":"2019-12-29","objectID":"/file_descriptor/:3:0","tags":["linux"],"title":"谈谈文件描述符","uri":"/file_descriptor/"},{"categories":["Linux"],"content":"1. 系统最大的文件描述符数量 系统文件最大值取决于内存大小，在kernel初始化时定义 代码: /* * One file with associated inode and dcache is very roughly 1K. Per default * do not use more than 10% of our memory for files. */ void __init files_maxfiles_init(void) { unsigned long n; unsigned long nr_pages = totalram_pages(); unsigned long memreserve = (nr_pages - nr_free_pages()) * 3/2; memreserve = min(memreserve, nr_pages - 1); n = ((nr_pages - memreserve) * (PAGE_SIZE / 1024)) / 10; files_stat.max_files = max_t(unsigned long, n, NR_FILE); } 由代码可知，file-max的值不超过内存的10% #获取total ram pages 和 PAGE_SIZE大小 $ getconf -a | grep \"PAGE\" PAGESIZE 4096 PAGE_SIZE 4096 _AVPHYS_PAGES 565489 _PHYS_PAGES 1011579 #查看系统最大打开文件描述符数 $ cat /proc/sys/fs/file-max 399894 #查看当前系统使用的打开文件描述符数 $ cat /proc/sys/fs/file-nr 928 0 399894 | | |_ Max no. of file descriptors allowed on the system | | | |__ Total free allocated file descriptors | |__ Total allocated file descriptors #设置系统最大文件描述符 #临时性 $ echo 1000000 \u003e /proc/sys/fs/file-max #永久性 #在/etc/sysctl.conf中设置 fs.file-max = 1000000 $ sysctl -p ","date":"2019-12-29","objectID":"/file_descriptor/:3:1","tags":["linux"],"title":"谈谈文件描述符","uri":"/file_descriptor/"},{"categories":["Linux"],"content":"2. 进程最大描述符 # 查看某个进程的使用 $ ls -l /proc/2374/fd | wc -l # 进程最大打开文件描述符数 #soft limit $ ulimit -n 65535 #hard limit $ ulimit -Hn 65535 #soft limit不能大于hard limit #设置 #临时性 $ ulimit -Sn 1600000 #永久性 $ vim /etc/security/limits.conf root soft nofile 65535 root hard nofile 65535 #设置nofile的hard limit还有一点要注意的就是hard limit不能大于/proc/sys/fs/nr_open ","date":"2019-12-29","objectID":"/file_descriptor/:3:2","tags":["linux"],"title":"谈谈文件描述符","uri":"/file_descriptor/"},{"categories":["Linux"],"content":"3. 总结 1. 所有进程打开的文件描述符数不能超过/proc/sys/fs/file-max 2. 单个进程打开的文件描述符数不能超过user limit中nofile的soft limit 3. nofile的soft limit不能超过其hard limit 4. nofile的hard limit不能超过/proc/sys/fs/nr_open ","date":"2019-12-29","objectID":"/file_descriptor/:3:3","tags":["linux"],"title":"谈谈文件描述符","uri":"/file_descriptor/"},{"categories":["Linux"],"content":"Linux PAM模块","date":"2019-09-05","objectID":"/linux_pam/","tags":["linux","security"],"title":"Linux PAM模块","uri":"/linux_pam/"},{"categories":["Linux"],"content":"概念 Linux-PAM（Pluggable Authentication Modules for Linux）是一套共享库,使本地系统管理员可以随意选择程序的认证方式。换句话说，不用(重新编写)重新编译一个包含PAM功能的应用程序，就可以改变它使用的认证机制，这种方式下，就算升级本地认证机制,也不用修改程序。 ","date":"2019-09-05","objectID":"/linux_pam/:1:0","tags":["linux","security"],"title":"Linux PAM模块","uri":"/linux_pam/"},{"categories":["Linux"],"content":"工作机制 当应用程序希望与PAM交互以处理事件时，他们必须包括libpam，该libpam允许通过库提供的API进行通信。 当PAM看到必须处理的新事件时，它将查看/etc/pam.d中的相关配置文件，并确定在某些阶段必须使用哪些模块。 ","date":"2019-09-05","objectID":"/linux_pam/:2:0","tags":["linux","security"],"title":"Linux PAM模块","uri":"/linux_pam/"},{"categories":["Linux"],"content":"/etc/pam.d配置文件介绍 配置文件语法 type control module-path module-arguments 配置文件分为四列 第一列代表模块类型 第二列代表控制标记 第三列代表模块路径 第四列代表模块参数 ","date":"2019-09-05","objectID":"/linux_pam/:3:0","tags":["linux","security"],"title":"Linux PAM模块","uri":"/linux_pam/"},{"categories":["Linux"],"content":"类型 类型 是规则对应的管理组。它用于指定后续模块要与哪个管理组关联。 目前有四种类型: account 此模块类型执行基于非身份验证的帐户管理。 通常用于限制/允许对服务的访问，例如是否允许登录,是否达到最大用户数,或是root用户是否允许在这个终端登录等。 auth 此模块为用户验证提供两方面服务。让应用程序提示用户输入密码或者其他标记，确认用户合法性；通过他的凭证许可权限，设定组成员关系或者其他优先权。 password 此模块用于控制用户更改密码的全过程。 session 此模块处理为用户提供服务之前/后需要做的些事情。包括：开启/关闭交换数据的信息，监视目录等，设置用户会话环境等。也就是说这是在系统正式进行服务提供之前的最后一道关口。 如果在类型前加一个短横线 -，就表示如果找不到这个模块，导致无法被加载时，这一事件不会被记录在日志中。这个功能适用于那些认证时非必需的、安装时可能没被安装进系统的模块。 ","date":"2019-09-05","objectID":"/linux_pam/:3:1","tags":["linux","security"],"title":"Linux PAM模块","uri":"/linux_pam/"},{"categories":["Linux"],"content":"控制标记 流程栈（stack） 它是认证时执行步骤和规则的堆叠。在某个服务的配置文件中，它体现在了配置文件中的自上而下的执行顺序中。栈是可以被引用的，即在一个栈（或者流程）中嵌入另一个栈。 控制标记 规定如何处理PAM模块鉴别认证的结果，简而言之就是鉴别认证成功或者失败之后会发生什么事，如何进行控制。一般有两种形式，一种是比较常见的“关键字”方式，另一种则是用方括号（[]）包含的“value =action”方式。 关键字方式: required 如果本条目没有被满足，那最终本次认证一定失败，但认证过程不因此打断。整个栈运行完毕之后才会返回“认证失败”信号。 requisite 如果本条目没有被满足，那本次认证一定失败，而且整个栈立即中止并返回错误信号。 sufficient 如果本条目的条件被满足，且本条目之前没有任何required条目失败，则立即返回“认证成功”信号；如果对本条目的验证失败，不对结果造成影响。 optional 该条目仅在整个栈中只有这一个条目时才有决定性作用，否则无论该条验证成功与否都和最终结果无关。 include 将其他配置文件中的流程栈包含在当前的位置，就好像将其他配置文件中的内容复制粘贴到这里一样。 substack 运行其他配置文件中的流程，并将整个运行结果作为该行的结果进行输出。该模式和 include 的不同点在于认证结果的作用域：如果某个流程栈 include 了一个带 requisite 的栈，这个 requisite 失败将直接导致认证失败，同时退出栈；而某个流程栈 substack 了同样的栈时，requisite 的失败只会导致这个子栈返回失败信号，母栈并不会在此退出。 value = action方式: 另外还有一种比较复杂的格式为value = action的语法来设置控制标志，标志之间会以空格分开。格式如下： [value1 = action1 value2 = action2 ……] 其中value可以是下列Linux PAM库的返回值： success、open_err、symbol_err、service_err、 system_err、buf_err、perm_denied、auth_err、cred_insufficient、authinfo_unavail、user_unknown、maxtries、new_authtok_reqd、acct_expired、 session_err、cred_unavail、cred_expired、cred_err、no_module_data、conv_err、 authtok_err、authtok_recover_err、authtok_lock_busy、authtok_disable_aging、 try_again、ignore、abort、authtok_expired、module_unknown、bad_item和default。其中，default代表其他所有没有明确说明的返回值。 流程栈中很可能有多个验证规则，每条验证的返回值可能不尽相同，那么到底哪一个验证规则能作为最终的结果呢？这就需要 actionN 的值来决定了。actionN 的值有以下几种： ignore 在一个栈中有多个认证条目的情况下，如果标记 ignore 的返回值被命中，那么这条返回值不会对最终的认证结果产生影响。 bad 标记 bad 的返回值被命中时，最终的认证结果注定会失败。此外，如果这条 bad 的返回值是整个栈的第一个失败项，那么整个栈的返回值一定是这个返回值，后面的认证无论结果怎样都改变不了现状了。 die 标记 die 的返回值被命中时，马上退出栈并宣告失败。整个返回值为这个 die 的返回值。 ok 在一个栈的运行过程中，如果 ok 前面没有返回值，或者前面的返回值为 PAM_SUCCESS，那么这个标记了 ok 的返回值将覆盖前面的返回值。但如果前面执行过的验证中有最终将导致失败的返回值，那 ok 标记的值将不会起作用。 done 在前面没有 bad 值被命中的情况下，done 值被命中之后将马上被返回，并退出整个栈。 N（无符号整数） 功效和 ok 类似，并且会跳过接下来的 N 个验证步骤。如果 N = 0 则和 ok 完全相同。 reset 清空之前生效的返回值，并且从下面的验证起重新开始。 关键字的控制方式也可以用value = action方式来表示 #required [success=ok new_authtok_reqd=ok ignore=ignore default=bad] #requisite [success=ok new_authtok_reqd=ok ignore=ignore default=die] #sufficient [success=done new_authtok_reqd=done default=ignore] #optional [success=ok new_authtok_reqd=ok default=ignore] ","date":"2019-09-05","objectID":"/linux_pam/:3:2","tags":["linux","security"],"title":"Linux PAM模块","uri":"/linux_pam/"},{"categories":["Linux"],"content":"模块路径 模块路径 是应用程序要使用的PAM的绝对路径，或者是默认模块位置的相对路径名，一般为/lib/security /或/lib64/security/，取决于系统架构。 ","date":"2019-09-05","objectID":"/linux_pam/:3:3","tags":["linux","security"],"title":"Linux PAM模块","uri":"/linux_pam/"},{"categories":["Linux"],"content":"模块参数 模块参数 将只和特定模块相关，因此某个模块的文档中一定包含其参数的信息。如果需要在单个参数中使用空格，可以将整个参数用方括号（[]）包裹起来。 ","date":"2019-09-05","objectID":"/linux_pam/:3:4","tags":["linux","security"],"title":"Linux PAM模块","uri":"/linux_pam/"},{"categories":["Linux"],"content":"一个例子 以/etc/pam.d/sshd为例 加载/etc/pam.d/password-auth配置文件 大部分模块的配置文件可在/etc/security中找到，并进行配置 我们比较常进行配置的最大文件数和最大进程数就是在limit.conf中配置，在sshd中会加载到 ","date":"2019-09-05","objectID":"/linux_pam/:4:0","tags":["linux","security"],"title":"Linux PAM模块","uri":"/linux_pam/"},{"categories":["Security"],"content":"理解SSL/TLS协议","date":"2017-01-08","objectID":"/tls/","tags":["security"],"title":"理解SSL/TLS协议","uri":"/tls/"},{"categories":["Security"],"content":"背景 早期我们在访问web时使用HTTP协议，该协议在传输数据时使用明文传输，明文传输带来了以下风险： 信息窃听风险，第三方可以获取通信内容 信息篡改风险，第三方可以篡改通信内容 身份冒充风险，第三方可以冒充他人身份参与通信 为了解决明文传输所带来的风险，网景公司在1994年设计了SSL用于Web的安全传输协议，这是SSL的起源。IETF将SSL进行标准化，1999年公布了第一版TLS标准文件。随后又公布了 RFC 5246（2008年8月）与 RFC 6176 （2011年3月）。该协议在web中被广泛应用。 ","date":"2017-01-08","objectID":"/tls/:1:0","tags":["security"],"title":"理解SSL/TLS协议","uri":"/tls/"},{"categories":["Security"],"content":"SSL/TLS协议 TLS（Transport Layer Security，传输层安全协议），及其前身SSL（Secure Sockets Layer，安全套接层）是一种安全协议，目的是为互联网通信，提供安全及数据完整性保障。 TLS协议使用以下三种机制为信息通信提供安全传输： 隐秘性，所有通信都通过加密后进行传播 身份认证，通过证书进行认证 可靠性，通过校验数据完整性维护一个可靠的安全连接 ","date":"2017-01-08","objectID":"/tls/:2:0","tags":["security"],"title":"理解SSL/TLS协议","uri":"/tls/"},{"categories":["Security"],"content":"以TLS1.2为例说明TLS协议 TLS协议由TLS Record Protocol和TLS Handshake Protocol两层协议组成 ","date":"2017-01-08","objectID":"/tls/:3:0","tags":["security"],"title":"理解SSL/TLS协议","uri":"/tls/"},{"categories":["Security"],"content":"TLS Record Protocol 该协议提供了连接安全的两个基本特性： 连接私有 对称密码用于数据加密，这种对称加密是为每条连接唯一生成的并基于另一个人协商的秘密协议 连接可靠 消息传输包括一条消息 使用密钥MAC进行完整性检查，安全哈希函数（例如， SHA-1等）用于MAC计算。 ","date":"2017-01-08","objectID":"/tls/:3:1","tags":["security"],"title":"理解SSL/TLS协议","uri":"/tls/"},{"categories":["Security"],"content":"TLS Handshake Protocol 该协议提供了连接安全的三个基本特性： 可以使用非对称身份验证对等方的身份，或者 公钥，密码学等 共享密钥的协商是安全的 谈判可靠 一个TLS握手协议一般涉及以下步骤： 交换hello信息用于算法协商，交换随机值，并检查会话是否恢复 交换必要的密码信息以允许客户端和服务端同意使用premaster secret 交换证书和密码信息以允许客户端和服务端进行身份验证 通过随机值和premaster secret生成master secret 向record layer提供安全参数 允许客户端和服务器验证其对等方具有计算出的相同安全参数，并且握手发生在没有被攻击者篡改的情况下 TLS握手的完整消息流 ClientHello 客户端提供了以下内容: 支持的协议版本 客户端随机数据(后续用于握手) 可选的session id 加密套件列表 压缩方法列表 扩展列表 ServerHello 服务端提供了以下内容： 选择后的协议版本 选择后的加密套件 选择后的压缩方法 服务端随机数据(后续用于握手) session id 扩展列表 ServerCertificate 服务端提供了证书，证书包含以下内容： 服务端的hostname 服务端所使用的公钥 来自受信任的第三方的证明，证明此hostname的所有者拥有此公钥的私钥 ServerKeyExchange(可选) 服务端仅在证书包含的信息不足以使客户端进行premaster secret交换时发送该消息 CertificateRequest(可选) 当服务端需要客户端证书时发送，需要加密套件支持 ServerHelloDone 服务端表明已经完成了一半的handshake ClientCertificate(可选) 当服务端有需要验证客户端证书时发送，如果加密套件不支持，则消息不包含证书 ClientKeyExchange 生成一个48byte的premaster secret，并通过服务端证书包含的公钥进行加密发送给服务端 CertificateVerify(可选) 该消息只在客户端证书具有签名能力时发送 ClientChangeCipherSpec(message) 一种协议，数据只有一字节，用于告知Server端已经切换到之前协商好的加密套件的状态，准备使用之前协商好的加密套件加密数据并进行传输了。 ClientFinished 客户端和服务端现在都拥有3个数值 ClientHello.random ServerHello.random premaster secret master secret由上面三个数值计算而成 master_secret = PRF(pre_master_secret,\"master secret\",ClientHello.random + ServerHello.random)[0..47]; 使用master secret加密finished消息发送给服务端 ServerChangeCipherSpec(message) 同上 ServerFinished 同上 根据之前的握手信息，如果客户端和服务端都能对Finish信息进行正常加解密且消息正确的被验证，则说明握手通道已经建立成功。 接下来，双方所有的通信数据都通过Master Secret进行加密后传输。　 ","date":"2017-01-08","objectID":"/tls/:3:2","tags":["security"],"title":"理解SSL/TLS协议","uri":"/tls/"},{"categories":["Security"],"content":"参考： WIKI/Transport_Layer_Security RFC 5246 图示TLS连接 ","date":"2017-01-08","objectID":"/tls/:3:3","tags":["security"],"title":"理解SSL/TLS协议","uri":"/tls/"},{"categories":["Network"],"content":"谈谈HTTP","date":"2016-05-29","objectID":"/http/","tags":["network"],"title":"谈谈HTTP","uri":"/http/"},{"categories":["Network"],"content":"写在前面 如今网络已经无处不在，人们通过网络获取浏览各种信息，其中，大部分都是通过浏览器访问各种网页来获取我们想要的信息，那么浏览器与网页(服务端)究竟是如何通信的呢？这就得从HTTP协议说起了，浏览器获取网页信息都是基于HTTP协议来处理的。 ","date":"2016-05-29","objectID":"/http/:0:1","tags":["network"],"title":"谈谈HTTP","uri":"/http/"},{"categories":["Network"],"content":"概念 HTTP（HyperText Transfer Protocol，超文本传输协议）是互联网上应用最为广泛的一种网络协议。设计HTTP最初的目的是为了提供一种发布和接收HTML页面的方法。通过HTTP或者HTTPS协议请求的资源由统一资源标识符（Uniform Resource Identifiers，URI）来标识。HTTP是一个应用层协议，由请求和响应构成，是一个标准的客户端服务器模型。其具有如下特点： 支持客户/服务器模式。 简单快速：客户向服务器请求服务时，只需传送请求方法和路径。请求方法常用的有GET、HEAD、POST。每种方法规定了客户与服务器联系的类型不同。由于HTTP协议简单，使得HTTP服务器的程序规模小，因而通信速度很快。 灵活：HTTP允许传输任意类型的数据对象。正在传输的类型由Content-Type加以标记。 无连接：无连接的含义是限制每次连接只处理一个请求。服务器处理完客户的请求，并收到客户的应答后，即断开连接。采用这种方式可以节省传输时间。 无状态：HTTP协议是无状态协议。无状态是指协议对于事务处理没有记忆能力。缺少状态意味着如果后续处理需要前面的信息，则它必须重传，这样可能导致每次连接传送的数据量增大。另一方面，在服务器不需要先前信息时它的应答就较快 PS：尽管TCP/IP协议是互联网上最流行的应用，HTTP协议中，并没有规定必须使用它或它支持的层。事实上，HTTP可以在任何互联网协议上，或其他网络上实现。HTTP假定其下层协议提供可靠的传输。因此，任何能够提供这种保证的协议都可以被其使用。因此也就是其在TCP/IP协议族使用TCP作为其传输层。 ","date":"2016-05-29","objectID":"/http/:0:2","tags":["network"],"title":"谈谈HTTP","uri":"/http/"},{"categories":["Network"],"content":"工作流程 HTTP协议的通信过程永远是客户端发起请求(request)，服务器回送响应(respone)，如下图所示： 一个完整的HTTP操作称为一个事务，其流程可分为四步： 建立连接(TCP三次握手) 客户端发送一个请求报文给服务器 服务器响应对应信息 客户端接收信息，然后断开连接 ","date":"2016-05-29","objectID":"/http/:0:3","tags":["network"],"title":"谈谈HTTP","uri":"/http/"},{"categories":["Network"],"content":"请求和响应详解 请求报文 请求行：由请求方法、URL和HTTP版本组成 eg：GET /index.html HTTP/1.1 请求方法 GET：请求获取URI所标识的资源 HEAD：请求获取URI所标识的资源，但不传回资源的文本部分 POST：向指定URI资源提交数据，请求服务器进行处理 PUT：向指定URI资源上传其最新内容 DELETE：请求服务器删除URI所标识的资源 TRACE：回显服务器收到的请求，主要用于测试或诊断 OPTIONS：请求URI资源所支持的HTTP请求方法 CONNECT：HTTP/1.1协议中预留给能够将连接改为管道方式的代理服务器。通常用于SSL加密服务器的链接 URL 请求的资源路径 协议版本 现在大部分为HTTP/1.0 和 HTTP/1.1 请求头部 eg：host:www.google.com host为必选，其他都为可选参数 空行 消息体 请求所带的文本 响应报文 状态行：由协议版本、状态码和描述信息组成 eg：HTTP/1.1 200 OK 协议版本 状态码：用于告诉客户端，服务器是否产生预期的响应 1XX：提示信息，表示请求已被成功接收，继续处理 2XX：成功，表示请求已被成功接收，理解 3XX：重定向，要完成请求必须进行更进一步的处理 4XX：客户端错误，请求有语法错误或请求无法实现 5XX：服务器端错误，服务器未能实现合法的请求 描述信息 响应头部 空行 消息体 一个例子 访问codecc.xyz首页 Request，首行为请求行，其余为请求头部 Respone，首行为响应状态行，空行前为响应头部，其余为响应数据 ","date":"2016-05-29","objectID":"/http/:0:4","tags":["network"],"title":"谈谈HTTP","uri":"/http/"},{"categories":["Network"],"content":"参考 https://en.wikipedia.org/wiki/Hypertext_Transfer_Protocol ","date":"2016-05-29","objectID":"/http/:0:5","tags":["network"],"title":"谈谈HTTP","uri":"/http/"},{"categories":["Linux"],"content":"理解系统启动过程","date":"2016-05-21","objectID":"/system_start/","tags":["linux"],"title":"理解系统启动过程","uri":"/system_start/"},{"categories":["Linux"],"content":"前言 Linux是一种自由和开放源代码的类UNIX操作系统。该操作系统的内核由林纳斯·托瓦兹在1991年10月5日首次发布。在加上用户空间的应用程序之后，成为Linux操作系统。Linux是自由软件和开放源代码软件发展中最著名的例子。 接触Linux的时间也不算短了，一直都是直接使用Linux操作系统进行一些工作，很少去了解系统从开机到能使用的整个过程，感觉有需要好好理解下整个系统的启动过程，故写这篇博客加深一下理解。 ","date":"2016-05-21","objectID":"/system_start/:0:1","tags":["linux"],"title":"理解系统启动过程","uri":"/system_start/"},{"categories":["Linux"],"content":"启动过程 先通过一张图来简单了解下整个系统启动的流程，整个过程基本可以分为POST–\u003eBIOS–\u003eMBR(GRUB)–\u003eKernel–\u003eInit–\u003eRunlevel。下面会详细说明每个过程的作用。 BIOS BIOS(Basic Input/Output System)，基本输入输出系统，该系统存储于主板的ROM芯片上，计算机在开机时，会最先读取该系统，然后会有一个加电自检过程，这个过程其实就是检查CPU和内存，计算机最基本的组成单元(控制器、运算器和存储器)，还会检查其他硬件，若没有异常就开始加载BIOS程序到内存当中。详细的BIOS功能，这边就不说了，BIOS主要的一个功能就是存储了磁盘的启动顺序，BIOS会按照启动顺序去查找第一个磁盘头的MBR信息，并加载和执行MBR中的Bootloader程序，若第一个磁盘不存在MBR，则会继续查找第二个磁盘(PS：启动顺序可以在BIOS的界面中进行设置)，一旦BootLoader程序被检测并加载内存中，BIOS就将控制权交接给了BootLoader程序。 MBR MBR(Master Boot Record)，主引导记录，MBR存储于磁盘的头部，大小为512bytes，其中，446bytes用于存储BootLoader程序，64bytes用于存储分区表信息，最后2bytes用于MBR的有效性检查。 GRUB GRUB(Grand Unified Bootloader)，多系统启动程序，其执行过程可分为三个步骤： Stage1：这个其实就是MBR，它的主要工作就是查找并加载第二段Bootloader程序(stage2)，但系统在没启动时，MBR根本找不到文件系统，也就找不到stage2所存放的位置，因此，就有了stage1_5 Stage1_5：该步骤就是为了识别文件系统 Stage2：GRUB程序会根据/boot/grub/grub.conf文件查找Kernel的信息，然后开始加载Kernel程序，当Kernel程序被检测并在加载到内存中，GRUB就将控制权交接给了Kernel程序。 PS：实际上这个步骤/boot还没被挂载，GRUB直接识别grub所在磁盘的文件系统，所以实际上应该是/grub/grub.conf文件，该配置文件的信息如下： grub.conf: #boot=/dev/sda default=0 #设定默认启动的title的编号，从0开始 timeout=5 #等待用户选择的超时时间 splashimage=(hd0,0)/boot/grub/splash.xpm.gz #GRUB的背景图片 hiddenmenu #隐藏菜单 title CentOS (2.6.18-194.el5PAE) #内核标题 root (hd0,0) #内核文件所在的设备 kernel /vmlinuz-2.6.18-194.el5PAE ro root=LABEL=/ #内核文件路径以及传递给内核的参数 initrd /initrd-2.6.18-194.el5PAE.img #ramdisk文件路径 ``` Kernel Kernel，内核，Kernel是Linux系统最主要的程序，实际上，Kernel的文件很小，只保留了最基本的模块，并以压缩的文件形式存储在硬盘中，当GRUB将Kernel读进内存，内存开始解压缩内核文件。讲内核启动，应该先讲下initrd这个文件， initrd(Initial RAM Disk)，它在stage2这个步骤就被拷贝到了内存中，这个文件是在安装系统时产生的，是一个临时的根文件系统(rootfs)。因为Kernel为了精简，只保留了最基本的模块，因此，Kernel上并没有各种硬件的驱动程序，也就无法识rootfs所在的设备，故产生了initrd这个文件，该文件装载了必要的驱动模块，当Kernel启动时，可以从initrd文件中装载驱动模块，直到挂载真正的rootfs，然后将initrd从内存中移除。 Kernel会以只读方式挂载根文件系统，当根文件系统被挂载后，开始装载第一个进程(用户空间的进程)，执行/sbin/init，之后就将控制权交接给了init程序。 Init init，初始化，顾名思义，该程序就是进行OS初始化操作，实际上是根据/etc/inittab(定义了系统默认运行级别)设定的动作进行脚本的执行，第一个被执行的脚本为/etc/rc.d/rc.sysinit，这个是真正的OS初始化脚本，简单讲下这个脚本的任务(可以去看看实际脚本，看看都做了什么)： 激活udev和selinux; 根据/etc/sysctl.conf文件，来设定内核参数; 设定系统时钟; 装载硬盘映射; 启用交换分区; 设置主机名; 根文件系统检测，并以读写方式重新挂载根文件系统; 激活RAID和LVM设备; 启用磁盘配额; 根据/etc/fstab，检查并挂载其他文件系统; 清理过期的锁和PID文件 执行完后，根据配置的启动级别，执行对应目录底下的脚本，最后执行/etc/rc.d/rc.local这个脚本，至此，系统启动完成。 Runlevel runlevel，运行级别，不同的级别会启动的服务不一样，init会根据定义的级别去执行相应目录下的脚本，Linux的启动级别分为以下几种： 0：关机模式 1：单一用户模式(直接以管理员身份进入) 2：多用户模式（无网络） 3：多用户模式（命令行） 4：保留 5：多用户模式（图形界面） 6：重启 在不同的运行级别下，/etc/rc.d/rc这个脚本会分别执行不同目录下的脚本： Runlevel 0 – /etc/rc.d/rc0.d/ Runlevel 1 – /etc/rc.d/rc1.d/ Runlevel 2 – /etc/rc.d/rc2.d/ Runlevel 3 – /etc/rc.d/rc3.d/ Runlevel 4 – /etc/rc.d/rc4.d/ Runlevel 5 – /etc/rc.d/rc5.d/ Runlevel 6 – /etc/rc.d/rc6.d/ 这些目录下的脚本只有K*和S*开头的文件，K开头的文件为开机需要执行关闭的服务，S开头的文件为开机需要执行开启的服务。 ","date":"2016-05-21","objectID":"/system_start/:0:2","tags":["linux"],"title":"理解系统启动过程","uri":"/system_start/"},{"categories":["Linux"],"content":"参考 http://www.thegeekstuff.com/2011/02/linux-boot-process/ [http://www.ibm.com/developerworks/library/l-linuxboot/]( ","date":"2016-05-21","objectID":"/system_start/:0:3","tags":["linux"],"title":"理解系统启动过程","uri":"/system_start/"},{"categories":["Network"],"content":"谈谈DNS","date":"2016-05-21","objectID":"/dns/","tags":["network"],"title":"谈谈DNS","uri":"/dns/"},{"categories":["Network"],"content":"写在前面 目前，我们大部分的网络通信都是基于TCP/IP协议的，而TCP/IP又基于IP地址作为唯一标识进行通信，随着需要记忆的IP地址数量的增多，肯定会超出我们的记忆能力范围，但如果使用一种利于人们的记忆的方式，如域名，例如\"www.google.com\"，我们便可以轻松的记忆这种方式的标识，而不是繁杂的数字。而DNS(域名系统)就是为了可以使用这种方式提供服务的。 ","date":"2016-05-21","objectID":"/dns/:0:1","tags":["network"],"title":"谈谈DNS","uri":"/dns/"},{"categories":["Network"],"content":"概念 DNS(Domain Name System)，域名系统，它是因特网的一项服务。它作为将域名和IP地址相互映射的一个分布式数据库，能够使人更方便地访问互联网。DNS使用TCP和UDP端口53。当前，对于每一级域名长度的限制是63个字符，域名总长度则不能超过253个字符。 DNS Domain Namespace，DNS域命名空间，是一种分层树状结构，其格式如下:“www.google.com”,以点\".“为分隔。结构如图所示： 根域：绝对域名(FQDN)，以点”.“结尾的域名 顶级域：用来指示某个国家/地区或组织使用的名称的类型名称，例如.com 二级域：个人或组织在因特网上使用的注册名称，例如google.com 子域：已注册的二级域名派生的域名，一般就是网站名，例如www.google.com 主机名：标识网络上的特定计算机，例如h1.www.google.com DNS资源记录：(即映射关系，通常由域名管理员进行配置)，常见类型如下： SOA：起始授权机构 NS：名称服务器 MX：邮件服务器 A：IP地址(最常用，映射IP地址) CNAME：别名(较常用，映射到其他域名) ","date":"2016-05-21","objectID":"/dns/:0:2","tags":["network"],"title":"谈谈DNS","uri":"/dns/"},{"categories":["Network"],"content":"DNS工作原理 当我们请求一个域名时，会通过DNS服务器将域名解析成IP访问最终的主机，那么，DNS是如何查询到域名所对应的IP并返回给我们的呢？请工作机制如图所示： 当我们请求一个域名时，直到获取到IP地址，整个过程是如何工作的？以请求www.codecc.xyz为例： 首先，我们的主机会去查找本地的hosts文件和本地DNS解析器缓存，如果hosts文件和本地DNS缓存存在www.codecc.xyz和IP的映射关系，则完成域名解析，请求该IP地址，否则进入第二步。 当hosts和本地DNS解析器缓存都没有对应的网址映射关系，则会根据机器(/etc/reslove.conf)配置的本地DNS服务器进行查询，此服务器收到查询时，如果要查询的域名在本地配置区域资源或者缓存中存在映射关系，则跳到步骤9，将解析结果直接返回给客户机。 PS：一二步骤为递归查询，其余步骤为迭代查询 若本地DNS服务器不存在该域名的映射关系，就把请求发送至13台根DNS服务器。 根DNS服务器会判断这个域名(.xyz)由谁来授权管理，并返回一个负责该顶级域的DNS服务器的一个IP给本地DNS服务器。 本地DNS服务器收到该IP后，会再将查询请求发送至(.xyz)所在的DNS服务器。 如果(.xyz)的DNS服务器无法解析该域名，就会去判断这个二级域名(codecc.xyz)的管理者，返回一个负责该二级域的DNS服务器的IP给本地DNS服务器。 本地DNS服务器收到该IP后，会再次将查询请求发送至(codecc.xyz)所在的DNS服务器。 (codecc.xyz)的DNS服务器会存有www.codecc.xzy的映射关系，将解析后的IP返回给本地DNS服务器 本地DNS服务器根据查询到的解析IP发送给客户机，至此，DNS解析完成。 ","date":"2016-05-21","objectID":"/dns/:0:3","tags":["network"],"title":"谈谈DNS","uri":"/dns/"},{"categories":["Network"],"content":"常用DNS查询命令 windows： nslookup 域名 Linux： nslookup 域名 dig 域名 ","date":"2016-05-21","objectID":"/dns/:0:4","tags":["network"],"title":"谈谈DNS","uri":"/dns/"},{"categories":["Network"],"content":"参考 https://en.wikipedia.org/wiki/Domain_Name_System https://technet.microsoft.com/en-us/library/cc772774(v=ws.10).aspx 《TCP/IP详解卷1：协议》 ","date":"2016-05-21","objectID":"/dns/:0:5","tags":["network"],"title":"谈谈DNS","uri":"/dns/"},{"categories":["Security"],"content":"WEB安全之CSP","date":"2016-05-20","objectID":"/webcsp/","tags":["security","web"],"title":"WEB安全之CSP","uri":"/webcsp/"},{"categories":["Security"],"content":"概念 内容安全策略(Content-Security-Policy，CSP)：是一种web应用技术用于帮助缓解大部分类型的内容注入攻击，包括XSS攻击和数据注入等，这些攻击可实现数据窃取、网站破坏和作为恶意软件分发版本等行为。该策略可让网站管理员指定客户端允许加载的各类可信任资源。 ","date":"2016-05-20","objectID":"/webcsp/:0:1","tags":["security","web"],"title":"WEB安全之CSP","uri":"/webcsp/"},{"categories":["Security"],"content":"浏览器支持 统计来源：caniuse.com/contentsecuritypolicy \u0026 Mozilla ","date":"2016-05-20","objectID":"/webcsp/:0:2","tags":["security","web"],"title":"WEB安全之CSP","uri":"/webcsp/"},{"categories":["Security"],"content":"指令参考 Content-Security-Policy 响应头的值可配置一个或多个，多个指令以分号;隔开。 指令 示例 描述 default-src ‘self’ cdn.example.com 默认配置，若其他指令没有配置，都以此配置的规则为准 script-src ‘self’ js.example.com 定义允许加载的JavaScript来源 style-src ‘self’ css.example.com 定义允许加载的样式表来源 img-src ‘self’ img.example.com 定义允许加载的图片来源 connect-src ‘self’ 适用于XMLHttpRequest(AJAX),WebSocket或EventSource，当为不允许的来源，浏览器返回一个400的状态码。 font-src font.example.com 定义允许加载的字体来源 object-src ‘self’ 定义允许加载的插件来源.eg,\u003cobject\u003e,\u003cembed\u003e或\u003capplet\u003e media-src media.example.com 定义允许加载的audio和video.eg,HTML5,\u003caudio\u003e,\u003cvideo\u003e元素 frame-src ‘self’ 定义允许加载的框架来源 sandbox allow-forms allow-scripts 授权一个沙箱用来请求具有iframe sanbox等类似属性的资源,该沙箱默认为同源策略,禁止弹出窗口,执行插件和脚本.若要允许其他,可增加配置:allow-forms,allow-same-origin,allow-scripts,allow-top-navigation report-uri /some-report-uri 该配置让浏览器发送一个失败报告到指定的路径，也可以增加-Report-only到HTTP头,让浏览器只发送报告(不做阻止动作) ","date":"2016-05-20","objectID":"/webcsp/:0:3","tags":["security","web"],"title":"WEB安全之CSP","uri":"/webcsp/"},{"categories":["Security"],"content":"来源配置参考 所有的指令都要在配置后面添加来源列表，多个来源列表可用空格隔开，*和none只能存在一个。 指令 示例 描述 * img-src * 无限制，允许所有 ‘none’ object-src ‘none’ 禁止加载任何路径的资源 ‘self’ script-src ‘self’ 允许加载同源的资源 data: img-src ‘self’ data: 允许通过数据模式加载资源 domain.ccc.com img-src img.ccc.com 允许加载匹配域名的资源 *.ccc.com img-src *.ccc.com 允许加载匹配域名的资源 https://img.ccc.com img-src https://img.ccc.com 允许加载匹配https方式的域名资源 https: img-src https: 允许加载所有匹配https方式的资源 ‘unsafe-inline’ script-src ‘unsafe-inline’ 允许使用内联元素,类似,Style attribute,onclick,scripttag bodies ‘unsafe-eval’ script-src ‘unsafe-eval’ 允许不安全的动态编码，例如eval() ","date":"2016-05-20","objectID":"/webcsp/:0:4","tags":["security","web"],"title":"WEB安全之CSP","uri":"/webcsp/"},{"categories":["Security"],"content":"例子 只允许加载同源的所有资源 default-src 'self'; 支持*号匹配 default-src 'self' https://*.ccc.com:*; 只允许加载同源的脚本 script-src 'self'; 只允许加载同源的和www.ccc.com的脚本 script-src 'self' www.ccc.com; ","date":"2016-05-20","objectID":"/webcsp/:0:5","tags":["security","web"],"title":"WEB安全之CSP","uri":"/webcsp/"},{"categories":["Security"],"content":"常见配置 该策略允许加载同源的图片、脚本、AJAX和CSS资源，并阻止加载其他任何资源，对于大多数网站是一个不错的配置。 default-src 'none'; script-src 'self'; connect-src 'self'; img-src 'self'; style-src 'self'; 被禁止时的报错信息： 谷歌浏览器可通过谷歌开发工具查看该报错，通常是按F12 Refused to load the script ‘script-uri’ because it violates the following Content Security Policy directive: “your CSP directive”. Firefox 可通过 Web Developer Tools 查看报错 Content Security Policy: A violation occurred for a report-only CSP policy (“An attempt to execute inline scripts has been blocked”). The behavior was allowed, and a CSP report was sent. ","date":"2016-05-20","objectID":"/webcsp/:0:6","tags":["security","web"],"title":"WEB安全之CSP","uri":"/webcsp/"},{"categories":["Security"],"content":"参考 http://content-security-policy.com/ https://developer.mozilla.org/en-US/docs/Web/Security/CSP http://www.w3.org/TR/CSP2/ ","date":"2016-05-20","objectID":"/webcsp/:0:7","tags":["security","web"],"title":"WEB安全之CSP","uri":"/webcsp/"},{"categories":["Network"],"content":"初识网络通信","date":"2016-05-18","objectID":"/network_comm/","tags":["network"],"title":"初识网络通信","uri":"/network_comm/"},{"categories":["Network"],"content":"写在前面 在计算机刚出现的时候，只能在本机进行一些运算处理，想将一台计算机中的数据转移到另一台计算机中，需要通过外部存储介质来传输，例如磁带、软盘。而网络技术的出现，使得计算机间可以通过一些传输介质(网线、光纤等)，实现快速的数据传输和信息交互。如今，网络已无处不在，那么，计算机之间究竟是如何通信的呢？下面会通过一些基础的网络知识来简单理解计算机之间的通信过程。 ","date":"2016-05-18","objectID":"/network_comm/:0:1","tags":["network"],"title":"初识网络通信","uri":"/network_comm/"},{"categories":["Network"],"content":"网络通信模型 网络通信模型是一种概念模型和框架，旨在使各种计算机在世界范围内互连为网络。其中有OSI七层模型和TCP/IP四层模型，现在大部分网络通信都是以TCP/IP四层模型为基础的。 它们的对应层次如下图： OSI有七层：从上到下依次为应用层、表示层、会话层、传输层、网络层、数据链路层、物理层 TCP/IP有四层：从上到下依次为应用层、传输层、互连层(网络层)、网络接口层(链路层)。 因为目前大部分TCP/IP模型，所以就以TCP/IP为例，我们来理解下数据间的通信，下图是两台计算机通信的数据的传输过程： ","date":"2016-05-18","objectID":"/network_comm/:0:2","tags":["network"],"title":"初识网络通信","uri":"/network_comm/"},{"categories":["Network"],"content":"数据封装 在详细了解TCP/IP每一层各自的作用前，先要理解数据封装的概念，数据在通过网络接口传送出去前，会经过层层封装，每层都会在前面的基础上添加自己的信息，在传输到对方计算机后，又会被层层进行解封装后得到最后的数据。其过程如下图所示： ","date":"2016-05-18","objectID":"/network_comm/:0:3","tags":["network"],"title":"初识网络通信","uri":"/network_comm/"},{"categories":["Network"],"content":"TCP/IP参考模型 TCP/IP参考模型是一个抽象的分层模型，这个模型中，所有的TCP/IP系列网络协议都被归类到4个抽象的\"层\"中。每一抽象层创建在低一层提供的服务上，并且为高一层提供服务。 完成一些特定的任务需要众多的协议协同工作，这些协议分布在参考模型的不同层中的，因此有时称它们为一个协议栈。 应用层(Application Layer) 该层包括所有和应用程序协同工作，利用基础网络交换应用程序专用的数据的协议。 应用层是大多数普通与网络相关的程序为了通过网络与其他程序通信所使用的层。这个层的处理过程是应用特有的；数据从网络相关的程序以这种应用内部使用的格式进行传送，然后被编码成标准协议的格式。 常见的应用层协议有HTTP、FTP、DNS、SNMP(基于UDP) 传输层(Transport Layer) 主要为两台主机上的应用程序提供端到端的通信，包括TCP协议（传输控制协议）和UDP（用户数据报协议）。 端口号由此层提供，且在一台计算机中具有唯一性。 UDP为应用层提供一种非常简单的服务。它只是把称作数据报的分组从一台主机发送到另一台主机，但并不保证该数据报能到达另一端。任何必需的可靠性必须由应用层来提供。 TCP为两台主机提供高可靠性的数据通信。它所做的工作包括把应用程序交给它的数据分成合适的小块交给下面的网络层，确认接收到的分组，设置发送最后确认分组的超时时钟等,由于运输层提供了高可靠性的端到端的通信，因此应用层可以忽略所有这些细节。 因为TCP是一种面向连接的协议，所以两个在使用TCP的应用在彼此交换数据前必须先建立一个TCP连接，也就是有名的TCP三次握手，如下图所示： 建立连接协议过程：（TCP三次握手协议） 客户端发送一个SYN段指明客户打算连接的服务器的端口，以及初始序号（ISN）。 服务器发回包含服务器的初始序号的SYN报文段作为应答。同时，将确认序号设置为客户的ISN加1以对客户的SYN报文段进行确认。一个SYN占用一个序号。 客户将确认序号设置为服务器的ISN加1以对服务器的SYN报文段进行确认。 网络层(Internet Layer) 处理分组在网络中的活动。网络层协议包括IP协议（网际协议），ICPM协议（Internet互联网控制报文协议），以及IGMP协议（Internet组管理协议），其中的IP协议身是TCP/IP协议簇中最为核心的协议。IP提供的是不可靠、无连接的数据包传送服务。 IP地址 讲到IP协议就应该讲讲IP地址，IP地址是分配给网络上使用IP协议的设备的数字标签，有IPv4和IPv6两大类，我们目前使用的大部分还是IPv4的地址，以下简称IP地址，IP地址由32位二进制数组成，为便于使用，常以XXX.XXX.XXX.XXX形式表示。 IP地址由两个字段组成：网络号(net-id)和主机号(host-id)，为方便IP地址管理，IP地址被分为五类，如下图： 其中A、B、C类地址为单播（unicast）地址；D类地址为组播（multicast）地址；E类地址为保留地址，以备将来的特殊用途。目前大量使用中的IP地址属于A、B、C三类地址。 A类地址范围：0.0.0.0～127.255.255.255 B类地址范围：128.0.0.0～191.255.255.255 C类地址范围：192.0.0.0～223.255.255.255 私网地址范围：10.0.0.0～10.255.255.255 ，172.16.0.0～172.31.255.255 ，192.168.0.0～192.168.255.255，私网地址只能在本地局域网中使用，不在公网中使用。 子网和掩码 传统的IP地址分配方式，对IP地址的浪费非常严重。为了充分利用已有的IP地址，人们提出了掩码（mask）和子网（subnet）的概念。 掩码是一个与IP地址对应的32位数字，这些数字中一些为1，另外一些为0。原则上这些1和0可以任意组合，不过一般在设计掩码时，网络号码和子网号码的比特值为1，主机号码的比特值为0。掩码可以把IP地址分为两个部分：子网地址和主机地址。IP地址与掩码中为1的位对应的部分为子网地址，其他的位对应的部分则是主机地址。当不进行子网划分时，子网掩码即为默认值，此时子网掩码中“1”的长度就是网络号码的长度。即A类地址对应的掩码默认值为255.0.0.0；B类地址的掩码默认值为255.255.0.0；C类地址掩码的默认值为255.255.255.0。 路由 概念：若目的主机与源主机在同一共享网络内，IP数据报直接送达目的主机，否则，主机把数据报发往默认的路由器上，由路由器进行数据报转发。 链路层(Link Layer) 通常包括设备驱动程序和网络接口卡。处理与传输媒介的物理接口细节。 主要协议有：ARP协议和RARP协议 MAC地址 ：数据链路层具有自己的寻址机制(48bit地址)，当一台主机把以太网数据帧发送到位于同一局域网上得另一台主机时，是根据48bit的以太网地址来确定目的接口的。 而ARP和RARP协议是为IP地址和MAC地址提供映射的： ","date":"2016-05-18","objectID":"/network_comm/:0:4","tags":["network"],"title":"初识网络通信","uri":"/network_comm/"},{"categories":["Network"],"content":"使用 我们在判断两台主机应用之间的网络是否正常，通常是判断到对方IP和端口是否能通。 常用网络判断命令： Windows ping $IP：最常用的判断网络是否可达的命令。 tracert $IP：跟踪路由，即打印出本机到到目的IP，所经过路由。 telnet $IP $port：可以测试某个IP和应用端口是否能通。 netstat：查看本机监听和建立连接的端口。 Linux ping $IP：最常用的判断网络是否可达的命令 traceroute $IP：跟踪路由，即打印出本机到到目的IP，所经过路由。 或者使用mtr -ni 0.1 $IP，可以实现以上两个共同的效果 nc -vz $IP $PORT：测试到目的IP的应用端口是否能通。 netstat -tupln：可以查看本机目前监听的端口 ","date":"2016-05-18","objectID":"/network_comm/:0:5","tags":["network"],"title":"初识网络通信","uri":"/network_comm/"},{"categories":["Network"],"content":"参考 https://en.wikipedia.org/wiki/Internet_protocol_suite 《TCP/IP详解卷1:协议》 ","date":"2016-05-18","objectID":"/network_comm/:0:6","tags":["network"],"title":"初识网络通信","uri":"/network_comm/"},{"categories":null,"content":"About CC Software Developer Networking Engineer ","date":"2019-08-02","objectID":"/about/:1:0","tags":null,"title":"About Mess","uri":"/about/"},{"categories":null,"content":"Working Experience CF (2019-current) DevOps Engineer CNC (2015-2018) Operations Engineer(SA) ","date":"2019-08-02","objectID":"/about/:2:0","tags":null,"title":"About Mess","uri":"/about/"},{"categories":null,"content":"Focus \u0026 Interests Linux Networking Cloud Native Golang, Shell DevOps ","date":"2019-08-02","objectID":"/about/:3:0","tags":null,"title":"About Mess","uri":"/about/"}]